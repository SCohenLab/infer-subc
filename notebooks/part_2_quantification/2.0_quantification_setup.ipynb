{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Welcome to `infer-subc`**\n",
    "-----\n",
    "\n",
    "## **<ins>Part 2</ins>: Quantification**\n",
    "\n",
    "### **Modular Quantification Pipelines**\n",
    "\n",
    "`Part 2` of infer-subc carries out quantification on segmented organelle objects. Four modular pipelines are included: \n",
    "- [2.1 Organelle morphology](2.1_organelle_morphology.ipynb) (amount, size, shape)\n",
    "- [2.2 Organelle interactions]() (amounts, size, shape)\n",
    "- [2.3 Subcellular distribution]() in XY and Z separately (of organelles and interaction sites)\n",
    "- [2.4 Cell morphology]() (size, shape)\n",
    "\n",
    "### **Foundational Quantification Methods**\n",
    "\n",
    "These pipelines utilize common methods that are outlined and explained in the `'methods_...'` notebooks:\n",
    "- [Morphology measurements](method_morphology.ipynb)\n",
    "- [Creating organelle interaction sites](method_interactions.ipynb)\n",
    "- [Distribution measurements](method_distribution.ipynb)\n",
    "\n",
    "### **Organelle Signature Analysis**\n",
    "\n",
    "When several organelles are derived from the same cell (e.g., multispectral imaging of lysosomes, mitochondria, Golgi, peroxisomes, ER, and lipid droplets), these analyses can be combined to create the *`organelle signature`*, a descriptive summary of the organelles in a particular cell type under particular conditions. Because the organelle signature is quantitative, it can be used to compare between cell types and conditions. \n",
    "\n",
    "To analyze the organelle morphology, interactions, and distribution and cell morphology together for *`organelle signature analysis`*, the above mentioned pipelines can be run in combination to quantify the organelle and cell features and summarize them per cell. The following notebook outlines this process: [batch_process_quantification](batch_process_quantification.ipynb) notebook \n",
    "\n",
    "### **File Format**\n",
    "#### ‚û°Ô∏è Part 2 input formatting:\n",
    "`infer-subc` quantification pipelines take in the following files:\n",
    "1. **\"Raw\" image** - The intensity image used to generate the segmentations. *If you did not use infer-subc `Part 1` to create your segmentations, please see the [1.0_image_setup](https://github.com/SCohenLab/infer-subc/blob/49b0100ea8ab74fe03ad638702b0ceb41d636bd2/notebooks/part_1_segmentation_workflows/1.0_image_setup.ipynb) notebook for information about the file types accepted.*\n",
    "2. **Organelle segmentation file(s)** - The unsigned integer (i.e., all pixels/voxels in an object are labeled with a unique object number) files associated to each of the organelles you'd like to quantify. *Organelle segmentations can be created using infer-subc or through alternative methods that may be preferred or more well suited for a particular experimental setup.*\n",
    "    - ***Naming:*** The files should have the same route file name as the raw image. The only thing differentiating them should be the file suffix. You will be able to specify the character(s) used to separate the route name from the suffix (e.g., \"-\" in the example below) and the masks/organelle names used as the suffix (e.g., \"cell\", \"nuc\", \"lyso\", etc. in the example below). Make sure that the \"nuc\" and \"cell\" segmentations exist as seperate files prior to quantification. This can be done using the [quality check segmentation notebook](..notebooks/part_1_segmentation_workflows/quality_check_segmentations.ipynb).\n",
    "    - ***File location:*** All organelle segmentation files that you would like to include in the same analysis should be saved in the same location. Our convention is to quantify all the image files from one experiment at the same time. The quantitative metrics for each experiment are subsequently combined and summarized per cell. See the file structure example below.\n",
    "    - ***File type:*** The files can be saved as '.tif' or '.tiff' files. The default output from the Napari `organelle-segmenter-plugin` and [`batch_process_segmentation` notebook]() is '.tiff', but if any files are editted and saved directly from Napari using `File` > `Save Selected Layers...`, the default if '.tif'. *Other file types are currently not supported!*\n",
    "3. **Mask file(s)** - At least one binary mask file is required. The mask will be used to determine the area from which to quantify the organelles. For single cell analysis, this would typically be the \"cell mask\". This is especially important to measure subcellular distribution and metrics such as the organelle volume fraction (organelle volume / cell volume). Additional masks, like the nucleus or other subcellular regions (e.g., neurites and soma in neurons), can be included for morphological analysis, too. The naming, location, and file type follows the same conventions used for segmentation files.\n",
    "4. **Information for each quantification method**: Each quantification method has a specific set of parameters that you can specify for your desired outcome. See the `'Method_...'` notebooks in this section for more details on these paratmeters.\n",
    "\n",
    "> **Example file naming & folder architecture:**\n",
    "> - üìÇ experiment_1\n",
    ">     - üìÇ raw_data\n",
    ">         - üìú date_condition1_cell1.czi\n",
    ">         - üìú date_condition2_cell1.czi\n",
    ">         - üìú ...\n",
    ">     - üìÇ segmentation_data\n",
    ">         - üìú date_condition1_cell1-cell.tif\n",
    ">         - üìú date_condition1_cell1-nuc.tiff\n",
    ">         - üìú date_condition1_cell1-lyso.tiff\n",
    ">         - üìú date_condition1_cell1-mito.tiff\n",
    ">         - üìú date_condition1_cell1-golgi.tif\n",
    ">         - üìú date_condition1_cell1-perox.tiff\n",
    ">         - üìú date_condition1_cell1-ER.tiff\n",
    ">         - üìú date_condition1_cell1-LD.tiff\n",
    ">         - üìú date_condition2_cell1-cell.tiff\n",
    ">         - üìú date_condition2_cell1-nuc.tiff\n",
    ">         - üìú date_condition2_cell1-lyso.tiff\n",
    ">         - üìú date_condition2_cell1-mito.tiff\n",
    ">         - üìú date_condition2_cell1-golgi.tiff\n",
    ">         - üìú date_condition2_cell1-perox.tiff\n",
    ">         - üìú date_condition2_cell1-ER.tiff\n",
    ">         - üìú date_condition2_cell1-LD.tiff\n",
    ">         - üìú ...\n",
    ">     - üìÇ quantification_output\n",
    "> - üìÇ experiment_2\n",
    ">     - üìÇ raw_data\n",
    ">     - üìÇ segmentation_data\n",
    ">     - üìÇ quantification_output\n",
    "> \n",
    "> ***Experiment Folder**: 'experiment_1' and 'experiment_2' do not have to be in the same location. You will initially quantify each experiment's worth of data separately.*\n",
    "\n",
    "#### Part 2 expected output: ‚û°Ô∏è \n",
    "A. **batch_process_quantification()** will result in the following files:\n",
    "- `file-prefix_interactions.csv` - metrics derived from the [organelle interaction method]()\n",
    "- `file-prefix_distributions.csv` - metrics derived from the [subcellular distribution method]()\n",
    "- `file-prefix_organelles.csv` - metrics derived from the [organelle morphology method]()\n",
    "- `file-prefix_regions.csv` - metrics derived from the [cell regions morphology method]()\n",
    "\n",
    "B. **batch_process_summarystats()** will result in the following files:\n",
    "- `file-prefix_distribution_summarystats.csv` - per cell summary statistics of the metrics included in the 'file-prefix_distributions.csv' file\n",
    "- `file-prefix_per_interaction_summary.csv` - per cell summary statistics of the metrics included in the 'file-prefix_contacts.csv' file\n",
    "- `file-prefix_per_org_summary.csv` - per cell summary statistics of the metrics included in the 'file-prefix_organelles.csv file\n",
    "- `file-prefix_per_region_summary.csv` - per cell summary statistics of the metrics included in the 'file-prefix_regions.csv'\n",
    "- `file-prefix_summarystats_combined.csv` - combined metrics for all organelles and interaction sites summarized per cell. Each column includes the calculated values for a single metric; each row includes the values associated to a single cell. The definitions for each metric are included here: [batch_summary_stats_output_definitions.xlsx](batch_summary_stats_output_definitions.xlsx)\n",
    "\n",
    "**'file-prefix' is user defined and can differ between experiments as needed.*\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Using Jupyter notebooks in `infer-subc`**\n",
    "\n",
    "#### üìç **Purpose of `Part 2: Quantification` Notebooks:**\n",
    "\n",
    "The Part 2 Juptyer notebooks include expository examples of the quantification methods, modular quantification pipelines, and a pipeline to carry out all of the quantification types in combination. Use the following check list and guidelines to batch process your quantification.\n",
    "\n",
    "#### ‚úÖ **Part 2: Quantification Checklist:**\n",
    "1. <input type=\"checkbox\"/>  **Quantification Setup**\n",
    "   \n",
    "   Use [2.0_quantification_setup](2.0_quantification_setup.ipynb) (this notebook) to ensure your raw and segmentation files can be read into memory and metadata is accessible\n",
    "   \n",
    "2. <input type=\"checkbox\"/>  **Batch process quantification for each experiment separately**\n",
    "\n",
    "   The first step in the quantification pipeline is to quantify features from the segmentation and raw images. We recommend that this is carried out on each experimental replicate *separately*. Save the quantification output for each experimental replicate in a separate folder named according for that replicate. The folder name will be used as  part of th metadata in following step.\n",
    "   \n",
    "   Use the `batch_process_quantification()` function available in the [batch_process_quantification notebook](batch_process_quantification.ipynb) notebook for the combined analysis, or comparible function(s) for the modular analysis to quantify each experimental replicate, *separately*.\n",
    "3. <input type=\"checkbox\"/>  **Summarize metrics per cell across one or more experiments** \n",
    "   \n",
    "   Now, the quantification results from one or more experiment are summarized per cell.\n",
    "\n",
    "   Use the `batch_summary_stats()` function available in the [batch_process_quantification notebook](batch_process_quantification.ipynb) notebook for the combined analysis, or comparible functions(s) for the modular analysis to summarize the quantified features per cell. \n",
    "\n",
    "#### üìñ **How to use Jupyter Notebooks:**  \n",
    " \n",
    "Advance through each block of code below by pressing `Shift`+`Enter` or pressing the \"Execute Cell\" (`‚ñ∂Ô∏è`) button to the left of each block.\n",
    " \n",
    "\n",
    "You will see a series of instructions before each block of code. Be on the look out for the following headers and follow the instructions accordingly:\n",
    "- &#x1F3C3; **Run code; no user input required** - proceed without adding anything to the code block\n",
    "- &#x1F453; **FYI** (for your information) - helpful information usually to bring context to what is going on\n",
    "- &#x1F6D1; &#x270D; **User Input Required** - stop and input the appropriate information about your data. The following indicator will also be present in the code block:\n",
    "   ```python \n",
    "   #### USER INPUT REQUIRED ###\n",
    "   ```\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----- \n",
    "\n",
    "# **2.0 Quantification Setup**\n",
    "\n",
    "The first thing we need to be able to do is access the raw image and segmentation files and interact with them (e.g., read the metadata and visualize the image). The following notebook walks you through the steps used in all of the `part_2_quantification` notebooks to load your image of choice.\n",
    "\n",
    "### üë£ **Summary of steps**  \n",
    "\n",
    "**IMPORTS**\n",
    "\n",
    "- **`Step 1`** - load all python packages necessary for this notebooks\n",
    "\n",
    "**LOAD AND READ IN RAW IMAGE FOR PROCESSING**\n",
    "\n",
    "- **`Step 2`** - Select image from file list\n",
    "- **`Step 3`** - Load image into memory and print the associated metadata\n",
    "\n",
    "**READ IN THE MATCHING SEGMENTATION FILES**\n",
    "\n",
    "- **`Step 4`** - Load the segmentation files that match the raw image\n",
    "\n",
    "**VIZUALIZE**\n",
    "\n",
    "- **`Step 5`** - View the raw image and matching segmentation files using Napari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## **IMPORTS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &#x1F3C3; **Run code; no user input required**\n",
    "\n",
    "&#x1F453; **FYI:** This code block loads all of the necessary python packages and functions you will need for this notebook. The convention with notebooks (and python in general) is to import the nescessary packages as the first thing. We are using `napari` for visualization. The underlying data format are `numpy` `ndarrays`. Quantitative data will be formated, summarize, and exported as '.csv' files using `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import napari\n",
    "from napari.utils.notebook_display import nbscreenshot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from infer_subc.core.file_io import (read_czi_image,\n",
    "                                     read_tiff_image,\n",
    "                                     export_inferred_organelle,\n",
    "                                     import_inferred_organelle,\n",
    "                                     list_image_files,\n",
    "                                     sample_input_quant,\n",
    "                                     copy_raw)\n",
    "from infer_subc.utils.batch import find_segmentation_tiff_files\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## **LOAD IMAGE AND READ RAW IMAGE METADATA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &#x1F6D1; &#x270D; **User Input Required:**\n",
    "\n",
    "Please specify the following information about your data:\n",
    "- `raw_img_type`: the file type of your raw image written in quotation marks; EX: \".czi\" or \".tiff\"\n",
    "- `data_root_path`: the path to folder that contains subfolders, including your input data and a separate folder for segmentation outputs to be saved; EX: \"C:/Users/{your-user-name}/Documents/Exp1\"\n",
    "- `raw_data_path`: the path to the folder that contains your input data; EX: data_root_path / \"input\"\n",
    "- `seg_data_path`: the path to the folder that contains your segmentation files; EX: data_root_path / \"segmentations\"\n",
    "- `quant_data_path`: the path to the folder where quantification output files will be saved; EX: data_root_path / \"quantification\"\n",
    "\n",
    "\n",
    "Follow this example's formatting:\n",
    "> ```python \n",
    "> raw_img_type = \".czi\"\n",
    "> data_root_path = Path(os.path.expanduser(\"~\")) / \"Documents/Python_Scripts/Infer-subc\"\n",
    "> raw_data_path = data_root_path / \"raw\"\n",
    "> seg_data_path = data_root_path / \"out\"\n",
    "> quant_data_path = data_root_path / \"quant\"\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### üìù **Sample Data**\n",
    "To run the quantification setup notebook with sample data, set `sample_data_type` to \"neuron_1\", \"astrocyte\", \"neuron_2\", or \"iPSC\". Ignore changing the \"USER SPECIFIED\" strings as the directories will be set automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USER INPUT REQUIRED ###\n",
    "\n",
    "# If using the sample data, select which cell type you would like analyze (\"neuron_1\", \"astrocyte\", \"neuron_2\" or \"iPSC\"):\n",
    "# If not using the sample data, set sample_data_type to None\n",
    "\n",
    "sample_data_type = \"astrocyte\"\n",
    "\n",
    "# If you are not using the sample data, please edit \"USER SPECIFIED\" as necessary.\n",
    "\n",
    "## Define the path to the directory that contains the input image folder.\n",
    "data_root_path = Path(\"USER SPECIFIED\")\n",
    "\n",
    "# Specify the file type of your raw data that will be analyzed. Ex) \".czi\" or \".tiff\"\n",
    "raw_img_type = \"USER SPECIFIED\"\n",
    "\n",
    "## Specify which subfolder that contains the input data and the input data file extension\n",
    "raw_data_path = data_root_path / \"USER SPECIFIED\"\n",
    "\n",
    "## Specify which subfolder that contains the segmentations\n",
    "seg_data_path = data_root_path / \"USER SPECIFIED\"\n",
    "\n",
    "## Specify the output folder to save the quantification outputs if.\n",
    "## If its not already created, the code below will create it for you\n",
    "quant_data_path = data_root_path / \"USER SPECIFIED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OPTIONAL ###\n",
    "# copy sample data raw images into quant folder, NECESSARY IF USING SAMPLE DATA\n",
    "# if you wish to copy files over, insert a list containing any combination of the \n",
    "# following: 'neuron_1', 'astrocyte', 'neuron_2', 'iPSC'\n",
    "copy_raw([]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &#x1F3C3; **Run code; no user input required**\n",
    "&#x1F453; **FYI:** \n",
    "- A list of the images included in the `raw_data_path` folder is printed below for easy reference.\n",
    "- If the `quant_data_path` folder does not exist, it will be created now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If sample_data_type is set to \"neuron_1\", \"astrocyte\", \"neuron_2\" or \"iPSC\" then the sample data is used and the directories are set\n",
    "if sample_data_type != None:\n",
    "    data_root_path, raw_img_type, raw_data_path, seg_data_path, quant_data_path = sample_input_quant(sample_data_type)\n",
    "\n",
    "# Create the output directory to save the segmentation outputs in.\n",
    "if not Path.exists(quant_data_path):\n",
    "    Path.mkdir(quant_data_path)\n",
    "    print(f\"making {quant_data_path}\")\n",
    "\n",
    "# Create a list of the file paths for each image in the input folder. Select test image path.\n",
    "raw_img_file_list = list_image_files(raw_data_path,raw_img_type)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.DataFrame({\"Image Name\":raw_img_file_list})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &#x1F6D1; &#x270D; **User Input Required:**\n",
    "\n",
    "Use the list above to specify which image you wish to analyze:\n",
    "- `test_img_n`: the index, or number, associated with your image of choice from the list above.\n",
    "\n",
    "\n",
    "Follow this example's formatting:\n",
    "> ```python \n",
    "> test_img_n = 5\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### USER INPUT REQUIRED ###\n",
    "test_img_n = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &#x1F3C3; **Run code; no user input required**\n",
    "\n",
    "&#x1F453; **FYI:** This code block reads the image and image metadata into memory. Then, the metadata is printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the image and metadata as an ndarray and dictionary from the test image selected above. \n",
    "test_img_name = raw_img_file_list[test_img_n]\n",
    "img_data,meta_dict = read_czi_image(test_img_name)\n",
    "\n",
    "# Define some of the metadata features.\n",
    "channel_names = meta_dict['name']\n",
    "meta = meta_dict['metadata']['aicsimage']\n",
    "scale = meta_dict['scale']\n",
    "channel_axis = meta_dict['channel_axis']\n",
    "file_path = meta_dict['file_name']\n",
    "\n",
    "print(\"Metadata information\")\n",
    "print(f\"File path: {file_path}\")\n",
    "for i in list(range(len(channel_names))):\n",
    "    print(f\"Channel {i} name: {channel_names[i]}\")\n",
    "print(f\"Scale (ZYX): {scale}\")\n",
    "print(f\"Channel axis: {channel_axis}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## **READ IN THE MATCHING SEGMENTATION FILES**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#x1F453; **FYI:** This code block finds the segmentation files that match your selected raw image.\n",
    "\n",
    "#### &#x1F6D1; &#x270D; **User Input Required:**\n",
    "\n",
    "Specify the following information:\n",
    "- `org_file_names`: a list of image suffixes used for each of the segmentation files you would like to include in the analysis\n",
    "- `org_channels_ordered`: a list of channel indexes corresponding to the channel number for each organelle listed in the org_file_names variable; in the example below, the index of the lysosome channel with suffix \"lyso\" is channel index number 1\n",
    "- `regions_file_names`: a list of the masks or subcellular regions to include in the analysis\n",
    "- `suffix_separator`: the character used to separate the route file name (same as the raw file) and the organelle/mask suffix\n",
    "- `mask_name`: the suffix of one of the \"regions_file_names\" that will be used to define the analysis region (aka - mask) the organelles for single cell analysis\n",
    "\n",
    "Follow this example's formatting:\n",
    "> ```python\n",
    "> org_file_names = [\"lyso\", \"mito\", \"golgi\", \"perox\", \"ER\", \"LD\"]\n",
    "> org_channels_ordered = [1, 2, 3, 4, 5, 6]\n",
    "> regions_file_names = [\"cell\", \"nuc\"]\n",
    "> suffix_separator = \"-\"\n",
    "> mask_name = \"cell\"\n",
    "> ```\n",
    "\n",
    "If using neuron_1\n",
    "> ```python\n",
    "> org_channels_ordered = [3, 4, 2, 5, 1, 0]\n",
    "> ```\n",
    "\n",
    "If using astrocyte\n",
    "> ```python\n",
    "> org_channels_ordered = [3, 4, 2, 5, 1, 0]\n",
    "> ```\n",
    "\n",
    "If using neuron_2\n",
    "> ```python\n",
    "> org_channels_ordered = [4, 3, 2, 1, 0, 6]\n",
    "> ```\n",
    "\n",
    "If using iPSC\n",
    "> ```python\n",
    "> org_channels_ordered = [2, 3, 4, 5, 6, 0]\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### USER INPUT REQUIRED ###\n",
    "org_file_names = [\"lyso\", \"mito\", \"golgi\", \"perox\", \"ER\", \"LD\"]\n",
    "org_channels_ordered = [3, 4, 2, 5, 1, 0]\n",
    "regions_file_names = [\"cell\", \"nuc\"]\n",
    "mask_name = \"cell\"\n",
    "suffix_separator = \"-\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &#x1F3C3; **Run code; no user input required**\n",
    "\n",
    "&#x1F453; **FYI:** This code finds the matching segmentation file from the `seg_file_path` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find file paths for segmentations\n",
    "all_suffixes = org_file_names + regions_file_names\n",
    "filez = find_segmentation_tiff_files(file_path, all_suffixes, seg_data_path, suffix_separator)\n",
    "\n",
    "# read the segmentation and masks/regions files into memory\n",
    "organelles = [read_tiff_image(filez[org]) for org in org_file_names]\n",
    "regions = [] \n",
    "for m in regions_file_names:\n",
    "    mfile = read_tiff_image(filez[m])\n",
    "    regions.append(mfile)\n",
    "\n",
    "# match the intensity channels to the segmentation files\n",
    "intensities = [img_data[ch] for ch in org_channels_ordered]\n",
    "\n",
    "# specifiy the mask image\n",
    "m = regions_file_names.index(mask_name)\n",
    "mask = regions[m]\n",
    "\n",
    "# print paths to matching seg files\n",
    "print(\"The following matching files were found:\")\n",
    "filez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## **VISUALIZE THE IMAGES**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &#x1F3C3; **Run code; no user input required**\n",
    "\n",
    "&#x1F453; **FYI:** This code block opens your image in Napari. The Napari graphical user interface (GUI) will open as a separate window. You can learn more about how to use the Napari GUI [here](https://napari.org/stable/tutorials/fundamentals/quick_start.html).\n",
    "\n",
    "*In the Napari viewer, the organelle segmentations are added as \"labels\" layers where each object appears as a different color.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open viewer and add images\n",
    "viewer = napari.Viewer()\n",
    "for r, reg in enumerate(regions_file_names):\n",
    "    viewer.add_image(regions[r],\n",
    "                     scale=scale,\n",
    "                     name=f\"{reg} mask\")\n",
    "\n",
    "# colors = [\"red\", \"bop orange\", \"yellow\", \"green\", \"blue\", \"cyan\", \"magenta\", \"bop purple\"]\n",
    "for o, org in enumerate(org_file_names):\n",
    "    viewer.add_image(intensities[o],\n",
    "                     scale=scale,\n",
    "                     name=f\"{org} intensity channel\")\n",
    "    viewer.add_labels(organelles[o],\n",
    "                      scale=scale,\n",
    "                      name=f\"{org} segmentation\")\n",
    "viewer.grid.enabled = True\n",
    "viewer.reset_view()\n",
    "    \n",
    "print(\"Proceed to Napari window to view your selected image.\")\n",
    "\n",
    "# screenshot viewer\n",
    "nbscreenshot(viewer, canvas_only = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "### **NEXT STEPS**\n",
    "\n",
    "Now that your files are able to be accessed, you can continue on the the methods notebooks that illustrate how each quantification method is carried out:\n",
    "- [**Organelle Morphology**]() - quantification of the morphology of each organelle; this notebook uses a single organelle type within a single cell as an example\n",
    "- [**Organelle distribution**]() - quantification of the subcellular distribution of organelles in  XY and Z separately; this notebook uses a single organelle type within a single cell as an example\n",
    "- [**Organelle Interactions**]() - creation of organelle interaction sites; quantification of their morphology and distribution; this notebook uses a single cell as an example\n",
    "- [**Cell Region Morphology**]() - quantification of the morphology of cell regions / masks\n",
    "- [**Combined Analysis and Batch Processing**]() - this notebook includes functions for processing all three quantification methods one a single cell and batch processing the combined quantification across multiple cells\n",
    "- [**Create Summary Statistics per Cell**]() - this notebook includes a description of the summary statistics function used to summarize all of the data per cell across several experiments\n",
    "\n",
    "\n",
    "If you understand the methods included in infer-subc and are ready to quantify your data, please proceed to [**batch_process_quantification**](batch_process_quantification.ipynb).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "infer-subc-sc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
