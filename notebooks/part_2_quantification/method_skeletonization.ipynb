{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "301523e5",
   "metadata": {},
   "source": [
    "Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f711b0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top level imports\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "from typing import Optional, Union, Tuple, List, Any\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from infer_subc.core.file_io import (read_tiff_image,\n",
    "                                        list_image_files,\n",
    "                                        read_czi_image)\n",
    "from infer_subc.core.img import apply_mask\n",
    "from skimage.morphology import skeletonize\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.sparse import csgraph\n",
    "from infer_subc.quantification.stats import *\n",
    "from infer_subc.quantification.stats import _assert_uint16_labels\n",
    "\n",
    "# for the sake of staying within the notebook, I will copy the functions directly\n",
    "\n",
    "# from infer_subc.quantification.stats_helpers import make_dict, multi_contact, inkeys\n",
    "from scipy.sparse._coo import coo_matrix\n",
    "from infer_subc.utils.batch import find_segmentation_tiff_files\n",
    "from datetime import datetime\n",
    "import string\n",
    "\n",
    "from skan import csr\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1ce640",
   "metadata": {},
   "source": [
    "If using the sample data, these are the correct constants\n",
    "\n",
    "Neuron 1: \n",
    "\n",
    "> ```python\n",
    "> LD_CH = 0\n",
    "> ER_CH = 1\n",
    "> GOLGI_CH = 2\n",
    "> LYSO_CH = 3\n",
    "> MITO_CH = 4\n",
    "> PEROX_CH = 5\n",
    "> ```\n",
    "\n",
    "Astrocyte: \n",
    "\n",
    "> ```python\n",
    "> LD_CH = 0\n",
    "> ER_CH = 1\n",
    "> GOLGI_CH = 2\n",
    "> LYSO_CH = 3\n",
    "> MITO_CH = 4\n",
    "> PEROX_CH = 5\n",
    "> ```\n",
    "\n",
    "Neuron 2: \n",
    "\n",
    "> ```python\n",
    "> LD_CH = 6\n",
    "> ER_CH = 0\n",
    "> GOLGI_CH = 2\n",
    "> LYSO_CH = 4\n",
    "> MITO_CH = 3\n",
    "> PEROX_CH = 1\n",
    "> ```\n",
    "\n",
    "IPSC: \n",
    "\n",
    "> ```python\n",
    "> LD_CH = 0\n",
    "> ER_CH = 6\n",
    "> GOLGI_CH = 4\n",
    "> LYSO_CH = 2\n",
    "> MITO_CH = 3\n",
    "> PEROX_CH = 5\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "733fa2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establishing the constants\n",
    "LD_CH = 0\n",
    "ER_CH = 1\n",
    "GOLGI_CH = 2\n",
    "LYSO_CH = 3\n",
    "MITO_CH = 4\n",
    "PEROX_CH = 5\n",
    "\n",
    "region_names = ['nuc', 'cell']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9935c4",
   "metadata": {},
   "source": [
    "# ***SKELETONIZATION STATS NOTEBOOK â˜ ï¸***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0212e3",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57242c02",
   "metadata": {},
   "source": [
    "## **OBJECTIVE**\n",
    "\n",
    "After quantifying the organelle and region segmentations, we can also skeletonize these segmentations for additional morphological analysis. The purpose of this notebook is to break down infer-subc's skeletonization analysis in real time, step-by-step. As quoted by Dakai Jin et al. in their work titled *Skeletonization*, â€œSkeletonization provides a simple yet compact representation of an object while capturing its essential topologic and geometric featuresâ€. It can also be used to identify and distinguish organelle shapes. Supplementary metrics, including those related to skeleton shape, length, complexity, and connectivity, are added to the quantification output. Throughout the notebook, skeletons will be analyzed in a fashion that bears some resemblance to trees or node graphs in computer science. Therefore, some of the terminology is shared with these data science concepts. However, there are various nuanced differences; some terms will have an altered definition to match the needs of infer-subc.\n",
    "\n",
    "We aim to describe in depth the morphology and distribution of the organelle skeletons in three-dimensional anisotropic data, building off of [Skan](https://skeleton-analysis.org/stable/index.html) (skeletonization python package) while also introducing new concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f1a7ce",
   "metadata": {},
   "source": [
    "## **Key Terms**\n",
    "\n",
    "**These are terms and defenitions relating to skeletonization that will be used throughout the notebook**\n",
    "\n",
    "> **Node Related Terms**\n",
    "\n",
    "`Point`\n",
    "- Any voxel/pixel in the skeleton; each point is given an id in skan.\n",
    "\n",
    "`Connectivity`\n",
    "- A value that describes the number of neighboring points a voxel/pixel has\n",
    "\n",
    "`Node`\n",
    "- A point in a skeleton where at least one branch branch begins or ends (i.e. any point that does not have a connectivity of 2)\n",
    "\n",
    "`Endpoint`\n",
    "- a type of node that is either the beginning or end of a singular branch, all endpoints have a connectivity of one\n",
    "\n",
    "`Junction Node`\n",
    "- a type of node that is present in multiple branches, functioning as the beginning and or end for each branch it is involved with. All junction nodes have connectivity of at least 3\n",
    "\n",
    "`Path Point`\n",
    "- A point that is not a node; all path points have a connectivity of two\n",
    "\n",
    "`node-id`\n",
    "- A numeric identification number given to every point in the skeleton, (path points do have node-ids)\n",
    "\n",
    "`Source Node`\n",
    "- In theory the start of a branch. In practice, this is automatically assigned to the node with the lower node ID and is not based on direction of any kind. There is no inherent difference between a source node and a destination node.\n",
    "\n",
    "`Destination Node`\n",
    "- In theory, the end of a branch. In practice this is automatically assigned to the node with the higher node ID and is not based on direction of any kind. There is no inherent difference between a source node and a destination node.\n",
    "\n",
    "> **Branch Related Terms**\n",
    "\n",
    "`Branch`\n",
    "- a direct pathway between two nodes (or the same node if the branch is a cycle)\n",
    "\n",
    "`Type 0 Branch`\n",
    "- a branch that starts at an endpoint and ends at an endpoint (Endpoint to Endpoint), and thus will be isolated\n",
    "\n",
    "`Type 1 Branch`\n",
    "- a branch with one endpoint and one junction node as the source and destination nodes regardless of order (Junction to Endpoint)\n",
    "\n",
    "`Type 2 Branch`\n",
    "- a branch that both starts and ends at junction nodes (Junction to Junction)\n",
    "\n",
    "`Type 3 Branch`\n",
    "- A branch that begins at a point and cycles back to the same point (Cycle). There are two types of Cycles: a connected cycle has a singular junction node that is shared with another branch (for a simple example, imagine a lollipop). An isolated cycle is exclusively composed of path points, thus having zero nodes.\n",
    "\n",
    "`Path`\n",
    "- A node to node pathway within a skeleton object (can be the same node). The pathway can include intermediate nodes in any order possible by the structure of the skeleton object. Thus, there is technically an infinite amount of paths within a skeleton object with multiple nodes. This also means that every branch is a path but not vice versa.\n",
    "\n",
    "`Main Path`\n",
    "- The longest direct path in a skeleton object between two nodes; there can be intermediate nodes but they must not repeat\n",
    "\n",
    "> **Skeleton Object Related Terms**\n",
    "\n",
    "`Skeleton`\n",
    "- a thinned representation of a shape that reveals equidistant lines with respect to the shape's surface\n",
    "\n",
    "`Organelle Skeleton`\n",
    "- the skeletonization of the aggregate organelle segmentation\n",
    "\n",
    "`Organelle Object`\n",
    "- a component of the organelle segmentation in which all voxels are connected via contact.\n",
    "\n",
    "`Skeleton Object`\n",
    "- The skeletonization of an organelle object (in some rare cases, an organelle object can spawn multiple non connected skeleton components, these components will still be observed as one skeleton object). Because of this there is a one to one correspondence between organelle objects and skeleton objects (skeleton object IDs are identical to the organelle object they represent).\n",
    "\n",
    "`Punctate`\n",
    "- A type of skeleton object usually representing a blob-like or spherical organelle object. Absolute Punctates fall under this category; however, a skeleton object consisting of one endpoint to endpoint (Type 0) branch can also be deemed a punctate if the branch length is under a predetermined threshold. This is due to some spherical objects returning skeletons that are not absolute punctates, but endpoint to endpoint branches of minimal length.\n",
    "\n",
    "`Absolute Punctate`\n",
    "- a type of skeleton subobject only consisting of a singular node and no branches, all absolute punctates have a connectivity of zero\n",
    "\n",
    "`Rod`\n",
    "- A type of skeleton object consisting of one endpoint to endpoint (Type 0) branch of sufficient length (longer than the punctate threshold)\n",
    "\n",
    "`Isolated Cycle`\n",
    "-  A type of skeleton object consisting of one cycle (type 3) branch \n",
    "\n",
    "`Network`\n",
    "- A type of skeleton object consisting of multiple branches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a049826",
   "metadata": {},
   "source": [
    "---\n",
    "**Assumptions**\n",
    "\n",
    "- Each organelle object has **one** skeleton object (one-to-one correspondance); in the case where a skeleton object is not created\n",
    "an absolute punctate will be placed in the center of the organelle object (this issue is only seen with blob-like objects)\n",
    "\n",
    "- Even if two organelle objects are in contact, their skeleton objects will be identified as **seperate** skeleton objects\n",
    "\n",
    "- There is **no inherit difference** between a source node and destination node !Also write about why they are labled this way!\n",
    "\n",
    "- The only circumstance of when a skeleton object will contain **zero nodes**, is in the case of an **isolated cycle** (as they will all be path points)\n",
    "\n",
    "- In the rare case that an organelle object is skeletonized and has **multiple disconnected branches**, the object will still be\n",
    "identified as a **network**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5d6a8d",
   "metadata": {},
   "source": [
    "## summary of steps\n",
    "\n",
    "âž¡ï¸ **INPUT**\n",
    "\n",
    "- setup\n",
    "\n",
    "    - choose test img to observe (test_img_n = user input)\n",
    "    - choose organelle to observe (org = user input)\n",
    "\n",
    "- loading the image and the masks\n",
    "\n",
    "- creation of the skeleton object\n",
    "\n",
    "    - collect labeled organelle segmentation\n",
    "    - define skel_plus(labeled organelle segmentation)\n",
    "    - csr.Skeleton(labeled skeleton)\n",
    "\n",
    "ðŸ“ **DEFINE MEASUREMENTS**\n",
    "\n",
    "- branch table\n",
    "- node table\n",
    "- skeleton object table\n",
    "- skeleton summary table\n",
    "\n",
    "ðŸ› ï¸ **DEFINE FUNCTIONS**\n",
    "\n",
    "- define skeleton metrics function\n",
    "- update to get_org_morphology_3D\n",
    "- update to make_all_metrics_tables\n",
    "- update to batch_process_quantification\n",
    "- update to batch_summary_stats\n",
    "\n",
    "**OUTPUT** âž¡ï¸\n",
    "\n",
    "- branch output\n",
    "    - branch table column reference\n",
    "    - branch table\n",
    "- node output\n",
    "    - node table column reference\n",
    "    - node table\n",
    "- skeleton object output\n",
    "    - skeleton object table column reference\n",
    "    - skeleton object table\n",
    "    - skeleton summary output\n",
    "    - skeleton summary table column reference\n",
    "    - skeleton summary table\n",
    "- get_org_morphology_3D output\n",
    "    - get_org_morphology_3D reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658113ba",
   "metadata": {},
   "source": [
    "The computation and analysis in this notebook was made possible by Juan Nunez-Iglesias & skan contributors via the [Skan](https://skeleton-analysis.org/stable/) python package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7d31bf",
   "metadata": {},
   "source": [
    "###### *all of the skeleton related column labels will be identical in the other stats functions e.g. make_all_metrics_tables, batch_process_quantification (batch_summary_stats will just include summary metrics of these stats)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cebf63",
   "metadata": {},
   "source": [
    "## **INPUT**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aa124f",
   "metadata": {},
   "source": [
    "### **Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb32697",
   "metadata": {},
   "source": [
    "#### ðŸ›‘ âœ **User Input Required**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c741b095",
   "metadata": {},
   "source": [
    "Please specify the following information about your data: `raw_img_type`, `data_root_path`, `raw_data_path`, `seg_data_path`, and `quant_data_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e70036a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organelle to observe\n",
    "org = ER_CH\n",
    "\n",
    "### USER INPUT REQUIRED ###\n",
    "# If using the sample data, select which cell type you would like analyze (\"neuron_1\", \"astrocyte\", \"neuron_2\" or \"ipsc\"):\n",
    "# If not using the sample data, set sample_data_type to None\n",
    "sample_data_type = \"astrocyte\"\n",
    "\n",
    "# If you are not using the sample data, please edit \"USER SPECIFIED\" as necessary.\n",
    "\n",
    "## Define the path to the directory that contains the input image folder.\n",
    "data_root_path = Path(\"USER SPECIFIED\")\n",
    "\n",
    "# Specify the file type of your raw data that will be analyzed. Ex) \".czi\" or \".tiff\"\n",
    "raw_img_type = \"USER SPECIFIED\"\n",
    "\n",
    "## Specify which subfolder that contains the input data and the input data file extension\n",
    "raw_data_path = data_root_path / \"USER SPECIFIED\"\n",
    "\n",
    "## Specify which subfolder that contains the segmentations\n",
    "seg_data_path = data_root_path / \"USER SPECIFIED\"\n",
    "\n",
    "## Specify the output folder to save the quantification outputs if.\n",
    "## If its not already created, the code below will create it for you\n",
    "quant_data_path = data_root_path / \"USER SPECIFIED\"\n",
    "                \n",
    "# These are the organelles\n",
    "org_list = ['LD', 'ER', 'golgi', 'lyso', 'mito', 'perox']\n",
    "org_list2 = ['Lipid Droplets',\n",
    "            'Endoplasmic Reticulum',\n",
    "            'Golgi Apparatus',\n",
    "            'Lysosomes',\n",
    "            'Mitochondria',\n",
    "            'Peroxisomes']\n",
    "\n",
    "# Organelle name\n",
    "org_name = org_list[org]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d81e027",
   "metadata": {},
   "source": [
    "#### ðŸƒ **Run code; no user input required**\n",
    "\n",
    "ðŸ‘“ **FYI**:\n",
    "\n",
    "- A list of the images included in the raw_data_path folder is printed below for easy reference.\n",
    "- If the quant_data_path folder does not exist, it will be created now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd75b80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the sake of remaining within the notebook this function will be placed here for now\n",
    "\n",
    "def sample_input_quant(cell_type: Union[str, None]) -> tuple[Path, str, Path, Path, Path]:\n",
    "    \"\"\"\n",
    "    automatically sets the necessary paths for sample data if cell_type in the quantification\n",
    "    notebook is set equal to \"neuron_1\", \"astrocyte\", \"neuron_2\" or \"ipsc\".\n",
    "    \"\"\"\n",
    "    cell_type_list = [\"neuron_1\",\"astrocyte\",\"neuron_2\",\"ipsc\"]\n",
    "    \n",
    "    if cell_type in cell_type_list:\n",
    "\n",
    "        sd_fol = Path(os.getcwd()).parents[1] / \"sample_data\"\n",
    "\n",
    "        data_root_path = sd_fol /  f\"example_{cell_type}\"\n",
    "\n",
    "        # Specify the file type of the sample data\n",
    "        raw_img_type = \".tiff\"\n",
    "\n",
    "        ## Specify which subfolder that contains the input data and the input data file extension\n",
    "        raw_data_path = data_root_path / \"raw\"\n",
    "\n",
    "        ## Specify the location of the segmentations.\n",
    "        seg_data_path = sd_fol / \"example_quant\" / \"seg\"\n",
    "\n",
    "        # Where to output the quantification\n",
    "        quant_data_path = sd_fol / \"example_quant\" / \"quant\"\n",
    "\n",
    "        return data_root_path, raw_img_type, raw_data_path, seg_data_path, quant_data_path\n",
    "    else:\n",
    "        raise ValueError('Sample data file type must be \"neuron_1\", \"astrocyte\", \"neuron_2\" or \"ipsc\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cb2f634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c:\\Users\\redre\\Documents\\CohenLab\\scohen_lab_repo\\infer-subc\\sample_data\\example_astrocyte\\raw\\05052022_astro_control_2_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                Image Name\n",
       "0  c:\\Users\\redre\\Documents\\CohenLab\\scohen_lab_repo\\infer-subc\\sample_data\\example_astrocyte\\raw\\05052022_astro_control_2_Linear unmixing_0_cmle.ome.tiff"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If sample_data_type is set to \"neuron_1\", \"astrocyte\", \"neuron_2\" or \"ipsc\" then the sample data is used and the directories are set\n",
    "if sample_data_type != None:\n",
    "    data_root_path, raw_img_type, raw_data_path, seg_data_path, quant_data_path = sample_input_quant(sample_data_type)\n",
    "\n",
    "# Create the output directory to save the segmentation outputs in.\n",
    "if not Path.exists(quant_data_path):\n",
    "    Path.mkdir(quant_data_path)\n",
    "    print(f\"making {quant_data_path}\")\n",
    "\n",
    "# Create a list of the file paths for each image in the input folder. Select test image path.\n",
    "raw_img_file_list = list_image_files(raw_data_path,raw_img_type)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.DataFrame({\"Image Name\":raw_img_file_list})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf82ecf",
   "metadata": {},
   "source": [
    "#### ðŸ›‘ âœ **User Input Required**:\n",
    "Use the list above to specify which image you wish to analyze:\n",
    "\n",
    "- `test_img_n`: the index, or number, associated with your image of choice from the list above.\n",
    "\n",
    "Follow this example's formatting\n",
    "\n",
    "> ```python\n",
    "> test_img_n = 5\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f653198",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### USER INPUT REQUIRED ###\n",
    "test_img_n = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048edfcb",
   "metadata": {},
   "source": [
    "### **Loading the image and masks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e7012e",
   "metadata": {},
   "source": [
    "#### ðŸƒ **Run code; no user input required**\n",
    "ðŸ‘“ FYI: This code block reads the image and image metadata into memory. Then, the metadata is printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4acf587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata information\n",
      "File path: c:\\Users\\redre\\Documents\\CohenLab\\scohen_lab_repo\\infer-subc\\sample_data\\example_astrocyte\\raw\\05052022_astro_control_2_Linear unmixing_0_cmle.ome.tiff\n",
      "Channel 0 name: 05052022_astro_control_2_Linear unmixing_0_cmle.ome :: Channel:0\n",
      "Channel 1 name: 05052022_astro_control_2_Linear unmixing_0_cmle.ome :: Channel:1\n",
      "Channel 2 name: 05052022_astro_control_2_Linear unmixing_0_cmle.ome :: Channel:2\n",
      "Channel 3 name: 05052022_astro_control_2_Linear unmixing_0_cmle.ome :: Channel:3\n",
      "Channel 4 name: 05052022_astro_control_2_Linear unmixing_0_cmle.ome :: Channel:4\n",
      "Channel 5 name: 05052022_astro_control_2_Linear unmixing_0_cmle.ome :: Channel:5\n",
      "Scale (ZYX): (0.396091, 0.079947, 0.079947)\n",
      "Channel axis: 0\n"
     ]
    }
   ],
   "source": [
    "# Read in the image and metadata as an ndarray and dictionary from the test image selected above. \n",
    "test_img_name = raw_img_file_list[test_img_n]\n",
    "img_data,meta_dict = read_czi_image(test_img_name)\n",
    "\n",
    "# Define some of the metadata features.\n",
    "channel_names = meta_dict['name']\n",
    "meta = meta_dict['metadata']['aicsimage']\n",
    "scale = meta_dict['scale']\n",
    "channel_axis = meta_dict['channel_axis']\n",
    "file_path = meta_dict['file_name']\n",
    "\n",
    "print(\"Metadata information\")\n",
    "print(f\"File path: {file_path}\")\n",
    "for i in list(range(len(channel_names))):\n",
    "    print(f\"Channel {i} name: {channel_names[i]}\")\n",
    "print(f\"Scale (ZYX): {scale}\")\n",
    "print(f\"Channel axis: {channel_axis}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20c6840a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### USER INPUT REQUIRED ###\n",
    "# These two lists must have the SAME corresponding items in the same order\n",
    "org_file_names = ['LD', 'ER', 'golgi', 'lyso', 'mito', 'perox']\n",
    "org_channels_ordered = [LD_CH, ER_CH, GOLGI_CH, LYSO_CH, MITO_CH, PEROX_CH]\n",
    "\n",
    "regions_file_names = [\"nuc\", \"cell\"]\n",
    "mask_name = \"cell\"\n",
    "suffix_separator = \"-\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72f4038",
   "metadata": {},
   "source": [
    "#### &#x1F3C3; **Run code; no user input required**\n",
    "\n",
    "&#x1F453; **FYI:** This code finds the matching segmentation file from the `seg_file_path` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbf69825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following matching files were found:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'raw': WindowsPath('c:/Users/redre/Documents/CohenLab/scohen_lab_repo/infer-subc/sample_data/example_astrocyte/raw/05052022_astro_control_2_Linear unmixing_0_cmle.ome.tiff'),\n",
       " 'LD': WindowsPath('c:/Users/redre/Documents/CohenLab/scohen_lab_repo/infer-subc/sample_data/example_quant/seg/05052022_astro_control_2_Linear unmixing_0_cmle.ome-LD.tiff'),\n",
       " 'ER': WindowsPath('c:/Users/redre/Documents/CohenLab/scohen_lab_repo/infer-subc/sample_data/example_quant/seg/05052022_astro_control_2_Linear unmixing_0_cmle.ome-ER.tiff'),\n",
       " 'golgi': WindowsPath('c:/Users/redre/Documents/CohenLab/scohen_lab_repo/infer-subc/sample_data/example_quant/seg/05052022_astro_control_2_Linear unmixing_0_cmle.ome-golgi.tiff'),\n",
       " 'lyso': WindowsPath('c:/Users/redre/Documents/CohenLab/scohen_lab_repo/infer-subc/sample_data/example_quant/seg/05052022_astro_control_2_Linear unmixing_0_cmle.ome-lyso.tiff'),\n",
       " 'mito': WindowsPath('c:/Users/redre/Documents/CohenLab/scohen_lab_repo/infer-subc/sample_data/example_quant/seg/05052022_astro_control_2_Linear unmixing_0_cmle.ome-mito.tiff'),\n",
       " 'perox': WindowsPath('c:/Users/redre/Documents/CohenLab/scohen_lab_repo/infer-subc/sample_data/example_quant/seg/05052022_astro_control_2_Linear unmixing_0_cmle.ome-perox.tiff'),\n",
       " 'nuc': WindowsPath('c:/Users/redre/Documents/CohenLab/scohen_lab_repo/infer-subc/sample_data/example_quant/seg/05052022_astro_control_2_Linear unmixing_0_cmle.ome-nuc.tiff'),\n",
       " 'cell': WindowsPath('c:/Users/redre/Documents/CohenLab/scohen_lab_repo/infer-subc/sample_data/example_quant/seg/05052022_astro_control_2_Linear unmixing_0_cmle.ome-cell.tiff')}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find file paths for segmentations\n",
    "all_suffixes = org_file_names + regions_file_names\n",
    "filez = find_segmentation_tiff_files(file_path, all_suffixes, seg_data_path, suffix_separator)\n",
    "\n",
    "# read the segmentation and masks/regions files into memory\n",
    "organelles = [read_tiff_image(filez[org]) for org in org_file_names]\n",
    "\n",
    "regions = [read_tiff_image(filez[m]) for m in regions_file_names]\n",
    "\n",
    "# match the intensity channels to the segmentation files\n",
    "intensities = [img_data[ch] for ch in org_channels_ordered]\n",
    "\n",
    "# specifiy the mask image\n",
    "m = regions_file_names.index(mask_name)\n",
    "mask = regions[m]\n",
    "\n",
    "# print paths to matching seg files\n",
    "print(\"The following matching files were found:\")\n",
    "filez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b1ddbf",
   "metadata": {},
   "source": [
    "###### The dimensions of the image in the real world will be taken into account as visualization more effectively depicts the cell as it was when the images were taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac2238f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is used to get the real world dimensions of the data !change name of dim variable for consistency!\n",
    "scale = meta_dict['scale']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c5f8d7",
   "metadata": {},
   "source": [
    "## **Creation of the skeleton object**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "030234c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the organelle segmentation\n",
    "org_seg = organelles[org_channels_ordered.index(org)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04f57650",
   "metadata": {},
   "outputs": [],
   "source": [
    "if org == ER_CH:\n",
    "    org_seg = (org_seg > 0).astype(np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d30a19c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skel_plus(segmentation: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    ''' A function that generates punctate objects for the round organelle objects that lack a skeleton.\n",
    "\n",
    "    As of 7/7/24 the original `skeletonize()` function from `skimage` labels the voxels of the skeleton with the same label,\n",
    "    however the label serves no purpose to us as of now. \n",
    "    The purpose of this function is to: \n",
    "\n",
    "    1) Establish punctate objects in the center of organelle objects that for some reason lack a skeleton object (if necessary)\n",
    "    * these objects are found to be round/spherical in every case this happens, so a punctate is appropriate\n",
    "    2) Return a skeleton with float labels (due to skan requiring this) corresponding to its location in the original segmentation \n",
    "    3) Return a boolean skeleton for input in computation\n",
    "    '''\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # This is the raw organelle skeleton, some fixing and relabeling has to be done before we can use the skeleton for computation\n",
    "    skeleton = skeletonize(segmentation.astype(bool)).astype(bool)\n",
    "\n",
    "    # All of the organelle object labels\n",
    "    all_lab = set(pd.unique(segmentation.ravel()))\n",
    "\n",
    "    # Applying the segmentation labels to the skeleton\n",
    "    lab_skel = _assert_uint16_labels(skeleton * segmentation)\n",
    "\n",
    "    # Labels present in the skeleton\n",
    "    skel_lab = set(pd.unique(lab_skel.ravel()))\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    print(f'Part One took {t1 - t0} sec(s)')\n",
    "\n",
    "    # Checker to see if there are any objects without a skeleton\n",
    "    if all_lab == skel_lab:\n",
    "\n",
    "        t2 = time.time()\n",
    "        print(f'Total Time: {t2 - t0} sec(s)')\n",
    "\n",
    "        return lab_skel.astype(float), skeleton\n",
    "    else:\n",
    "        # gets a list of the missing labels\n",
    "        mis_lab = all_lab - skel_lab\n",
    "\n",
    "        t2 = time.time()\n",
    "        print(f'Part Two took {t2 - t1} sec(s)')\n",
    "\n",
    "        for label in mis_lab:\n",
    "            # list of coordinates of the object's voxels\n",
    "            coord_list = np.nonzero(segmentation == label)\n",
    "\n",
    "            # The coordinate closest to the middle of the object (due to rounding)\n",
    "            av_coord = np.round(np.mean(coord_list,axis = 1)).astype(int)\n",
    "\n",
    "            #checker and result\n",
    "            if segmentation[tuple(av_coord)] == label:\n",
    "                lab_skel[tuple(av_coord)] = label\n",
    "            else:\n",
    "                print(\"Apperently the centermost point is not in the object???? :(\")\n",
    "                break\n",
    "\n",
    "        t3 = time.time()\n",
    "        print(f'Part Three took {t3 - t2} sec(s)')\n",
    "        print(f'Total time: {t3 - t0}')\n",
    "        \n",
    "        return (lab_skel.astype(float), lab_skel.astype(bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c1f8349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part One took 5.041286945343018 sec(s)\n",
      "Total Time: 5.041286945343018 sec(s)\n"
     ]
    }
   ],
   "source": [
    "org_skel_arr, org_skel_arr2 = skel_plus(org_seg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cb37c5",
   "metadata": {},
   "source": [
    "###### The computations will be done using the object below of the skeleton class via Skan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4b3b9c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Image layer 'org_seg' at 0x12b3dc799f0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import napari\n",
    "viewer = napari.Viewer()\n",
    "viewer.add_image(org_seg,\n",
    "                 scale = scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "736e8868",
   "metadata": {},
   "outputs": [],
   "source": [
    "org_skel = csr.Skeleton(\n",
    "    skeleton_image = org_skel_arr,\n",
    "    spacing = scale,\n",
    "    value_is_height = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e43b194",
   "metadata": {},
   "source": [
    "## **DEFINE MEASUREMENTS**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f09204",
   "metadata": {},
   "source": [
    "### **Branch Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2ec0a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a modified version of the summarize function optimized for the workflow\n",
    "\n",
    "summary = {}\n",
    "value_is_height = False\n",
    "ndim = org_skel.coordinates.shape[1]\n",
    "### using this .indptr method I could single out the nodes even faster\n",
    "endpoints_src = org_skel.paths.indices[org_skel.paths.indptr[:-1]]\n",
    "endpoints_dst = org_skel.paths.indices[org_skel.paths.indptr[1:] - 1]\n",
    "\n",
    "# Because the values of the path points are the organelle object ids (all path points and nodes \n",
    "# in a branch should come from the same organelle object)\n",
    "if not np.any(org_skel.path_stdev()):\n",
    "    # checker to see if all path points and nodes come from the same object\n",
    "    summary['skel-obj-id'] = org_skel.path_means().astype(int)\n",
    "else:\n",
    "    raise ValueError(\"at least one branch came from different organelle objects\")\n",
    "summary['node-id-src'] = endpoints_src\n",
    "summary['node-id-dst'] = endpoints_dst\n",
    "deg_src = org_skel.degrees[endpoints_src]\n",
    "deg_dst = org_skel.degrees[endpoints_dst]\n",
    "summary['deg_src'] = deg_src\n",
    "summary['deg_dst'] = deg_dst\n",
    "summary['branch-distance'] = org_skel.path_lengths()\n",
    "kind = np.full(deg_src.shape, 2)  # default: junction-to-junction\n",
    "kind[(deg_src == 1) | (deg_dst == 1)] = 1  # tip-junction\n",
    "kind[(deg_src == 1) & (deg_dst == 1)] = 0  # tip-tip\n",
    "kind[endpoints_src == endpoints_dst] = 3  # cycle\n",
    "summary['branch-type'] = kind\n",
    "for i in range(ndim):  # keep loops separate for best insertion order\n",
    "    summary[f'image-coord-src-{i}'] = org_skel.coordinates[endpoints_src, i]\n",
    "for i in range(ndim):\n",
    "    summary[f'image-coord-dst-{i}'] = org_skel.coordinates[endpoints_dst, i]\n",
    "coords_real_src = org_skel.coordinates[endpoints_src] * org_skel.spacing\n",
    "for i in range(ndim):\n",
    "    summary[f'coord-src-{i}'] = coords_real_src[:, i]\n",
    "coords_real_dst = org_skel.coordinates[endpoints_dst] * org_skel.spacing\n",
    "for i in range(ndim):\n",
    "    summary[f'coord-dst-{i}'] = coords_real_dst[:, i]\n",
    "summary['euclidean-distance'] = (\n",
    "        np.sqrt((coords_real_dst - coords_real_src)**2\n",
    "                @ np.ones(ndim + int(value_is_height)))\n",
    "        )\n",
    "\n",
    "summary['str-prop'] = summary['euclidean-distance'] / summary['branch-distance']\n",
    "branch_table = pd.DataFrame(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458b650f",
   "metadata": {},
   "source": [
    "### **Node Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2cb38e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary that reveals the branches a node is referenced in\n",
    "# If a node is an absolute punctate it will return \"NaN\"\n",
    "node2branches = dict()\n",
    "node2obj = dict()\n",
    "\n",
    "for branch in range(org_skel.n_paths):\n",
    "    obj = branch_table.loc[branch]['skel-obj-id']\n",
    "    for point in org_skel.path(branch):\n",
    "        if org_skel.degrees[point] != 2:\n",
    "            node2obj[point] = obj\n",
    "            try:\n",
    "                node2branches[point] += [branch]\n",
    "            except:\n",
    "                node2branches[point] = [branch]\n",
    "\n",
    "# the point ids for the points with zero connectivity\n",
    "abs_punc_ids = np.arange(len(org_skel.degrees))[org_skel.degrees == 0]\n",
    "abs_punc_obj = (org_skel_arr[org_skel_arr2][abs_punc_ids]).astype(int)\n",
    "\n",
    "for i,punc_id in enumerate(abs_punc_ids):\n",
    "    node2branches[punc_id] = []\n",
    "    node2obj[punc_id] = abs_punc_obj[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2682e95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = list(node2branches)\n",
    "\n",
    "node_lab = []\n",
    "base_n = [\"Abs Punctate\",\n",
    "        \"Endpoint\",\n",
    "        \"Path Point\"]\n",
    "\n",
    "if np.max(org_skel.degrees) <= 2:\n",
    "    node_lab = base_n[0:len(pd.unique(org_skel.degrees))]\n",
    "else:\n",
    "    node_lab = base_n\n",
    "    for i in np.arange(3, np.max(org_skel.degrees) + 1):\n",
    "        node_lab += [f\"{i}-way\"]\n",
    "    \n",
    "node_table_data = {\n",
    "    \"node-id\": nodes,\n",
    "    \"node-type\": np.array(node_lab)[org_skel.degrees[nodes]],\n",
    "    \"connectivity\": org_skel.degrees[nodes],\n",
    "    \"image-coord-0\": org_skel.coordinates[nodes,0],\n",
    "    \"image-coord-1\": org_skel.coordinates[nodes,1],\n",
    "    \"image-coord-2\": org_skel.coordinates[nodes,2],\n",
    "    \"coord-0\": org_skel.coordinates[nodes,0] * scale[0],\n",
    "    \"coord-1\": org_skel.coordinates[nodes,1] * scale[1],\n",
    "    \"coord-2\": org_skel.coordinates[nodes,2] * scale[2],\n",
    "    \"branch-id(s)\": [node2branches[key] for key in nodes],\n",
    "    'skel-obj-id': [int(node2obj[key]) for key in nodes]\n",
    "}\n",
    "\n",
    "node_table = pd.DataFrame(data=node_table_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218339c0",
   "metadata": {},
   "source": [
    "### **Skeleton Object Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e08284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to prevent the workflow acting as if the absolute punctates are their own skeleton objects\n",
    "if org_name != 'ER':\n",
    "    obj_list = list(branch_table.groupby('skel-obj-id').groups) + list(abs_punc_obj)\n",
    "else:\n",
    "    obj_list = list(branch_table.groupby('skel-obj-id').groups)\n",
    "\n",
    "\n",
    "#### This code was originally used to find the main path within a skeleton object, but this was before the one to one correspondence requirement.\n",
    "#### Because of this a skeleton object can consist of multiple disconnected components which complicates the code, so for now main paths will\n",
    "#### For now I will also be removing main paths from the visual section because for cells with a huge amount of branches it makes the workflow\n",
    "#### take longer\n",
    "\n",
    "# main_branches = branch_table[branch_table['in-main'] == True]\n",
    "\n",
    "# main_nodes_1 = dict(main_branches[main_branches['src-deg'] == 1].groupby('skeleton-id').groups)\n",
    "# main_nodes_2 = dict(main_branches[main_branches['dst-deg'] == 1].groupby('skeleton-id').groups)\n",
    "\n",
    "\n",
    "# main_nodes = dict((obj,[]) for obj in obj_list)\n",
    "\n",
    "# for obj in obj_list:\n",
    "#     main_nodes[obj] = []\n",
    "#     try:\n",
    "#         offset1 = list(main_nodes_1[obj])\n",
    "#         for off in offset1:\n",
    "#             main_nodes[obj] += [tuple((\n",
    "#                 main_branches['node-id-src'][off],\n",
    "#                 main_branches['src-deg'][off]))]\n",
    "#     except:\n",
    "#         0\n",
    "#     try:\n",
    "#         offset2 = list(main_nodes_2[obj])\n",
    "#         for off in offset2:\n",
    "#             main_nodes[obj] += [tuple((\n",
    "#                 main_branches['node-id-dst'][off],\n",
    "#                 main_branches['dst-deg'][off]))]\n",
    "#     except:\n",
    "#         0\n",
    "#     # Added this because of isolated cycles that are connected to other branches\n",
    "#     if len(main_nodes[obj]) < 2:\n",
    "#         try:\n",
    "#             offset = list(main_nodes_2[obj])\n",
    "#             main_nodes[obj] += [tuple((\n",
    "#                 int(main_branches['node-id-src'][offset].iloc[0]),\n",
    "#                 int(main_branches['src-deg'][offset].iloc[0])))]\n",
    "#         except:\n",
    "#             0\n",
    "#         try:\n",
    "#             offset = list(main_nodes_1[obj])\n",
    "#             main_nodes[obj] += [tuple((\n",
    "#                 int(main_branches['node-id-dst'][offset].iloc[0]),\n",
    "#                 int(main_branches['dst-deg'][offset].iloc[0])))]\n",
    "#         except:\n",
    "#             0\n",
    "#     main_nodes[obj] = sorted(main_nodes[obj])\n",
    "#     # in the case of a isolated cycle that is its own object\n",
    "#     if len(main_nodes[obj]) < 2:\n",
    "#         main_nodes[obj] = list(main_nodes[obj]) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6083cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the highest value of length where a type 0 branch will still be considered punctate\n",
    "p_threshold = min(scale) * 2\n",
    "\n",
    "#dictionary that keeps track of the number of branches per each object\n",
    "objfreq = dict(branch_table.groupby(\"skel-obj-id\").count()['branch-distance'])\n",
    "#dictionary that keeps track of the total length per object\n",
    "sumobj = dict(branch_table.groupby(\"skel-obj-id\").sum()['branch-distance'])\n",
    "#dictonary that keeps of the average branch length per object\n",
    "aveobj = dict(branch_table.groupby(\"skel-obj-id\").mean()['branch-distance'])\n",
    "\n",
    "obj_class = dict()\n",
    "obj_class_n = dict()\n",
    "\n",
    "for obj in objfreq:\n",
    "    if objfreq[obj] == 1:\n",
    "        if sumobj[obj] > p_threshold:\n",
    "            obj_class[obj] = \"Rod\"\n",
    "            obj_class_n[obj] = 1\n",
    "        else:\n",
    "            obj_class[obj] = \"Punctate\"\n",
    "            obj_class_n[obj] = 0\n",
    "    else:\n",
    "        obj_class[obj] = \"Network\"\n",
    "        obj_class_n[obj] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d896c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for some reason this was faster\n",
    "skel_obj2branch = dict()\n",
    "skel_obj2_mean_str = dict(branch_table.groupby('skel-obj-id').mean()['str-prop'])\n",
    "\n",
    "for branch in range(org_skel.n_paths):\n",
    "    obj = branch_table['skel-obj-id'][branch]\n",
    "    try:\n",
    "      skel_obj2branch[obj] += [branch]\n",
    "    except:\n",
    "       skel_obj2branch[obj] = [branch]\n",
    "\n",
    "if org_name != 'ER':\n",
    "  for obj in abs_punc_obj:\n",
    "    skel_obj2branch[obj] = []\n",
    "    obj_class[obj] = \"Punctate\"\n",
    "    obj_class_n[obj] = 0\n",
    "    skel_obj2_mean_str[obj] = \"NaN\"\n",
    "    objfreq[obj] = 0\n",
    "    aveobj[obj] = 0\n",
    "    sumobj[obj] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ddce637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "brh_tcounts = dict()\n",
    "brh_tlist = dict()\n",
    "for i in range(4):\n",
    "    brh_tcounts[i] = dict((obj,0) for obj in obj_list)\n",
    "    brh_tlist[i] = dict((obj,[]) for obj in obj_list)\n",
    "\n",
    "branch_type_tables = [branch_table[np.array(branch_table[\"branch-type\"]) == i] for i in np.arange(4)]\n",
    "\n",
    "for i in range(4):\n",
    "    groupedby_skel = branch_type_tables[i].groupby('skel-obj-id')\n",
    "    for skel_obj in obj_list:\n",
    "        try:\n",
    "            # Although they get the branch row indexes this is fine because the branch row indexes\n",
    "            # are the branch ids :)\n",
    "            type_list = list(groupedby_skel.groups[skel_obj])\n",
    "            brh_tlist[i][skel_obj] = type_list\n",
    "            brh_tcounts[i][skel_obj] = len(type_list)\n",
    "        except:\n",
    "            None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "289b5e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to find the highest node degree in the skeleton object\n",
    "max_deg = dict([(obj,0) for obj in obj_list])\n",
    "\n",
    "obj2brh = branch_table.groupby('skel-obj-id').groups\n",
    "for obj in obj2brh:\n",
    "    for branch in obj2brh[obj]:\n",
    "        brh_max = np.max(org_skel.degrees[org_skel.path(branch)])\n",
    "        if brh_max > max_deg[obj]:\n",
    "            max_deg[obj] = brh_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0fe47deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "skel_obj2nodes = dict([(obj,[]) for obj in obj_list])\n",
    "ep_count = dict([(obj,0) for obj in obj_list])\n",
    "jn_count = dict([(obj,0) for obj in obj_list])\n",
    "jn_table = node_table[node_table['connectivity'] > 2]\n",
    "ave_jn_deg = dict([(obj,\"NaN\") for obj in obj_list])\n",
    "\n",
    "for obj in obj_list:\n",
    "    try:\n",
    "        skel_obj2nodes[obj] = list(node_table['node-id'][node_table.groupby(\"skel-obj-id\").groups[obj]])\n",
    "    except:\n",
    "        None\n",
    "    try:\n",
    "        ep_count[obj] = node_table[node_table['connectivity'] == 1].groupby('skel-obj-id').count()['connectivity'][obj]\n",
    "    except:\n",
    "        None\n",
    "    try:\n",
    "        jn_count[obj] = node_table[node_table['connectivity'] > 2].groupby('skel-obj-id').count()['connectivity'][obj]\n",
    "        if jn_count[obj] > 0:\n",
    "            ave_jn_deg[obj] = np.mean(jn_table['connectivity'][jn_table.groupby('skel-obj-id').groups[obj]])\n",
    "    except:\n",
    "        None\n",
    "        \n",
    "skel_table_data = {\n",
    "        \"skel-obj-id\": obj_list,\n",
    "        \"skel-type\": [obj_class[obj] for obj in obj_list],\n",
    "        \"skel-type-num\": [obj_class_n[obj] for obj in obj_list],\n",
    "        \"brh-count\": [objfreq[obj] for obj in obj_list],\n",
    "        \"branch-id(s)\": [skel_obj2branch[obj] for obj in obj_list],\n",
    "        \"min-brh-length\": [branch_table.groupby('skel-obj-id').min()['branch-distance'][obj] if objfreq[obj] != 0 else \"NaN\" for obj in obj_list],\n",
    "        \"max-brh-length\": [branch_table.groupby('skel-obj-id').max()['branch-distance'][obj] if objfreq[obj] != 0 else \"NaN\" for obj in obj_list],\n",
    "        \"ave-brh-length\": [aveobj[obj] for obj in obj_list],\n",
    "        # the requirement is changed to > 1 because the sd of a single value is undefined\n",
    "        \"sd-brh-length\": [branch_table.groupby('skel-obj-id').std()['branch-distance'][obj] if objfreq[obj] > 1 else \"NaN\" for obj in obj_list],\n",
    "        \"med-brh-length\" : [branch_table.groupby('skel-obj-id').median()['branch-distance'][obj] if objfreq[obj] != 0 else \"NaN\" for obj in obj_list],\n",
    "        \"total-length\": [sumobj[obj] for obj in obj_list],\n",
    "        \"brh-type-0-tot\": [brh_tcounts[0][obj] for obj in obj_list],\n",
    "        \"brh-type-0-id\": [brh_tlist[0][obj] for obj in obj_list],\n",
    "        \"brh-type-1-tot\": [brh_tcounts[1][obj] for obj in obj_list],\n",
    "        \"brh-type-1-ids\": [brh_tlist[1][obj] for obj in obj_list],\n",
    "        \"brh-type-2-tot\": [brh_tcounts[2][obj] for obj in obj_list],\n",
    "        \"brh-type-2-ids\": [brh_tlist[2][obj] for obj in obj_list],\n",
    "        \"brh-type-3-tot\": [brh_tcounts[3][obj] for obj in obj_list],\n",
    "        \"brh-type-3-ids\": [brh_tlist[3][obj] for obj in obj_list],\n",
    "        \"node-count\": [len(skel_obj2nodes[obj]) for obj in obj_list],\n",
    "        'ep-count': [ep_count[obj] for obj in obj_list],\n",
    "        'jn-count' : [jn_count[obj] for obj in obj_list],\n",
    "        'ave-jn-deg' : [ave_jn_deg[obj] for obj in obj_list],\n",
    "        'max-deg' : [max_deg[obj] for obj in obj_list],\n",
    "        \"node-id(s)\": [skel_obj2nodes[obj] for obj in obj_list],\n",
    "        \"mean-brh-str\": [skel_obj2_mean_str[obj] for obj in obj_list]}\n",
    "    \n",
    "\n",
    "skel_table = pd.DataFrame(data=skel_table_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d627f30e",
   "metadata": {},
   "source": [
    "### **Skeleton Summary Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c359a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "skel_sum_table_data = {\n",
    "\n",
    "    'total-length': np.sum(org_skel.path_lengths()),\n",
    "    'point-count': org_skel.graph.shape[0],\n",
    "    # -- Skeleton Object section -- #\n",
    "\n",
    "    'skel-obj-count': len(skel_table),\n",
    "    'punc-count': np.sum(skel_table['skel-type-num'] == 0),\n",
    "    'rod-count': np.sum(skel_table['skel-type-num'] == 1),\n",
    "    'net-count': np.sum(skel_table['skel-type-num'] == 2),\n",
    "    'prop-obj-punc': np.sum(skel_table['skel-type-num'] == 0) / len(skel_table),\n",
    "    'prop-obj-rod': np.sum(skel_table['skel-type-num'] == 1) / len(skel_table),\n",
    "    'prop-obj-net': np.sum(skel_table['skel-type-num'] == 2) / len(skel_table),\n",
    "    'punc-tot-len': np.sum(skel_table[skel_table['skel-type-num'] == 0]['total-length']),\n",
    "    'rod-tot-len': np.sum(skel_table[skel_table['skel-type-num'] == 1]['total-length']),\n",
    "    'net-tot-len': np.sum(skel_table[skel_table['skel-type-num'] == 2]['total-length']),\n",
    "    'prop-len-punc': np.sum(skel_table[skel_table['skel-type-num'] == 0]['total-length']) / np.sum(org_skel.path_lengths()),\n",
    "    'prop-len-rod': np.sum(skel_table[skel_table['skel-type-num'] == 1]['total-length']) / np.sum(org_skel.path_lengths()),\n",
    "    'prop-len-net': np.sum(skel_table[skel_table['skel-type-num'] == 2]['total-length']) / np.sum(org_skel.path_lengths()),\n",
    "    'ave-len-obj': np.mean(skel_table['total-length']),\n",
    "    'min-len-obj': np.min(skel_table['total-length']),\n",
    "    'max-len-obj': np.max(skel_table['total-length']),\n",
    "    'ave-brh-obj': np.mean(skel_table['brh-count']),\n",
    "    'min-brh-obj': np.min(skel_table['brh-count']),\n",
    "    'max-brh-obj': np.max(skel_table['brh-count']),\n",
    "\n",
    "    # -- Branch section -- #\n",
    "\n",
    "    'brh-count': org_skel.n_paths,\n",
    "    'min-brh-len': np.min(org_skel.path_lengths()),\n",
    "    'max-brh-len': np.max(org_skel.path_lengths()),\n",
    "    'ave-brh-len': np.mean(org_skel.path_lengths()),\n",
    "    'type-0-brhs': len(branch_table[branch_table['branch-type'] == 0]),\n",
    "    'type-1-brhs': len(branch_table[branch_table['branch-type'] == 1]),\n",
    "    'type-2-brhs': len(branch_table[branch_table['branch-type'] == 2]),\n",
    "    'type-3-brhs': len(branch_table[branch_table['branch-type'] == 3]),\n",
    "    'prop-brh-t0': len(branch_table[branch_table['branch-type'] == 0]) / len(branch_table),\n",
    "    'prop-brh-t1': len(branch_table[branch_table['branch-type'] == 1]) / len(branch_table),\n",
    "    'prop-brh-t2': len(branch_table[branch_table['branch-type'] == 2]) / len(branch_table),\n",
    "    'prop-brh-t3': len(branch_table[branch_table['branch-type'] == 3]) / len(branch_table),\n",
    "    't0-brh-len': np.sum(org_skel.path_lengths()[branch_table['branch-type'] == 0]),\n",
    "    't1-brh-len': np.sum(org_skel.path_lengths()[branch_table['branch-type'] == 1]),\n",
    "    't2-brh-len': np.sum(org_skel.path_lengths()[branch_table['branch-type'] == 2]),\n",
    "    't3-brh-len': np.sum(org_skel.path_lengths()[branch_table['branch-type'] == 3]),\n",
    "    'prop-len-t0': np.sum(org_skel.path_lengths()[branch_table['branch-type'] == 0]) / np.sum(org_skel.path_lengths()),\n",
    "    'prop-len-t1': np.sum(org_skel.path_lengths()[branch_table['branch-type'] == 1]) / np.sum(org_skel.path_lengths()),\n",
    "    'prop-len-t2': np.sum(org_skel.path_lengths()[branch_table['branch-type'] == 2]) / np.sum(org_skel.path_lengths()),\n",
    "    'prop-len-t3': np.sum(org_skel.path_lengths()[branch_table['branch-type'] == 3]) / np.sum(org_skel.path_lengths()),\n",
    "    \n",
    "    # -- Node section -- #\n",
    "\n",
    "    'node-count': len(node_table),\n",
    "    'ave-deg-nodes': np.mean(node_table['connectivity']),\n",
    "    'ep-count': np.sum(node_table['connectivity'] == 1),\n",
    "    'jn-count': np.sum(node_table['connectivity'] > 2),\n",
    "    'ap-count': np.sum(node_table['connectivity'] == 0),\n",
    "    'prop-ep': np.sum(node_table['connectivity'] == 1) / len(node_table),\n",
    "    'prop-jn': np.sum(node_table['connectivity'] > 2) / len(node_table),\n",
    "    'prop-ap': np.sum(node_table['connectivity'] == 0) / len(node_table)\n",
    "}\n",
    "\n",
    "skel_sum_table = pd.DataFrame(data=skel_sum_table_data, index = [org_list[org]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bfc32f",
   "metadata": {},
   "source": [
    "## **DEFINE FUNCTIONS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37efd059",
   "metadata": {},
   "source": [
    "### **Define Skeleton Metrics Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c21b56c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skeleton_metrics(org_seg: np.ndarray,\n",
    "                     org_name: str,\n",
    "                     scale: Union[Tuple, None],\n",
    "                     output_all_tables: bool = False):\n",
    "    \n",
    "    '''\n",
    "    The function that returns the skeleton object table in the skeletonization portion of the workflow.\n",
    "    The branch, node and summary table can also be returned if the user sets `output_all_tables` to true\n",
    "    \n",
    "    Parameters:\n",
    "\n",
    "    * org_seg: the labeled organelle segmentation (int ndarray)\n",
    "    * org_name: the shorted name of the organelle (str)\n",
    "    * scale: the real world dimensions from the metadata of the image (Tuple, ndarray or list of 3 floats)\n",
    "    * output_all_tables: an option that allows you to return the branch, node, skeleton object and skeleton summary table\n",
    "    in that order (bool)\n",
    "\n",
    "    Output:\n",
    "\n",
    "    if output_all_tables is False (default)\n",
    "\n",
    "    * skeleton object table (pandas table)\n",
    "\n",
    "    if output_all_tables is False (default)\n",
    "\n",
    "    * branch table (pandas table)\n",
    "    * node table (pandas table)\n",
    "    * skeleton object table (pandas table)\n",
    "    * skeleton summary table (pandas table)\n",
    "    \n",
    "    '''\n",
    "\n",
    "    org_skel_arr, org_skel_arr2 = skel_plus(org_seg)\n",
    "\n",
    "    org_skel = csr.Skeleton(\n",
    "    skeleton_image = org_skel_arr,\n",
    "    spacing = scale,\n",
    "    value_is_height = False\n",
    "    )\n",
    "\n",
    "    ##############################################################################\n",
    "    # BRANCH TABLE\n",
    "    ##############################################################################\n",
    "\n",
    "    # This is a modified version of the summarize() function from skan optimized for skeleton metrics\n",
    "\n",
    "    # This is a modified version of the summarize function optimized for the workflow\n",
    "\n",
    "    summary = {}\n",
    "    value_is_height = False\n",
    "    ndim = org_skel.coordinates.shape[1]\n",
    "    endpoints_src = org_skel.paths.indices[org_skel.paths.indptr[:-1]]\n",
    "    endpoints_dst = org_skel.paths.indices[org_skel.paths.indptr[1:] - 1]\n",
    "\n",
    "    # Because the values of the path points are the organelle object ids (all path points and nodes \n",
    "    # in a branch should come from the same organelle object)\n",
    "    if not np.any(org_skel.path_stdev()):\n",
    "        # checker to see if all path points and nodes come from the same object\n",
    "        summary['skel-obj-id'] = org_skel.path_means().astype(int)\n",
    "    else:\n",
    "        raise ValueError(\"at least one branch came from different organelle objects\")\n",
    "    summary['node-id-src'] = endpoints_src\n",
    "    summary['node-id-dst'] = endpoints_dst\n",
    "    deg_src = org_skel.degrees[endpoints_src]\n",
    "    deg_dst = org_skel.degrees[endpoints_dst]\n",
    "    summary['deg_src'] = deg_src\n",
    "    summary['deg_dst'] = deg_dst\n",
    "    summary['branch-distance'] = org_skel.path_lengths()\n",
    "    kind = np.full(deg_src.shape, 2)  # default: junction-to-junction\n",
    "    kind[(deg_src == 1) | (deg_dst == 1)] = 1  # tip-junction\n",
    "    kind[(deg_src == 1) & (deg_dst == 1)] = 0  # tip-tip\n",
    "    kind[endpoints_src == endpoints_dst] = 3  # cycle\n",
    "    summary['branch-type'] = kind\n",
    "    for i in range(ndim):  # keep loops separate for best insertion order\n",
    "        summary[f'image-coord-src-{i}'] = org_skel.coordinates[endpoints_src, i]\n",
    "    for i in range(ndim):\n",
    "        summary[f'image-coord-dst-{i}'] = org_skel.coordinates[endpoints_dst, i]\n",
    "    coords_real_src = org_skel.coordinates[endpoints_src] * org_skel.spacing\n",
    "    for i in range(ndim):\n",
    "        summary[f'coord-src-{i}'] = coords_real_src[:, i]\n",
    "    coords_real_dst = org_skel.coordinates[endpoints_dst] * org_skel.spacing\n",
    "    for i in range(ndim):\n",
    "        summary[f'coord-dst-{i}'] = coords_real_dst[:, i]\n",
    "    summary['euclidean-distance'] = (\n",
    "            np.sqrt((coords_real_dst - coords_real_src)**2\n",
    "                    @ np.ones(ndim + int(value_is_height)))\n",
    "            )\n",
    "\n",
    "    summary['str-prop'] = summary['euclidean-distance'] / summary['branch-distance']\n",
    "    branch_table = pd.DataFrame(summary)\n",
    "\n",
    "    ##############################################################################\n",
    "    # NODE TABLE\n",
    "    ##############################################################################\n",
    "\n",
    "   # A dictionary that reveals the branches a node is referenced in\n",
    "    # If a node is an absolute punctate it will return \"NaN\"\n",
    "    node2branches = dict()\n",
    "    node2obj = dict()\n",
    "\n",
    "    for branch in range(org_skel.n_paths):\n",
    "        obj = branch_table.loc[branch]['skel-obj-id']\n",
    "        for point in org_skel.path(branch):\n",
    "            if org_skel.degrees[point] != 2:\n",
    "                node2obj[point] = obj\n",
    "                try:\n",
    "                    node2branches[point] += [branch]\n",
    "                except:\n",
    "                    node2branches[point] = [branch]\n",
    "\n",
    "    # the point ids for the points with zero connectivity\n",
    "    abs_punc_ids = np.arange(len(org_skel.degrees))[org_skel.degrees == 0]\n",
    "    abs_punc_obj = (org_skel_arr[org_skel_arr2][abs_punc_ids]).astype(int)\n",
    "\n",
    "    for i,punc_id in enumerate(abs_punc_ids):\n",
    "        node2branches[punc_id] = []\n",
    "        node2obj[punc_id] = abs_punc_obj[i]\n",
    "    \n",
    "    # note that if a node has a duplicate branch listed, the branch was a cycle\n",
    "    nodes = list(node2branches)\n",
    "\n",
    "    node_lab = []\n",
    "    base_n = [\"Abs Punctate\",\n",
    "            \"Endpoint\",\n",
    "            \"Path Point\"]\n",
    "\n",
    "    if np.max(org_skel.degrees) <= 2:\n",
    "        node_lab = base_n[0:len(pd.unique(org_skel.degrees))]\n",
    "    else:\n",
    "        node_lab = base_n\n",
    "        for i in np.arange(3, np.max(org_skel.degrees) + 1):\n",
    "            node_lab += [f\"{i}-way\"]\n",
    "        \n",
    "    node_table_data = {\n",
    "        \"node-id\": nodes,\n",
    "        \"node-type\": np.array(node_lab)[org_skel.degrees[nodes]],\n",
    "        \"connectivity\": org_skel.degrees[nodes],\n",
    "        \"image-coord-0\": org_skel.coordinates[nodes,0],\n",
    "        \"image-coord-1\": org_skel.coordinates[nodes,1],\n",
    "        \"image-coord-2\": org_skel.coordinates[nodes,2],\n",
    "        \"coord-0\": org_skel.coordinates[nodes,0] * scale[0],\n",
    "        \"coord-1\": org_skel.coordinates[nodes,1] * scale[1],\n",
    "        \"coord-2\": org_skel.coordinates[nodes,2] * scale[2],\n",
    "        \"branch-id(s)\": [node2branches[key] for key in nodes],\n",
    "        'skel-obj-id': [int(node2obj[key]) for key in nodes]\n",
    "    }\n",
    "\n",
    "    node_table = pd.DataFrame(data=node_table_data)\n",
    "\n",
    "    ##############################################################################\n",
    "    # SKELETON OBJECT TABLE\n",
    "    ##############################################################################\n",
    "\n",
    "    # This is to prevent the workflow acting as if the absolute punctates are their own skeleton objects\n",
    "    if org_name != 'ER':\n",
    "        obj_list = list(branch_table.groupby('skel-obj-id').groups) + list(abs_punc_obj)\n",
    "    else:\n",
    "        obj_list = list(branch_table.groupby('skel-obj-id').groups)\n",
    "\n",
    "\n",
    "    # the highest value of length where a type 0 branch will still be considered punctate\n",
    "    p_threshold = min(scale) * 2\n",
    "\n",
    "    #dictionary that keeps track of the number of branches per each object\n",
    "    objfreq = dict(branch_table.groupby(\"skel-obj-id\").count()['branch-distance'])\n",
    "    #dictionary that keeps track of the total length per object\n",
    "    sumobj = dict(branch_table.groupby(\"skel-obj-id\").sum()['branch-distance'])\n",
    "    #dictonary that keeps of the average branch length per object\n",
    "    aveobj = dict(branch_table.groupby(\"skel-obj-id\").mean()['branch-distance'])\n",
    "\n",
    "    obj_class = dict()\n",
    "    obj_class_n = dict()\n",
    "\n",
    "    for obj in objfreq:\n",
    "        if objfreq[obj] == 1:\n",
    "            if sumobj[obj] > p_threshold:\n",
    "                obj_class[obj] = \"Rod\"\n",
    "                obj_class_n[obj] = 1\n",
    "            else:\n",
    "                obj_class[obj] = \"Punctate\"\n",
    "                obj_class_n[obj] = 0\n",
    "        else:\n",
    "            obj_class[obj] = \"Network\"\n",
    "            obj_class_n[obj] = 2\n",
    "\n",
    "    # for some reason this was faster\n",
    "    skel_obj2branch = dict()\n",
    "    skel_obj2_mean_str = dict(branch_table.groupby('skel-obj-id').mean()['str-prop'])\n",
    "\n",
    "    for branch in range(org_skel.n_paths):\n",
    "        obj = branch_table['skel-obj-id'][branch]\n",
    "        try:\n",
    "            skel_obj2branch[obj] += [branch]\n",
    "        except:\n",
    "            skel_obj2branch[obj] = [branch]\n",
    "\n",
    "    if org_name != 'ER':\n",
    "        for obj in abs_punc_obj:\n",
    "            skel_obj2branch[obj] = []\n",
    "            obj_class[obj] = \"Punctate\"\n",
    "            obj_class_n[obj] = 0\n",
    "            skel_obj2_mean_str[obj] = \"NaN\"\n",
    "            objfreq[obj] = 0\n",
    "            aveobj[obj] = 0\n",
    "            sumobj[obj] = 0\n",
    "\n",
    "    \n",
    "    brh_tcounts = dict()\n",
    "    brh_tlist = dict()\n",
    "    for i in range(4):\n",
    "        brh_tcounts[i] = dict((obj,0) for obj in obj_list)\n",
    "        brh_tlist[i] = dict((obj,[]) for obj in obj_list)\n",
    "\n",
    "    branch_type_tables = [branch_table[np.array(branch_table[\"branch-type\"]) == i] for i in np.arange(4)]\n",
    "\n",
    "    for i in range(4):\n",
    "        groupedby_skel = branch_type_tables[i].groupby('skel-obj-id')\n",
    "        for skel_obj in obj_list:\n",
    "            try:\n",
    "                # Although they get the branch row indexes this is fine because the branch row indexes\n",
    "                # are the branch ids :)\n",
    "                type_list = list(groupedby_skel.groups[skel_obj])\n",
    "                brh_tlist[i][skel_obj] = type_list\n",
    "                brh_tcounts[i][skel_obj] = len(type_list)\n",
    "            except:\n",
    "                None\n",
    "\n",
    "   # Code to find the highest node degree in the skeleton object\n",
    "    max_deg = dict([(obj,0) for obj in obj_list])\n",
    "\n",
    "    obj2brh = branch_table.groupby('skel-obj-id').groups\n",
    "    for obj in obj2brh:\n",
    "        for branch in obj2brh[obj]:\n",
    "            brh_max = np.max(org_skel.degrees[org_skel.path(branch)])\n",
    "            if brh_max > max_deg[obj]:\n",
    "                max_deg[obj] = brh_max\n",
    "            \n",
    "\n",
    "    skel_obj2nodes = dict([(obj,[]) for obj in obj_list])\n",
    "    ep_count = dict([(obj,0) for obj in obj_list])\n",
    "    jn_count = dict([(obj,0) for obj in obj_list])\n",
    "    jn_table = node_table[node_table['connectivity'] > 2]\n",
    "    ave_jn_deg = dict([(obj,\"NaN\") for obj in obj_list])\n",
    "\n",
    "    for obj in obj_list:\n",
    "        try:\n",
    "            skel_obj2nodes[obj] = list(node_table['node-id'][node_table.groupby(\"skel-obj-id\").groups[obj]])\n",
    "        except:\n",
    "            None\n",
    "        try:\n",
    "            ep_count[obj] = node_table[node_table['connectivity'] == 1].groupby('skel-obj-id').count()['connectivity'][obj]\n",
    "        except:\n",
    "            None\n",
    "        try:\n",
    "            jn_count[obj] = node_table[node_table['connectivity'] > 2].groupby('skel-obj-id').count()['connectivity'][obj]\n",
    "            if jn_count[obj] > 0:\n",
    "                ave_jn_deg[obj] = np.mean(jn_table['connectivity'][jn_table.groupby('skel-obj-id').groups[obj]])\n",
    "        except:\n",
    "            None\n",
    "            \n",
    "    skel_table_data = {\n",
    "            \"skel-obj-id\": obj_list,\n",
    "            \"skel-type\": [obj_class[obj] for obj in obj_list],\n",
    "            \"skel-type-num\": [obj_class_n[obj] for obj in obj_list],\n",
    "            \"brh-count\": [objfreq[obj] for obj in obj_list],\n",
    "            \"branch-id(s)\": [skel_obj2branch[obj] for obj in obj_list],\n",
    "            \"min-brh-length\": [branch_table.groupby('skel-obj-id').min()['branch-distance'][obj] if objfreq[obj] != 0 else \"NaN\" for obj in obj_list],\n",
    "            \"max-brh-length\": [branch_table.groupby('skel-obj-id').max()['branch-distance'][obj] if objfreq[obj] != 0 else \"NaN\" for obj in obj_list],\n",
    "            \"ave-brh-length\": [aveobj[obj] for obj in obj_list],\n",
    "            # the requirement is changed to > 1 because the sd of a single value is undefined\n",
    "            \"sd-brh-length\": [branch_table.groupby('skel-obj-id').std()['branch-distance'][obj] if objfreq[obj] > 1 else \"NaN\" for obj in obj_list],\n",
    "            \"med-brh-length\" : [branch_table.groupby('skel-obj-id').median()['branch-distance'][obj] if objfreq[obj] != 0 else \"NaN\" for obj in obj_list],\n",
    "            \"total-length\": [sumobj[obj] for obj in obj_list],\n",
    "            \"brh-type-0-tot\": [brh_tcounts[0][obj] for obj in obj_list],\n",
    "            \"brh-type-0-id\": [brh_tlist[0][obj] for obj in obj_list],\n",
    "            \"brh-type-1-tot\": [brh_tcounts[1][obj] for obj in obj_list],\n",
    "            \"brh-type-1-ids\": [brh_tlist[1][obj] for obj in obj_list],\n",
    "            \"brh-type-2-tot\": [brh_tcounts[2][obj] for obj in obj_list],\n",
    "            \"brh-type-2-ids\": [brh_tlist[2][obj] for obj in obj_list],\n",
    "            \"brh-type-3-tot\": [brh_tcounts[3][obj] for obj in obj_list],\n",
    "            \"brh-type-3-ids\": [brh_tlist[3][obj] for obj in obj_list],\n",
    "            \"node-count\": [len(skel_obj2nodes[obj]) for obj in obj_list],\n",
    "            'ep-count': [ep_count[obj] for obj in obj_list],\n",
    "            'jn-count' : [jn_count[obj] for obj in obj_list],\n",
    "            'ave-jn-deg' : [ave_jn_deg[obj] for obj in obj_list],\n",
    "            'max-deg' : [max_deg[obj] for obj in obj_list],\n",
    "            \"node-id(s)\": [skel_obj2nodes[obj] for obj in obj_list],\n",
    "            \"mean-brh-str\": [skel_obj2_mean_str[obj] for obj in obj_list]}\n",
    "\n",
    "    skel_table = pd.DataFrame(data=skel_table_data)\n",
    "\n",
    "    if not output_all_tables:\n",
    "        return skel_table.sort_values('skel-obj-id')\n",
    "\n",
    "    ##############################################################################\n",
    "    # SKELETON SUMMARY TABLE\n",
    "    ##############################################################################\n",
    "\n",
    "    skel_sum_table_data = {\n",
    "\n",
    "    'total-length': np.sum(org_skel.path_lengths()),\n",
    "    'point-count': org_skel.graph.shape[0],\n",
    "\n",
    "    # -- Skeleton Object section -- #\n",
    "\n",
    "    'skel-obj-count': len(skel_table),\n",
    "    'punc-count': np.sum(skel_table['skel-type-num'] == 0),\n",
    "    'rod-count': np.sum(skel_table['skel-type-num'] == 1),\n",
    "    'net-count': np.sum(skel_table['skel-type-num'] == 2),\n",
    "    'prop-obj-punc': np.sum(skel_table['skel-type-num'] == 0) / len(skel_table),\n",
    "    'prop-obj-rod': np.sum(skel_table['skel-type-num'] == 1) / len(skel_table),\n",
    "    'prop-obj-net': np.sum(skel_table['skel-type-num'] == 2) / len(skel_table),\n",
    "    'punc-tot-len': np.sum(skel_table[skel_table['skel-type-num'] == 0]['total-length']),\n",
    "    'rod-tot-len': np.sum(skel_table[skel_table['skel-type-num'] == 1]['total-length']),\n",
    "    'net-tot-len': np.sum(skel_table[skel_table['skel-type-num'] == 2]['total-length']),\n",
    "    'prop-len-punc': np.sum(skel_table[skel_table['skel-type-num'] == 0]['total-length']) / np.sum(org_skel.path_lengths()),\n",
    "    'prop-len-rod': np.sum(skel_table[skel_table['skel-type-num'] == 1]['total-length']) / np.sum(org_skel.path_lengths()),\n",
    "    'prop-len-net': np.sum(skel_table[skel_table['skel-type-num'] == 2]['total-length']) / np.sum(org_skel.path_lengths()),\n",
    "    'ave-len-obj': np.mean(skel_table['total-length']),\n",
    "    'min-len-obj': np.min(skel_table['total-length']),\n",
    "    'max-len-obj': np.max(skel_table['total-length']),\n",
    "    'ave-brh-obj': np.mean(skel_table['brh-count']),\n",
    "    'min-brh-obj': np.min(skel_table['brh-count']),\n",
    "    'max-brh-obj': np.max(skel_table['brh-count']),\n",
    "\n",
    "    # -- Branch section -- #\n",
    "\n",
    "    'brh-count': org_skel.n_paths,\n",
    "    'min-brh-len': np.min(org_skel.path_lengths()),\n",
    "    'max-brh-len': np.max(org_skel.path_lengths()),\n",
    "    'ave-brh-len': np.mean(org_skel.path_lengths()),\n",
    "    'type-0-brhs': len(branch_table[branch_table['branch-type'] == 0]),\n",
    "    'type-1-brhs': len(branch_table[branch_table['branch-type'] == 1]),\n",
    "    'type-2-brhs': len(branch_table[branch_table['branch-type'] == 2]),\n",
    "    'type-3-brhs': len(branch_table[branch_table['branch-type'] == 3]),\n",
    "    'prop-brh-t0': len(branch_table[branch_table['branch-type'] == 0]) / len(branch_table),\n",
    "    'prop-brh-t1': len(branch_table[branch_table['branch-type'] == 1]) / len(branch_table),\n",
    "    'prop-brh-t2': len(branch_table[branch_table['branch-type'] == 2]) / len(branch_table),\n",
    "    'prop-brh-t3': len(branch_table[branch_table['branch-type'] == 3]) / len(branch_table),\n",
    "    't0-brh-len': np.sum(org_skel.path_lengths()[branch_table['branch-type'] == 0]),\n",
    "    't1-brh-len': np.sum(org_skel.path_lengths()[branch_table['branch-type'] == 1]),\n",
    "    't2-brh-len': np.sum(org_skel.path_lengths()[branch_table['branch-type'] == 2]),\n",
    "    't3-brh-len': np.sum(org_skel.path_lengths()[branch_table['branch-type'] == 3]),\n",
    "    'prop-len-t0': np.sum(org_skel.path_lengths()[branch_table['branch-type'] == 0]) / np.sum(org_skel.path_lengths()),\n",
    "    'prop-len-t1': np.sum(org_skel.path_lengths()[branch_table['branch-type'] == 1]) / np.sum(org_skel.path_lengths()),\n",
    "    'prop-len-t2': np.sum(org_skel.path_lengths()[branch_table['branch-type'] == 2]) / np.sum(org_skel.path_lengths()),\n",
    "    'prop-len-t3': np.sum(org_skel.path_lengths()[branch_table['branch-type'] == 3]) / np.sum(org_skel.path_lengths()),\n",
    "    \n",
    "    # -- Node section -- #\n",
    "\n",
    "    'node-count': len(node_table),\n",
    "    'ave-deg-nodes': np.mean(node_table['connectivity']),\n",
    "    'ep-count': np.sum(node_table['connectivity'] == 1),\n",
    "    'jn-count': np.sum(node_table['connectivity'] > 2),\n",
    "    'ap-count': np.sum(node_table['connectivity'] == 0),\n",
    "    'prop-ep': np.sum(node_table['connectivity'] == 1) / len(node_table),\n",
    "    'prop-jn': np.sum(node_table['connectivity'] > 2) / len(node_table),\n",
    "    'prop-ap': np.sum(node_table['connectivity'] == 0) / len(node_table)\n",
    "    }\n",
    "\n",
    "    skel_sum_table = pd.DataFrame(data=skel_sum_table_data, index = [org_name])\n",
    "\n",
    "    return branch_table, node_table.sort_values('node-id'), skel_table.sort_values('skel-obj-id'), skel_sum_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00046cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part One took 10.558670282363892 sec(s)\n",
      "Total Time: 10.558670282363892 sec(s)\n"
     ]
    }
   ],
   "source": [
    "_branch_table, _node_table, _skel_table, _skel_sum_table = skeleton_metrics(organelles[org_channels_ordered.index(org)],\n",
    "                org_list[org_channels_ordered.index(org)],\n",
    "                scale,\n",
    "                output_all_tables = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb89b00",
   "metadata": {},
   "source": [
    "### **Update to `get_morphology_metrics`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d3166d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_morphology_metrics(segmentation_img: np.ndarray, \n",
    "                           seg_name: str, \n",
    "                           intensity_img, \n",
    "                           mask: np.ndarray, \n",
    "                           mask_name: str,\n",
    "                           scale: Union[tuple, None]=None,\n",
    "                           skel_met: bool = False):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ------------\n",
    "    segmentation_img:\n",
    "        an np.ndarray of segmented objects \n",
    "    seg_name: str\n",
    "        a name or nickname (usually the segmentation file suffix) of the object being measured; this will be used for record keeping in the output table\n",
    "    intensity_img:\n",
    "        a single-channel np.ndarray contain gray scale values from the \"raw\" image the segmentation is based on; this image should be the same shape as the segmentation file\n",
    "    mask:\n",
    "        a binary np.ndarray mask of the area to measure from; this image should be the same shape as the segmentation file\n",
    "    scale: tuple, optional\n",
    "        a tuple that contains the real world dimensions for each dimension in the image (Z, Y, X)\n",
    "    skel_met:\n",
    "        a boolean that determines whether to include skeleton metrics\n",
    "\n",
    "\n",
    "    Regionprops measurements:\n",
    "    ------------------------\n",
    "    'label',\n",
    "    'centroid',\n",
    "    'bbox',\n",
    "    'area',\n",
    "    'equivalent_diameter',\n",
    "    'extent',\n",
    "    'euler_number',\n",
    "    'solidity',\n",
    "    'axis_major_length',\n",
    "    'min_intensity',\n",
    "    'max_intensity',\n",
    "    'mean_intensity'\n",
    "\n",
    "    Additional measurements:\n",
    "    -----------------------\n",
    "    'standard_deviation_intensity',\n",
    "    'surface_area',\n",
    "    'SA_to_volume_ratio`\n",
    "\n",
    "    Skeleton measurements (optional):\n",
    "    ------------------------\n",
    "    'skel-obj-id',\n",
    "    'skel-type',\n",
    "    'skel-brh-count',\n",
    "    'skel-min-brh-length',\n",
    "    'skel-max-brh-length',\n",
    "    'skel-ave-brh-length',\n",
    "    'skel-sd-brh-length',\n",
    "    'skel-med-brh-length',\n",
    "    'skel-total-length',\n",
    "    'skel-node-count',\n",
    "    'skel-ep-count',\n",
    "    'skel-jn-count',\n",
    "    'skel-ave-jn-deg',\n",
    "    'skel-max-deg',\n",
    "    'skel-mean-brh-str'\n",
    "\n",
    "    Returns\n",
    "    -------------\n",
    "    pandas dataframe of containing regionprops measurements (columns) for each object in the segmentation image (rows) and the regionprops object\n",
    "    \n",
    "    \"\"\"\n",
    "    # dealing with numerous solidity warning from regionprops\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    ###################################################\n",
    "    ## MASK THE ORGANELLE OBJECTS THAT WILL BE MEASURED\n",
    "    ###################################################\n",
    "    input_labels = _assert_uint16_labels(segmentation_img)\n",
    "    input_labels = apply_mask(input_labels, mask)\n",
    "\n",
    "    ##########################################\n",
    "    ## CREATE LIST OF REGIONPROPS MEASUREMENTS\n",
    "    ##########################################\n",
    "    # start with LABEL\n",
    "    properties = [\"label\", \"centroid\", \"bbox\", \"area\", \n",
    "                  \"equivalent_diameter\", \"extent\", \"euler_number\", \"solidity\", \"axis_major_length\",\n",
    "                  \"min_intensity\", \"max_intensity\", \"mean_intensity\"]\n",
    "\n",
    "    #######################\n",
    "    ## ADD EXTRA PROPERTIES\n",
    "    #######################\n",
    "    def standard_deviation_intensity(region, intensities):\n",
    "        return np.std(intensities[region])\n",
    "\n",
    "    extra_properties = [standard_deviation_intensity]\n",
    "\n",
    "    ##################\n",
    "    ## RUN REGIONPROPS\n",
    "    ##################\n",
    "    props = regionprops_table(input_labels, \n",
    "                           intensity_image=intensity_img, \n",
    "                           properties=properties,\n",
    "                           extra_properties=extra_properties,\n",
    "                           spacing=scale)\n",
    "    \n",
    "    # measure the mask volume as well for easier normalization in downstream functions\n",
    "    mask_vol = regionprops_table(mask,properties=[\"area\"], spacing=scale)['area'][0]\n",
    "\n",
    "    props_table = pd.DataFrame(props)\n",
    "\n",
    "    ##################################################################\n",
    "    ## RUN SURFACE AREA FUNCTION SEPARATELY AND APPEND THE PROPS_TABLE\n",
    "    ##################################################################\n",
    "    surface_area_tab = pd.DataFrame(surface_area_from_props(input_labels, props, scale))\n",
    "\n",
    "    #############################################\n",
    "    ## RENAME AND ADD ADDITIONAL METADATA COLUMNS\n",
    "    #############################################\n",
    "    props_table.insert(0, \"object\", seg_name)\n",
    "    props_table.rename(columns={\"area\": \"volume\"}, inplace=True)\n",
    "\n",
    "    if scale is not None:\n",
    "        round_scale = (round(scale[0], 4), round(scale[1], 4), round(scale[2], 4))\n",
    "        props_table.insert(props_table.columns.get_loc('label') + 1, column=\"scale\", value=f\"{round_scale}\")\n",
    "    else: \n",
    "        props_table.insert(props_table.columns.get_loc('label') + 1, column=\"scale\", value=f\"{tuple(np.ones(segmentation_img.ndim))}\") \n",
    "\n",
    "    props_table.insert(props_table.columns.get_loc('volume') + 1, \"surface_area\", surface_area_tab)\n",
    "    props_table.insert(props_table.columns.get_loc('surface_area') + 1, \"SA_to_volume_ratio\", props_table[\"surface_area\"].div(props_table[\"volume\"]))\n",
    "    props_table[f\"{mask_name}_volume\"] = mask_vol\n",
    "\n",
    "    ################################################################\n",
    "    ## ADD SKELETONIZATION OPTION FOR MEASURING LENGTH AND BRANCHING\n",
    "    ################################################################\n",
    "    if skel_met:\n",
    "        skel_table = skeleton_metrics(input_labels.astype(float),\n",
    "                 seg_name,\n",
    "                 scale)\n",
    "        \n",
    "        skel_tab = skel_table[['skel-obj-id',\n",
    "                                'skel-type',\n",
    "                                'brh-count',\n",
    "                                'min-brh-length',\n",
    "                                'max-brh-length',\n",
    "                                'ave-brh-length',\n",
    "                                'sd-brh-length',\n",
    "                                'med-brh-length',\n",
    "                                'total-length',\n",
    "                                'node-count',\n",
    "                                'ep-count',\n",
    "                                'jn-count',\n",
    "                                'ave-jn-deg',\n",
    "                                'max-deg',\n",
    "                                'mean-brh-str']]\n",
    "\n",
    "        props_table = props_table.merge(skel_tab, left_on = 'label', right_on = 'skel-obj-id').drop('skel-obj-id', axis = 1)\n",
    "\n",
    "        props_table.rename(columns = dict([(name, 'skel-' + name) for name in skel_tab.columns[2:]]), inplace = True)\n",
    "\n",
    "    # print this statement to let user known of suppressed warnings\n",
    "    if Warning: print(f\"Warning(s) suppressed while quantifying {seg_name}. See 'method_morphology.ipynb' notebook for more details.\")\n",
    "\n",
    "    return props_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a67811dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "###################################\n",
    "# Quantifying organelle morphology\n",
    "##################################\n",
    "##################################\n",
    "\n",
    "# quantify the morphology of one or more organelle from one cell\n",
    "def get_org_morphology(source_file_path: str,\n",
    "                         list_obj_names: List[str],\n",
    "                         list_obj_segs: List[np.ndarray],\n",
    "                         list_intensity_img: List[np.ndarray],\n",
    "                         list_region_names: List[str],\n",
    "                         list_region_segs: List[np.ndarray],\n",
    "                         mask_name: str,\n",
    "                         scale: Union[tuple,None] = None):\n",
    "    \"\"\"\n",
    "    Measure the amount, size, and shape of multiple organelles from a single cell\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    source_file: str\n",
    "        file path; this is used for recorder keeping of the file name in the output data tables\n",
    "    list_obj_names: List[str]\n",
    "        a list of object names (strings) that will be measured; this should match the order in list_obj_segs\n",
    "    list_obj_segs: List[np.ndarray]\n",
    "        a list of 3D (ZYX) segmentation np.ndarrays that will be measured per cell; the order should match the list_obj_names \n",
    "    list_intensity_img: List[np.ndarray]\n",
    "        a list of 3D (ZYX) grayscale np.ndarrays that will be used to measure fluoresence intensity in each region and object\n",
    "    list_region_names: List[str]\n",
    "        a list of region names (strings); these should include the mask (entire region being measured - usually the cell) \n",
    "        and other sub-mask regions from which we can meausure the objects in (ex - nucleus, neurites, soma, etc.). It should \n",
    "        also include the centering object used when created the XY distribution bins.\n",
    "        The order should match the list_region_segs\n",
    "    list_region_segs: List[np.ndarray]\n",
    "        a list of 3D (ZYX) binary np.ndarrays of the region masks; the order should match the list_region_names.\n",
    "    mask: str\n",
    "        a str of which region name (contained in the list_region_names list) should be used as the main mask (e.g., cell mask)\n",
    "    scale: Union[tuple,None] = None\n",
    "        a tuple that contains the real world dimensions for each dimension in the image (Z, Y, X)\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    Dataframe of measurements of organelle morphology\n",
    "\n",
    "    \"\"\"\n",
    "    print(f\"Quantifying organelle morphology from {source_file_path}.\")\n",
    "\n",
    "    # select the mask from the region list\n",
    "    mask = list_region_segs[list_region_names.index(mask_name)]\n",
    "    \n",
    "    # empty list to collect a morphology data for each organelle\n",
    "    org_tabs = []\n",
    "\n",
    "    # loop through the list of organelles and run the get_morphology_metrics function\n",
    "    for j, target in enumerate(list_obj_names):\n",
    "        # select intensity image\n",
    "        org_img = list_intensity_img[j]  \n",
    "        \n",
    "        # select segmentation and if ER, ensure it is only one object\n",
    "        if target == 'ER':\n",
    "            org_obj = (list_obj_segs[j] > 0).astype(np.uint16)  \n",
    "        else:\n",
    "            org_obj = list_obj_segs[j]\n",
    "        \n",
    "        # run get_morphology_metrics function to output a table of measurements\n",
    "        org_metrics = _get_morphology_metrics(segmentation_img=org_obj, \n",
    "                                            seg_name=target,\n",
    "                                            intensity_img=org_img, \n",
    "                                            mask=mask,\n",
    "                                            mask_name=mask_name,\n",
    "                                            scale=scale,\n",
    "                                            skel_met = skel_met)\n",
    "\n",
    "        # add table to list above\n",
    "        org_tabs.append(org_metrics)\n",
    "\n",
    "    # combine the lists for each organelle into one table\n",
    "    final_org_tab = pd.concat(org_tabs, ignore_index=True)\n",
    "\n",
    "    # add a new column to list the name of the image these data are derived from \n",
    "    final_org_tab.insert(loc=0,column='image_name',value=source_file_path.stem)\n",
    "    \n",
    "\n",
    "    return final_org_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ab7d227",
   "metadata": {},
   "outputs": [],
   "source": [
    "### OLD FUNCTION\n",
    "# def _get_org_morphology_3D(segmentation_img: np.ndarray, \n",
    "#                            seg_name: str, \n",
    "#                            intensity_img, \n",
    "#                            mask: np.ndarray, \n",
    "#                            scale: Union[tuple, None]=None,\n",
    "#                            skel_met: bool = False):\n",
    "#     \"\"\"\n",
    "#     Parameters\n",
    "#     ------------\n",
    "#     segmentation_img:\n",
    "#         a 3D (ZYX) np.ndarray of segmented objects \n",
    "#     seg_name: str\n",
    "#         a name or nickname of the object being measured; this will be used for record keeping in the output table\n",
    "#     intensity_img:\n",
    "#         a 3D (ZYX) np.ndarray contain gray scale values from the \"raw\" image the segmentation is based on )single channel)\n",
    "#     mask:\n",
    "#         a 3D (ZYX) binary np.ndarray mask of the area to measure from\n",
    "#     scale: tuple, optional\n",
    "#         a tuple that contains the real world dimensions for each dimension in the image (Z, Y, X)\n",
    "#     skel_met:\n",
    "#         a boolean regarding whether to include the skeleton metrics\n",
    "\n",
    "\n",
    "#     Regionprops measurements:\n",
    "#     ------------------------\n",
    "#     ['label',\n",
    "#     'centroid',\n",
    "#     'bbox',\n",
    "#     'area',\n",
    "#     'equivalent_diameter',\n",
    "#     'extent',\n",
    "#     'feret_diameter_max',\n",
    "#     'euler_number',\n",
    "#     'convex_area',\n",
    "#     'solidity',\n",
    "#     'axis_major_length',\n",
    "#     'axis_minor_length',\n",
    "#     'max_intensity',\n",
    "#     'mean_intensity',\n",
    "#     'min_intensity']\n",
    "\n",
    "#     Additional measurements:\n",
    "#     -----------------------\n",
    "#     ['standard_deviation_intensity',\n",
    "#     'surface_area']\n",
    "\n",
    "#     Skeleton metrics (if added):\n",
    "#     ------------------------\n",
    "#     ['skel-obj-id',\n",
    "#     'skel-type',\n",
    "#     'skel-brh-count',\n",
    "#     'skel-min-brh-length',\n",
    "#     'skel-max-brh-length',\n",
    "#     'skel-ave-brh-length',\n",
    "#     'skel-sd-brh-length',\n",
    "#     'skel-med-brh-length',\n",
    "#     'skel-total-length',\n",
    "#     'skel-node-count',\n",
    "#     'skel-ep-count',\n",
    "#     'skel-jn-count',\n",
    "#     'skel-ave-jn-deg',\n",
    "#     'skel-max-deg',\n",
    "#     'skel-mean-brh-str']\n",
    "\n",
    "\n",
    "#     Returns\n",
    "#     -------------\n",
    "#     pandas dataframe of containing regionprops measurements (columns) for each object in the segmentation image (rows) and the regionprops object\n",
    "    \n",
    "#     \"\"\"\n",
    "#     ###################################################\n",
    "#     ## MASK THE ORGANELLE OBJECTS THAT WILL BE MEASURED\n",
    "#     ###################################################\n",
    "#     # in case we sent a boolean mask (e.g. cyto, nucleus, cellmask)\n",
    "#     input_labels = _assert_uint16_labels(segmentation_img)\n",
    "\n",
    "#     # mask\n",
    "#     input_labels = apply_mask(input_labels, mask)\n",
    "\n",
    "#     ##########################################\n",
    "#     ## CREATE LIST OF REGIONPROPS MEASUREMENTS\n",
    "#     ##########################################\n",
    "#     # start with LABEL\n",
    "#     properties = [\"label\"]\n",
    "\n",
    "#     # add position\n",
    "#     properties = properties + [\"centroid\", \"bbox\"]\n",
    "\n",
    "#     # add area\n",
    "#     properties = properties + [\"area\", \"equivalent_diameter\"] # \"num_pixels\", \n",
    "\n",
    "#     # add shape measurements\n",
    "#     properties = properties + [\"extent\", \"euler_number\", \"solidity\", \"axis_major_length\"] # ,\"feret_diameter_max\", \"axis_minor_length\"]\n",
    "\n",
    "#     # add intensity values (used for quality checks)\n",
    "#     properties = properties + [\"min_intensity\", \"max_intensity\", \"mean_intensity\"]\n",
    "\n",
    "#     #######################\n",
    "#     ## ADD EXTRA PROPERTIES\n",
    "#     #######################\n",
    "#     def standard_deviation_intensity(region, intensities):\n",
    "#         return np.std(intensities[region])\n",
    "\n",
    "#     extra_properties = [standard_deviation_intensity]\n",
    "\n",
    "#     ##################\n",
    "#     ## RUN REGIONPROPS\n",
    "#     ##################\n",
    "#     props = regionprops_table(input_labels, \n",
    "#                            intensity_image=intensity_img, \n",
    "#                            properties=properties,\n",
    "#                            extra_properties=extra_properties,\n",
    "#                            spacing=scale)\n",
    "\n",
    "#     props_table = pd.DataFrame(props)\n",
    "#     props_table.insert(0, \"object\", seg_name)\n",
    "#     props_table.rename(columns={\"area\": \"volume\"}, inplace=True)\n",
    "\n",
    "#     if scale is not None:\n",
    "#         round_scale = (round(scale[0], 4), round(scale[1], 4), round(scale[2], 4))\n",
    "#         props_table.insert(loc=2, column=\"scale\", value=f\"{round_scale}\")\n",
    "#     else: \n",
    "#         props_table.insert(loc=2, column=\"scale\", value=f\"{tuple(np.ones(segmentation_img.ndim))}\") \n",
    "\n",
    "#     ##################################################################\n",
    "#     ## RUN SURFACE AREA FUNCTION SEPARATELY AND APPEND THE PROPS_TABLE\n",
    "#     ##################################################################\n",
    "#     surface_area_tab = pd.DataFrame(surface_area_from_props(input_labels, props, scale))\n",
    "\n",
    "#     props_table.insert(12, \"surface_area\", surface_area_tab)\n",
    "#     props_table.insert(14, \"SA_to_volume_ratio\", props_table[\"surface_area\"].div(props_table[\"volume\"]))\n",
    "\n",
    "#     ################################################################\n",
    "#     ## ADD SKELETONIZATION OPTION FOR MEASURING LENGTH AND BRANCHING\n",
    "#     ################################################################\n",
    "#     if skel_met:\n",
    "#         skel_table = skeleton_metrics(input_labels.astype(float),\n",
    "#                  seg_name,\n",
    "#                  dim)\n",
    "        \n",
    "#         skel_tab = skel_table[['skel-obj-id',\n",
    "#                                 'skel-type',\n",
    "#                                 'brh-count',\n",
    "#                                 'min-brh-length',\n",
    "#                                 'max-brh-length',\n",
    "#                                 'ave-brh-length',\n",
    "#                                 'sd-brh-length',\n",
    "#                                 'med-brh-length',\n",
    "#                                 'total-length',\n",
    "#                                 'node-count',\n",
    "#                                 'ep-count',\n",
    "#                                 'jn-count',\n",
    "#                                 'ave-jn-deg',\n",
    "#                                 'max-deg',\n",
    "#                                 'mean-brh-str']]\n",
    "\n",
    "#         props_table = props_table.merge(skel_tab, left_on = 'label', right_on = 'skel-obj-id').drop('skel-obj-id', axis = 1)\n",
    "\n",
    "#         props_table.rename(columns = dict([(name, 'skel-' + name) for name in skel_tab.columns[2:]]), inplace = True)\n",
    "\n",
    "#     return props_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d8c3f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part One took 10.746462106704712 sec(s)\n",
      "Total Time: 10.746462106704712 sec(s)\n",
      "Warning(s) suppressed while quantifying ER. See 'method_morphology.ipynb' notebook for more details.\n"
     ]
    }
   ],
   "source": [
    "# get_morph_table = _get_morphology_metrics(segmentation_img = organelles[org_channels_ordered.index(org)],\n",
    "#                       seg_name = org_list[org_channels_ordered.index(org)],\n",
    "#                       intensity_img = img_data[org],\n",
    "#                       mask = mask,\n",
    "#                       mask_name = mask_name,\n",
    "#                       skel_met = True,\n",
    "#                       scale = scale)\n",
    "\n",
    "get_morph_table = _get_morphology_metrics(segmentation_img = org_seg,\n",
    "                      seg_name = org_list[org_channels_ordered.index(org)],\n",
    "                      intensity_img = img_data[org],\n",
    "                      mask = mask,\n",
    "                      mask_name = mask_name,\n",
    "                      skel_met = True,\n",
    "                      scale = scale)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6046370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "testa_inputs = [org_seg,\n",
    " org_list[org_channels_ordered.index(org)],\n",
    " img_data[org],\n",
    " mask,\n",
    " mask_name,\n",
    " True,\n",
    " scale]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c858ebc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]]], dtype=uint16),\n",
       " 'ER',\n",
       " array([[[2.53243838e-02, 2.53243838e-02, 2.53243838e-02, ...,\n",
       "          2.53243838e-02, 2.53243838e-02, 2.53243838e-02],\n",
       "         [2.53243838e-02, 2.53243838e-02, 2.53243838e-02, ...,\n",
       "          2.53243838e-02, 2.53243838e-02, 2.53243838e-02],\n",
       "         [2.53243838e-02, 2.53243838e-02, 2.53243838e-02, ...,\n",
       "          2.53243838e-02, 2.53243838e-02, 2.53243838e-02],\n",
       "         ...,\n",
       "         [1.23809685e+02, 7.72415161e+01, 5.02285423e+01, ...,\n",
       "          2.53243838e-02, 2.53243838e-02, 2.53243838e-02],\n",
       "         [4.75179329e+01, 3.92883110e+01, 3.97080574e+01, ...,\n",
       "          2.53243838e-02, 2.53243838e-02, 2.53243838e-02],\n",
       "         [1.61747627e+01, 2.28648224e+01, 4.13089142e+01, ...,\n",
       "          2.53243838e-02, 2.53243838e-02, 3.06891911e-02]],\n",
       " \n",
       "        [[2.53243838e-02, 2.53243838e-02, 2.53243838e-02, ...,\n",
       "          2.53243838e-02, 2.53243838e-02, 2.53243838e-02],\n",
       "         [2.53243838e-02, 2.53243838e-02, 2.53243838e-02, ...,\n",
       "          2.53243838e-02, 2.53243838e-02, 2.53243838e-02],\n",
       "         [2.53243838e-02, 2.53243838e-02, 2.53243838e-02, ...,\n",
       "          2.53243838e-02, 2.53243838e-02, 2.53243838e-02],\n",
       "         ...,\n",
       "         [7.83014526e+02, 7.51444641e+02, 6.79784668e+02, ...,\n",
       "          2.53243838e-02, 2.53243838e-02, 2.53243838e-02],\n",
       "         [5.05467560e+02, 5.02518433e+02, 4.73716858e+02, ...,\n",
       "          2.53243838e-02, 2.53243838e-02, 2.53243838e-02],\n",
       "         [2.88165894e+02, 3.01965118e+02, 3.02315887e+02, ...,\n",
       "          2.53243838e-02, 2.53243838e-02, 2.53243838e-02]],\n",
       " \n",
       "        [[2.53243838e-02, 2.53243838e-02, 2.53243838e-02, ...,\n",
       "          2.53243838e-02, 2.53243838e-02, 2.53243838e-02],\n",
       "         [2.53243838e-02, 2.53243838e-02, 2.53243838e-02, ...,\n",
       "          2.53243838e-02, 2.53243838e-02, 2.53243838e-02],\n",
       "         [2.53243838e-02, 2.53243838e-02, 2.53243838e-02, ...,\n",
       "          2.53243838e-02, 2.53243838e-02, 2.53243838e-02],\n",
       "         ...,\n",
       "         [1.06600842e+03, 1.17792883e+03, 1.23318933e+03, ...,\n",
       "          2.53243838e-02, 2.53243838e-02, 2.53243838e-02],\n",
       "         [1.17648877e+03, 1.01248462e+03, 8.69871765e+02, ...,\n",
       "          2.53243838e-02, 2.53243838e-02, 2.53243838e-02],\n",
       "         [1.25346631e+03, 8.72467102e+02, 5.84505493e+02, ...,\n",
       "          2.53243838e-02, 2.53243838e-02, 2.53243838e-02]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[2.53243838e-02, 2.53243838e-02, 2.53243838e-02, ...,\n",
       "          3.80663604e-01, 4.41693813e-01, 3.66775960e-01],\n",
       "         [2.53243838e-02, 2.53243838e-02, 2.53243838e-02, ...,\n",
       "          5.90162158e-01, 7.40679264e-01, 6.76658034e-01],\n",
       "         [2.53243838e-02, 2.53243838e-02, 2.53243838e-02, ...,\n",
       "          7.52571523e-01, 9.92086232e-01, 9.83988643e-01],\n",
       "         ...,\n",
       "         [2.53243838e-02, 2.53243838e-02, 2.53243838e-02, ...,\n",
       "          2.53243838e-02, 2.53243838e-02, 2.53243838e-02],\n",
       "         [2.53243838e-02, 2.53243838e-02, 2.53243838e-02, ...,\n",
       "          2.53243838e-02, 2.53243838e-02, 2.53243838e-02],\n",
       "         [2.53243838e-02, 2.53243838e-02, 2.53243838e-02, ...,\n",
       "          2.53243838e-02, 2.53243838e-02, 2.53243838e-02]],\n",
       " \n",
       "        [[2.53243838e-02, 2.53243838e-02, 2.53243838e-02, ...,\n",
       "          2.54713402e+01, 2.73348007e+01, 2.05571575e+01],\n",
       "         [2.53243838e-02, 2.53243838e-02, 2.53243838e-02, ...,\n",
       "          3.36732254e+01, 4.23814278e+01, 3.88518448e+01],\n",
       "         [2.53243838e-02, 2.53243838e-02, 2.53243838e-02, ...,\n",
       "          3.92149391e+01, 5.44740944e+01, 5.71551323e+01],\n",
       "         ...,\n",
       "         [2.53243838e-02, 2.53243838e-02, 2.53243838e-02, ...,\n",
       "          2.53243838e-02, 2.53243838e-02, 2.53243838e-02],\n",
       "         [2.53243838e-02, 2.53243838e-02, 2.53243838e-02, ...,\n",
       "          2.53243838e-02, 2.53243838e-02, 2.53243838e-02],\n",
       "         [2.53243838e-02, 2.53243838e-02, 2.53243838e-02, ...,\n",
       "          2.53243838e-02, 2.53243838e-02, 2.53243838e-02]],\n",
       " \n",
       "        [[2.53243838e-02, 2.53243838e-02, 2.53243838e-02, ...,\n",
       "          6.52577881e+02, 5.29383118e+02, 3.00517090e+02],\n",
       "         [2.53243838e-02, 2.53243838e-02, 2.53243838e-02, ...,\n",
       "          5.71094604e+02, 5.67764099e+02, 4.36449036e+02],\n",
       "         [2.53243838e-02, 2.53243838e-02, 2.53243838e-02, ...,\n",
       "          4.77708496e+02, 5.77835510e+02, 5.85021545e+02],\n",
       "         ...,\n",
       "         [2.53243838e-02, 2.53243838e-02, 2.53243838e-02, ...,\n",
       "          2.53243838e-02, 2.53243838e-02, 2.53243838e-02],\n",
       "         [2.88737584e-02, 2.53243838e-02, 2.53243838e-02, ...,\n",
       "          2.53243838e-02, 2.53243838e-02, 2.53243838e-02],\n",
       "         [6.09313324e-02, 3.20724174e-02, 2.53243838e-02, ...,\n",
       "          2.53243838e-02, 2.53243838e-02, 2.53243838e-02]]], dtype=float32),\n",
       " array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]]], dtype=uint16),\n",
       " 'cell',\n",
       " True,\n",
       " (0.396091, 0.079947, 0.079947)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testa_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a563b97a",
   "metadata": {},
   "source": [
    "### **Update to `make_all_metrics_tables`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f45ae9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_all_metrics_tables(source_file: str,\n",
    "                             list_obj_names: List[str],\n",
    "                             list_obj_segs: List[np.ndarray],\n",
    "                             list_intensity_img: List[np.ndarray],\n",
    "                             list_region_names: List[str],\n",
    "                             list_region_segs: List[np.ndarray],\n",
    "                             mask: str,\n",
    "                             dist_centering_obj:str, \n",
    "                             dist_num_bins: int,\n",
    "                             dist_center_on: bool=False,\n",
    "                             dist_keep_center_as_bin: bool=True,\n",
    "                             dist_zernike_degrees: Union[int, None]=None,\n",
    "                             scale: Union[tuple,None] = None,\n",
    "                             include_contact_dist:bool=True,\n",
    "                             skel: List[str] = []):\n",
    "    \"\"\"\n",
    "    Measure the composition, morphology, distribution, and contacts of multiple organelles in a cell\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    source_file: str\n",
    "        file path; this is used for recorder keeping of the file name in the output data tables\n",
    "    list_obj_names: List[str]\n",
    "        a list of object names (strings) that will be measured; this should match the order in list_obj_segs\n",
    "    list_obj_segs: List[np.ndarray]\n",
    "        a list of 3D (ZYX) segmentation np.ndarrays that will be measured per cell; the order should match the list_obj_names \n",
    "    list_intensity_img: List[np.ndarray]\n",
    "        a list of 3D (ZYX) grayscale np.ndarrays that will be used to measure fluoresence intensity in each region and object\n",
    "    list_region_names: List[str]\n",
    "        a list of region names (strings); these should include the mask (entire region being measured - usually the cell) \n",
    "        and other sub-mask regions from which we can meausure the objects in (ex - nucleus, neurites, soma, etc.). It should \n",
    "        also include the centering object used when created the XY distribution bins.\n",
    "        The order should match the list_region_segs\n",
    "    list_region_segs: List[np.ndarray]\n",
    "        a list of 3D (ZYX) binary np.ndarrays of the region masks; the order should match the list_region_names.\n",
    "    mask: str\n",
    "        a str of which region name (contained in the list_region_names list) should be used as the main mask (e.g., cell mask)\n",
    "    dist_centering_obj:str\n",
    "        a str of which region name (contained in the list_region_names list) should be used as the centering object in \n",
    "        get_XY_distribution()\n",
    "    dist_num_bins: int\n",
    "        the number of concentric rings to draw between the centering object and edge of the mask in get_XY_distribution()\n",
    "    dist_center_on: bool=False,\n",
    "        for get_XY_distribution:\n",
    "        True = distribute the bins from the center of the centering object\n",
    "        False = distribute the bins from the edge of the centering object\n",
    "    dist_keep_center_as_bin: bool=True\n",
    "        for get_XY_distribution:\n",
    "        True = include the centering object area when creating the bins\n",
    "        False = do not include the centering object area when creating the bins\n",
    "    dist_zernike_degrees: Union[int, None]=None\n",
    "        for get_XY_distribution:\n",
    "        the number of zernike degrees to include for the zernike shape descriptors; if None, the zernike measurements will not \n",
    "        be included in the output\n",
    "    scale: Union[tuple,None] = None\n",
    "        a tuple that contains the real world dimensions for each dimension in the image (Z, Y, X)\n",
    "    include_contact_dist:bool=True\n",
    "        whether to include the distribution of contact sites in get_contact_metrics_3d(); True = include contact distribution\n",
    "    skel: List[str] = []\n",
    "        The organelles in which skeleton quantification will be ran on, (the list is empty by default)\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    4 Dataframes of measurements of organelle morphology, region morphology, contact morphology, and organelle/contact distributions\n",
    "\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    count = 0\n",
    "\n",
    "    # segmentation image for all masking steps below\n",
    "    mask = list_region_segs[list_region_names.index(mask)]\n",
    "\n",
    "    ######################\n",
    "    # measure cell regions\n",
    "    ######################\n",
    "    # create np.ndarray of intensity images\n",
    "    raw_image = np.stack(list_intensity_img)\n",
    "    \n",
    "    # container for region data\n",
    "    region_tabs = []\n",
    "    for r, r_name in enumerate(list_region_names):\n",
    "        region = list_region_segs[r]\n",
    "        region_metrics = get_region_morphology_3D(region_seg=region, \n",
    "                                                  region_name=r_name,\n",
    "                                                  channel_names=list_obj_names,\n",
    "                                                  intensity_img=raw_image, \n",
    "                                                  mask=mask,\n",
    "                                                  scale=scale)\n",
    "        region_tabs.append(region_metrics)\n",
    "\n",
    "    ##############################################################\n",
    "    # loop through all organelles to collect measurements for each\n",
    "    ##############################################################\n",
    "    # containers to collect per organelle information\n",
    "    org_tabs = []\n",
    "    dist_tabs = []\n",
    "    XY_bins = []\n",
    "    XY_wedges = []\n",
    "\n",
    "    for j, target in enumerate(list_obj_names):\n",
    "        \n",
    "        # organelle intensity image\n",
    "        org_img = list_intensity_img[j]\n",
    "\n",
    "        # organelle segmentation\n",
    "        if target == 'ER':\n",
    "            # ensure ER is only one object\n",
    "            org_obj = (list_obj_segs[j] > 0).astype(np.uint16)\n",
    "        else:\n",
    "            org_obj = list_obj_segs[j]\n",
    "\n",
    "        ### NRM DEBUGGING CODE ###\n",
    "        print(list_obj_names)\n",
    "        \n",
    "        print(f'target={target} in this iteration, channel {j}')\n",
    "        \n",
    "        viewer.add_image(org_img,\n",
    "                         name = f\"{target} intensity\",\n",
    "                         scale = scale)\n",
    "        \n",
    "        viewer.add_image(org_obj,\n",
    "                         name = f\"{target} object (seg)\",\n",
    "                         scale = scale)\n",
    "        \n",
    "         ### ################# ###\n",
    "\n",
    "        ##########################################################\n",
    "        # measure organelle morphology & number of objs contacting\n",
    "        ##########################################################\n",
    "         ### If the organelle is in the skel list, then skeletonization will be ran for that organelle\n",
    "        ### By default the list is empty\n",
    "        if target in skel:\n",
    "            skel_met = True\n",
    "        else:\n",
    "            skel_met = False\n",
    "\n",
    "        if target == \"ER\":\n",
    "            \n",
    "            testb_inputs = (org_obj,\n",
    "                         target,\n",
    "                         org_img,\n",
    "                         mask,\n",
    "                         mask_name,\n",
    "                         scale,\n",
    "                         skel_met)\n",
    "            \n",
    "            print(\"org obj test\")\n",
    "            print(np.all(testa_inputs[0] == testb_inputs[0]))\n",
    "\n",
    "            print(\"target test\")\n",
    "            print(testa_inputs[1], testb_inputs[1])\n",
    "            print(testa_inputs[1] == testb_inputs[1])\n",
    "\n",
    "            print(\"org img test\")\n",
    "            print(np.all(testa_inputs[2] == testb_inputs[2]))\n",
    "\n",
    "            print(\"mask test\")\n",
    "            viewer.add_image(testa_inputs[3],\n",
    "                             scale = scale,\n",
    "                             name = \"test A mask\")\n",
    "            viewer.add_image(testb_inputs[3],\n",
    "                             scale = scale,\n",
    "                             name = \"test B mask\")\n",
    "            print(np.all(testa_inputs[3] == testb_inputs[3]))\n",
    "\n",
    "            print(\"mask name test\")\n",
    "            print(np.all(testa_inputs[4] == testb_inputs[4]))\n",
    "\n",
    "            print(\"scale test\")\n",
    "            print(testa_inputs[5], testb_inputs[5])\n",
    "            print(testa_inputs[5] == testb_inputs[5])\n",
    "\n",
    "            print(\"skel test\")\n",
    "            print(testa_inputs[6], testb_inputs[6])\n",
    "            print(testa_inputs[6] == testb_inputs[6])\n",
    "            \n",
    "\n",
    "\n",
    "        \n",
    "        org_metrics = _get_morphology_metrics(segmentation_img=org_obj, \n",
    "                                            seg_name=target,\n",
    "                                            intensity_img=org_img, \n",
    "                                            mask = mask,\n",
    "                                            mask_name = mask_name,\n",
    "                                            scale=scale,\n",
    "                                            skel_met = skel_met)\n",
    "        \n",
    "\n",
    "\n",
    "        ### org_metrics.insert(loc=0,column='cell',value=1) \n",
    "        # ^^^ saving this thought for later when someone might have more than one cell per image.\n",
    "        # Not sure how they analysis process would fit in our pipelines as they exist now. \n",
    "        # Maybe here, iterating though the index of the masks above all of this and using that index as the cell number?\n",
    "\n",
    "        org_tabs.append(org_metrics)\n",
    "\n",
    "        ################################\n",
    "        # measure organelle distribution \n",
    "        ################################\n",
    "        centering = list_region_segs[list_region_names.index(dist_centering_obj)]\n",
    "        XY_org_distribution, XY_bin_masks, XY_wedge_masks = get_XY_distribution(mask=mask,\n",
    "                                                                                centering_obj=centering,\n",
    "                                                                                obj=org_obj,\n",
    "                                                                                obj_name=target,\n",
    "                                                                                scale=scale,\n",
    "                                                                                num_bins=dist_num_bins,\n",
    "                                                                                center_on=dist_center_on,\n",
    "                                                                                keep_center_as_bin=dist_keep_center_as_bin,\n",
    "                                                                                zernike_degrees=dist_zernike_degrees)\n",
    "        Z_org_distribution = get_Z_distribution(mask=mask, \n",
    "                                                obj=org_obj,\n",
    "                                                obj_name=target,\n",
    "                                                center_obj=centering,\n",
    "                                                scale=scale)\n",
    "        \n",
    "        org_distribution_metrics = pd.merge(XY_org_distribution, Z_org_distribution,on=[\"object\", \"scale\"])\n",
    "\n",
    "        dist_tabs.append(org_distribution_metrics)\n",
    "        XY_bins.append(XY_bin_masks)\n",
    "        XY_wedges.append(XY_wedge_masks)\n",
    "\n",
    "    #######################################\n",
    "    # collect non-redundant contact metrics \n",
    "    #######################################\n",
    "    # list the non-redundant organelle pairs\n",
    "    contact_combos = list(itertools.combinations(list_obj_names, 2))\n",
    "\n",
    "    # container to keep contact data in\n",
    "    contact_tabs = []\n",
    "\n",
    "    # loop through each pair and measure contacts\n",
    "    for pair in contact_combos:\n",
    "        # pair names\n",
    "        a_name = pair[0]\n",
    "        b_name = pair[1]\n",
    "\n",
    "        # segmentations to measure\n",
    "        if a_name == 'ER':\n",
    "            # ensure ER is only one object\n",
    "            a = (list_obj_segs[list_obj_names.index(a_name)] > 0).astype(np.uint16)\n",
    "        else:\n",
    "            a = list_obj_segs[list_obj_names.index(a_name)]\n",
    "        \n",
    "        if b_name == 'ER':\n",
    "            # ensure ER is only one object\n",
    "            b = (list_obj_segs[list_obj_names.index(b_name)] > 0).astype(np.uint16)\n",
    "        else:\n",
    "            b = list_obj_segs[list_obj_names.index(b_name)]\n",
    "        \n",
    "\n",
    "        if include_contact_dist == True:\n",
    "            contact_tab, contact_dist_tab = get_contact_metrics_3D(a, a_name, \n",
    "                                                                   b, b_name, \n",
    "                                                                   mask, \n",
    "                                                                   scale, \n",
    "                                                                   include_dist=include_contact_dist,\n",
    "                                                                   dist_centering_obj=centering,\n",
    "                                                                   dist_num_bins=dist_num_bins,\n",
    "                                                                   dist_zernike_degrees=dist_zernike_degrees,\n",
    "                                                                   dist_center_on=dist_center_on,\n",
    "                                                                   dist_keep_center_as_bin=dist_keep_center_as_bin)\n",
    "            dist_tabs.append(contact_dist_tab)\n",
    "        else:\n",
    "            contact_tab = get_contact_metrics_3D(a, a_name, \n",
    "                                                 b, b_name, \n",
    "                                                 mask, \n",
    "                                                 scale, \n",
    "                                                 include_dist=include_contact_dist)\n",
    "        contact_tabs.append(contact_tab)\n",
    "\n",
    "\n",
    "    ###########################################\n",
    "    # combine all tabs into one table per type:\n",
    "    ###########################################\n",
    "    final_org_tab = pd.concat(org_tabs, ignore_index=True)\n",
    "    final_org_tab.insert(loc=0,column='image_name',value=source_file.stem)\n",
    "\n",
    "    final_contact_tab = pd.concat(contact_tabs, ignore_index=True)\n",
    "    final_contact_tab.insert(loc=0,column='image_name',value=source_file.stem)\n",
    "\n",
    "    combined_dist_tab = pd.concat(dist_tabs, ignore_index=True)\n",
    "    combined_dist_tab.insert(loc=0,column='image_name',value=source_file.stem)\n",
    "\n",
    "    final_region_tab = pd.concat(region_tabs, ignore_index=True)\n",
    "    final_region_tab.insert(loc=0,column='image_name',value=source_file.stem)\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"It took {(end-start)/60} minutes to quantify one image.\")\n",
    "    return final_org_tab, final_contact_tab, combined_dist_tab, final_region_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d3c267d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OLD CODE\n",
    "\n",
    "# def _make_all_metrics_tables(source_file: str,\n",
    "#                              list_obj_names: List[str],\n",
    "#                              list_obj_segs: List[np.ndarray],\n",
    "#                              list_intensity_img: List[np.ndarray],\n",
    "#                              list_region_names: List[str],\n",
    "#                              list_region_segs: List[np.ndarray],\n",
    "#                              mask: str,\n",
    "#                              dist_centering_obj:str, \n",
    "#                              dist_num_bins: int,\n",
    "#                              dist_center_on: bool=False,\n",
    "#                              dist_keep_center_as_bin: bool=True,\n",
    "#                              dist_zernike_degrees: Union[int, None]=None,\n",
    "#                              scale: Union[tuple,None] = None,\n",
    "#                              include_contact_dist:bool=True,\n",
    "#                              splitter:str=\"_\",\n",
    "#                              skel: List[str] = []):\n",
    "#     \"\"\"\n",
    "#     Measure the composition, morphology, distribution, and contacts of multiple organelles in a cell\n",
    "\n",
    "#     Parameters:\n",
    "#     ----------\n",
    "#     source_file: str\n",
    "#         file path; this is used for recorder keeping of the file name in the output data tables\n",
    "#     list_obj_names: List[str]\n",
    "#         a list of object names (strings) that will be measured; this should match the order in list_obj_segs\n",
    "#     list_obj_segs: List[np.ndarray]\n",
    "#         a list of 3D (ZYX) segmentation np.ndarrays that will be measured per cell; the order should match the list_obj_names \n",
    "#     list_intensity_img: List[np.ndarray]\n",
    "#         a list of 3D (ZYX) grayscale np.ndarrays that will be used to measure fluoresence intensity in each region and object\n",
    "#     list_region_names: List[str]\n",
    "#         a list of region names (strings); these should include the mask (entire region being measured - usually the cell) \n",
    "#         and other sub-mask regions from which we can meausure the objects in (ex - nucleus, neurites, soma, etc.). It should \n",
    "#         also include the centering object used when created the XY distribution bins.\n",
    "#         The order should match the list_region_segs\n",
    "#     list_region_segs: List[np.ndarray]\n",
    "#         a list of 3D (ZYX) binary np.ndarrays of the region masks; the order should match the list_region_names.\n",
    "#     mask: str\n",
    "#         a str of which region name (contained in the list_region_names list) should be used as the main mask (e.g., cell mask)\n",
    "#     dist_centering_obj:str\n",
    "#         a str of which region name (contained in the list_region_names list) should be used as the centering object in \n",
    "#         get_XY_distribution()\n",
    "#     dist_num_bins: int\n",
    "#         the number of concentric rings to draw between the centering object and edge of the mask in get_XY_distribution()\n",
    "#     dist_center_on: bool=False,\n",
    "#         for get_XY_distribution:\n",
    "#         True = distribute the bins from the center of the centering object\n",
    "#         False = distribute the bins from the edge of the centering object\n",
    "#     dist_keep_center_as_bin: bool=True\n",
    "#         for get_XY_distribution:\n",
    "#         True = include the centering object area when creating the bins\n",
    "#         False = do not include the centering object area when creating the bins\n",
    "#     dist_zernike_degrees: Union[int, None]=None\n",
    "#         for get_XY_distribution:\n",
    "#         the number of zernike degrees to include for the zernike shape descriptors; if None, the zernike measurements will not \n",
    "#         be included in the output\n",
    "#     scale: Union[tuple,None] = None\n",
    "#         a tuple that contains the real world dimensions for each dimension in the image (Z, Y, X)\n",
    "#     include_contact_dist:bool=True\n",
    "#         whether to include the distribution of contact sites in get_contact_metrics_3d(); True = include contact distribution\n",
    "#     skel: List[str] = []\n",
    "#         The organelles in which skeleton quantification will be ran on, (the list is empty by default)\n",
    "\n",
    "#     Returns:\n",
    "#     ----------\n",
    "#     4 Dataframes of measurements of organelle morphology, region morphology, contact morphology, and organelle/contact distributions\n",
    "\n",
    "#     \"\"\"\n",
    "#     start = time.time()\n",
    "#     count = 0\n",
    "\n",
    "#     # segmentation image for all masking steps below\n",
    "#     mask = list_region_segs[list_region_names.index(mask)]\n",
    "\n",
    "#     # containers to collect per organelle information\n",
    "#     org_tabs = []\n",
    "#     dist_tabs = []\n",
    "#     XY_bins = []\n",
    "#     XY_wedges = []\n",
    "\n",
    "#     #############################\n",
    "#     # Measure Organelle Contacts \n",
    "#     #############################\n",
    "#     if len(list_obj_names) >=2:\n",
    "#         contact_tabs = []\n",
    "#         org_dict = make_dict(obj_names=list_obj_names,\n",
    "#                              obj_segs=list_obj_segs)\n",
    "#         all_conts, non_red_conts=multi_contact(org_segs=org_dict,\n",
    "#                                                organelles=list_obj_names,\n",
    "#                                                splitter=splitter,\n",
    "#                                                redundancy=False)\n",
    "#         if include_contact_dist:\n",
    "#             centering = list_region_segs[list_region_names.index(dist_centering_obj)]\n",
    "#             for orgs, site in all_conts.items():\n",
    "#                 cont_tab, dist_tab = get_contact_metrics_3D(orgs = orgs,\n",
    "#                                         organelle_segs = org_dict,\n",
    "#                                         mask = mask,\n",
    "#                                         splitter = splitter,\n",
    "#                                         scale = scale,\n",
    "#                                         include_dist = include_contact_dist, \n",
    "#                                         dist_centering_obj = centering,\n",
    "#                                         dist_num_bins = dist_num_bins,\n",
    "#                                         dist_zernike_degrees =  dist_zernike_degrees,\n",
    "#                                         dist_center_on = dist_center_on,\n",
    "#                                         dist_keep_center_as_bin = dist_keep_center_as_bin)\n",
    "#                 # for d_tabs,c_tabs in zip(dist_tab,cont_tab):\n",
    "#                 #     dist_tabs.append(d_tabs)\n",
    "#                 #     contact_tabs.append(c_tabs)\n",
    "#                 dist_tabs.append(dist_tab[0])\n",
    "#                 contact_tabs.append(cont_tab)\n",
    "#             ##########################################\n",
    "#             # Collecting empty distance table metrics\n",
    "#             ##########################################\n",
    "#             all_pos = []\n",
    "#             for n in list(map(lambda x:x+2, (range(len(list_obj_names)-1)))):\n",
    "#                 all_pos += itertools.combinations(list_obj_names, n)\n",
    "#             possib = [splitter.join(cont) for cont in all_pos if not inkeys(all_conts, splitter.join(cont), splitter)]\n",
    "#             del all_conts, non_red_conts\n",
    "#             for con in possib:\n",
    "#                 dist_tabs.append(get_empty_contact_dist_tabs(mask=mask,\n",
    "#                                                              name=con,\n",
    "#                                                              dist_centering_obj=centering,\n",
    "#                                                              scale=scale,\n",
    "#                                                              dist_zernike_degrees=dist_zernike_degrees,\n",
    "#                                                              dist_center_on=dist_center_on,\n",
    "#                                                              dist_keep_center_as_bin=dist_keep_center_as_bin,\n",
    "#                                                              dist_num_bins=dist_num_bins))\n",
    "#             del possib\n",
    "#         else:\n",
    "#             for orgs, site in all_conts.items():\n",
    "#                 cont_tab = get_contact_metrics_3D(orgs = orgs,\n",
    "#                                                    organelle_segs = org_dict,\n",
    "#                                                    mask = mask,\n",
    "#                                                    splitter = splitter,\n",
    "#                                                    scale = scale,\n",
    "#                                                    include_dist = False)\n",
    "#             del all_conts, non_red_conts\n",
    "\n",
    "#     ######################\n",
    "#     # measure cell regions\n",
    "#     ######################\n",
    "#     # create np.ndarray of intensity images\n",
    "#     raw_image = np.stack(list_intensity_img)\n",
    "\n",
    "#     # container for region data\n",
    "#     region_tabs = []\n",
    "#     for r, r_name in enumerate(list_region_names):\n",
    "#         region = list_region_segs[r]\n",
    "#         region_metrics = get_region_morphology_3D(region_seg=region, \n",
    "#                                                   region_name=r_name,\n",
    "#                                                   channel_names=list_obj_names,\n",
    "#                                                   intensity_img=raw_image, \n",
    "#                                                   mask=mask,\n",
    "#                                                   scale=scale)\n",
    "#         region_tabs.append(region_metrics)\n",
    "\n",
    "#     ##############################################################\n",
    "#     # loop through all organelles to collect measurements for each\n",
    "#     ##############################################################\n",
    "\n",
    "#     for j, target in enumerate(list_obj_names):\n",
    "#         print(target)\n",
    "#         # organelle intensity image\n",
    "#         org_img = list_intensity_img[j]\n",
    "\n",
    "#         # organelle segmentation\n",
    "#         if target == 'ER':\n",
    "#             # ensure ER is only one object\n",
    "#             org_obj = (list_obj_segs[j] > 0).astype(np.uint16)\n",
    "#         else:\n",
    "#             org_obj = list_obj_segs[j]\n",
    "\n",
    "#         ##########################################################\n",
    "#         # measure organelle morphology\n",
    "#         ##########################################################\n",
    "\n",
    "#         ### If the organelle is in the skel list, then skeletonization will be ran for that organelle\n",
    "#         ### By default the list is empty\n",
    "#         if target in skel:\n",
    "#             skel_met = True\n",
    "#         else:\n",
    "#             skel_met = False\n",
    "#         org_metrics = _get_org_morphology_3D(segmentation_img=org_obj, \n",
    "#                                             seg_name=target,\n",
    "#                                             intensity_img=org_img, \n",
    "#                                             mask=mask,\n",
    "#                                             scale=scale,\n",
    "#                                             skel_met = skel_met)\n",
    "\n",
    "#         ### org_metrics.insert(loc=0,column='cell',value=1) \n",
    "#         # ^^^ saving this thought for later when someone might have more than one cell per image.\n",
    "#         # Not sure how they analysis process would fit in our pipelines as they exist now. \n",
    "#         # Maybe here, iterating though the index of the masks above all of this and using that index as the cell number?\n",
    "\n",
    "#         # TODO: find a better way to quantify the number and area of contacts per organelle\n",
    "#             # I think it can be done during summarizing based on the label and object values in the contact sheet\n",
    "#         # for i, nmi in enumerate(list_obj_names):\n",
    "#         #     if i != j:\n",
    "#         #         if target == 'ER':\n",
    "#         #             b = (list_obj_segs[i] > 0).astype(np.uint16)\n",
    "#         #         else:\n",
    "#         #             b = list_obj_segs[i]\n",
    "            \n",
    "#         #         ov = []\n",
    "#         #         b_labs = []\n",
    "#         #         labs = []\n",
    "#         #         for idx, lab in enumerate(org_metrics[\"label\"]):\n",
    "#         #             xyz = tuple(rp[idx].coords.T)\n",
    "#         #             cmp_org = b[xyz]\n",
    "                    \n",
    "#         #             # total area (in voxels or real world units) where these two orgs overlap within the cell\n",
    "#         #             if scale != None:\n",
    "#         #                 overlap = sum(cmp_org > 0)*scale[0]*scale[1]*scale[2]\n",
    "#         #             else:\n",
    "#         #                 # total number of overlapping pixels\n",
    "#         #                 overlap = sum(cmp_org > 0)\n",
    "#         #                 # overlap?\n",
    "                    \n",
    "#         #             # which b organelles are involved in that overlap\n",
    "#         #             labs_b = cmp_org[cmp_org > 0]\n",
    "#         #             b_js = np.unique(labs_b).tolist()\n",
    "\n",
    "#         #             # if overlap > 0:\n",
    "#         #             labs.append(lab) # labs.append(lab)\n",
    "#         #             ov.append(overlap)\n",
    "#         #             b_labs.append(b_js)\n",
    "#         #         org_metrics[f\"{nmi}_overlap\"] = ov\n",
    "#         #         org_metrics[f\"{nmi}_labels\"] = b_labs \n",
    "\n",
    "#         org_tabs.append(org_metrics)\n",
    "\n",
    "#         ################################\n",
    "#         # measure organelle distribution \n",
    "#         ################################\n",
    "#         centering = list_region_segs[list_region_names.index(dist_centering_obj)]\n",
    "#         XY_org_distribution, XY_bin_masks, XY_wedge_masks = get_XY_distribution(mask=mask,\n",
    "#                                                                                 centering_obj=centering,\n",
    "#                                                                                 obj=org_obj,\n",
    "#                                                                                 obj_name=target,\n",
    "#                                                                                 scale=scale,\n",
    "#                                                                                 num_bins=dist_num_bins,\n",
    "#                                                                                 center_on=dist_center_on,\n",
    "#                                                                                 keep_center_as_bin=dist_keep_center_as_bin,\n",
    "#                                                                                 zernike_degrees=dist_zernike_degrees)\n",
    "#         Z_org_distribution = get_Z_distribution(mask=mask, \n",
    "#                                                 obj=org_obj,\n",
    "#                                                 obj_name=target,\n",
    "#                                                 center_obj=centering,\n",
    "#                                                 scale=scale)\n",
    "        \n",
    "#         org_distribution_metrics = pd.merge(XY_org_distribution, Z_org_distribution,on=[\"object\", \"scale\"])\n",
    "\n",
    "#         dist_tabs.append(org_distribution_metrics)\n",
    "#         XY_bins.append(XY_bin_masks)\n",
    "#         XY_wedges.append(XY_wedge_masks)\n",
    "\n",
    "#     # # list the non-redundant organelle pairs\n",
    "#     # contact_combos = list(itertools.combinations(list_obj_names, 2))\n",
    "\n",
    "#     # # container to keep contact data in\n",
    "#     # contact_tabs = []\n",
    "\n",
    "#     # # loop through each pair and measure contacts\n",
    "#     # for pair in contact_combos:\n",
    "#     #     # pair names\n",
    "#     #     a_name = pair[0]\n",
    "#     #     b_name = pair[1]\n",
    "\n",
    "#     #     # segmentations to measure\n",
    "#     #     if a_name == 'ER':\n",
    "#     #         # ensure ER is only one object\n",
    "#     #         a = (list_obj_segs[list_obj_names.index(a_name)] > 0).astype(np.uint16)\n",
    "#     #     else:\n",
    "#     #         a = list_obj_segs[list_obj_names.index(a_name)]\n",
    "        \n",
    "#     #     if b_name == 'ER':\n",
    "#     #         # ensure ER is only one object\n",
    "#     #         b = (list_obj_segs[list_obj_names.index(b_name)] > 0).astype(np.uint16)\n",
    "#     #     else:\n",
    "#     #         b = list_obj_segs[list_obj_names.index(b_name)]\n",
    "        \n",
    "\n",
    "#     #     if include_contact_dist == True:\n",
    "#     #         contact_tab, contact_dist_tab = get_contact_metrics_3D(a, a_name, \n",
    "#     #                                                                b, b_name, \n",
    "#     #                                                                mask, \n",
    "#     #                                                                scale, \n",
    "#     #                                                                include_dist=include_contact_dist,\n",
    "#     #                                                                dist_centering_obj=centering,\n",
    "#     #                                                                dist_num_bins=dist_num_bins,\n",
    "#     #                                                                dist_zernike_degrees=dist_zernike_degrees,\n",
    "#     #                                                                dist_center_on=dist_center_on,\n",
    "#     #                                                                dist_keep_center_as_bin=dist_keep_center_as_bin)\n",
    "#     #         dist_tabs.append(contact_dist_tab)\n",
    "#     #     else:\n",
    "#     #         contact_tab = get_contact_metrics_3D(a, a_name, \n",
    "#     #                                              b, b_name, \n",
    "#     #                                              mask, \n",
    "#     #                                              scale, \n",
    "#     #                                              include_dist=include_contact_dist)\n",
    "#     #     contact_tabs.append(contact_tab)\n",
    "\n",
    "\n",
    "#     ###########################################\n",
    "#     # combine all tabs into one table per type:\n",
    "#     ###########################################\n",
    "#     final_org_tab = pd.concat(org_tabs, ignore_index=True)\n",
    "#     final_org_tab.insert(loc=0,column='image_name',value=source_file.stem)\n",
    "\n",
    "#     final_contact_tab = pd.concat(contact_tabs, ignore_index=True)\n",
    "#     final_contact_tab.insert(loc=0,column='image_name',value=source_file.stem)\n",
    "\n",
    "#     combined_dist_tab = pd.concat(dist_tabs, ignore_index=True)\n",
    "#     combined_dist_tab.insert(loc=0,column='image_name',value=source_file.stem)\n",
    "\n",
    "#     final_region_tab = pd.concat(region_tabs, ignore_index=True)\n",
    "#     final_region_tab.insert(loc=0,column='image_name',value=source_file.stem)\n",
    "\n",
    "#     end = time.time()\n",
    "#     print(f\"It took {(end-start)/60} minutes to quantify one image.\")\n",
    "#     return final_org_tab, final_contact_tab, combined_dist_tab, final_region_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0e89396e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_contact_metrics_3D(orgs=orgs,\n",
    "#                                                             site=site,\n",
    "#                                                             HO = non_red_conts[orgs],\n",
    "#                                                             organelle_segs=org_dict,\n",
    "#                                                             mask=mask,\n",
    "#                                                             splitter=splitter,\n",
    "#                                                             scale=scale,\n",
    "#                                                             include_dist=include_contact_dist,\n",
    "#                                                             dist_centering_obj=x,\n",
    "#                                                             dist_num_bins=dist_num_bins,\n",
    "#                                                             dist_zernike_degrees=dist_zernike_degrees,\n",
    "#                                                             dist_center_on=dist_center_on,\n",
    "#                                                             dist_keep_center_as_bin=dist_keep_center_as_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d125b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder the organelles to match channel indicies\n",
    "region_names = ['nuc', 'cell']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "912bf1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import napari\n",
    "viewer = napari.Viewer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d57db52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test code to confirm the order\n",
    "\n",
    "for i in range(len(['LD', 'ER', 'golgi', 'lyso', 'mito', 'perox'])):\n",
    "    viewer.add_image(organelles[i],\n",
    "                     name = f\"seg_{org_list[i]}\")\n",
    "    viewer.add_image(intensities[i],\n",
    "                     name = f\"int_{org_list[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e5b65336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LD', 'ER', 'golgi', 'lyso', 'mito', 'perox']\n",
      "target=LD in this iteration, channel 0\n",
      "Warning(s) suppressed while quantifying LD. See 'method_morphology.ipynb' notebook for more details.\n",
      "['LD', 'ER', 'golgi', 'lyso', 'mito', 'perox']\n",
      "target=ER in this iteration, channel 1\n",
      "org obj test\n",
      "True\n",
      "target test\n",
      "ER ER\n",
      "True\n",
      "org img test\n",
      "True\n",
      "mask test\n",
      "True\n",
      "mask name test\n",
      "True\n",
      "scale test\n",
      "True (0.396091, 0.079947, 0.079947)\n",
      "False\n",
      "skel test\n",
      "(0.396091, 0.079947, 0.079947) True\n",
      "False\n",
      "Part One took 12.577378988265991 sec(s)\n",
      "Total Time: 12.577901601791382 sec(s)\n",
      "Warning(s) suppressed while quantifying ER. See 'method_morphology.ipynb' notebook for more details.\n",
      "['LD', 'ER', 'golgi', 'lyso', 'mito', 'perox']\n",
      "target=golgi in this iteration, channel 2\n",
      "Warning(s) suppressed while quantifying golgi. See 'method_morphology.ipynb' notebook for more details.\n",
      "['LD', 'ER', 'golgi', 'lyso', 'mito', 'perox']\n",
      "target=lyso in this iteration, channel 3\n",
      "Warning(s) suppressed while quantifying lyso. See 'method_morphology.ipynb' notebook for more details.\n",
      "['LD', 'ER', 'golgi', 'lyso', 'mito', 'perox']\n",
      "target=mito in this iteration, channel 4\n",
      "Warning(s) suppressed while quantifying mito. See 'method_morphology.ipynb' notebook for more details.\n",
      "['LD', 'ER', 'golgi', 'lyso', 'mito', 'perox']\n",
      "target=perox in this iteration, channel 5\n",
      "Warning(s) suppressed while quantifying perox. See 'method_morphology.ipynb' notebook for more details.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_final_org_tab, test_final_contact_tab, test_combined_dist_tab, test_final_regions_tab \u001b[38;5;241m=\u001b[39m \u001b[43m_make_all_metrics_tables\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtest_img_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                                                                                                                      \u001b[49m\u001b[43mlist_obj_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morg_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                                                                                                                      \u001b[49m\u001b[43mlist_obj_segs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43morganelles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                                                                                                                      \u001b[49m\u001b[43mlist_intensity_img\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mintensities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                                                                                                                      \u001b[49m\u001b[43mlist_region_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregion_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                                                                                                                      \u001b[49m\u001b[43mlist_region_segs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                                                                                                                      \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcell\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                                                                                                                      \u001b[49m\u001b[43mdist_centering_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnuc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                                                                                                                      \u001b[49m\u001b[43mdist_num_bins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m                                                                                                                      \u001b[49m\u001b[43mdist_center_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                                                                                                                      \u001b[49m\u001b[43mdist_keep_center_as_bin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                                                                                                                      \u001b[49m\u001b[43mdist_zernike_degrees\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                                                                                                                      \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                                                                                                                      \u001b[49m\u001b[43minclude_contact_dist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                                                                                                                      \u001b[49m\u001b[43mskel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mER\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[35], line 252\u001b[0m, in \u001b[0;36m_make_all_metrics_tables\u001b[1;34m(source_file, list_obj_names, list_obj_segs, list_intensity_img, list_region_names, list_region_segs, mask, dist_centering_obj, dist_num_bins, dist_center_on, dist_keep_center_as_bin, dist_zernike_degrees, scale, include_contact_dist, skel)\u001b[0m\n\u001b[0;32m    248\u001b[0m     b \u001b[38;5;241m=\u001b[39m list_obj_segs[list_obj_names\u001b[38;5;241m.\u001b[39mindex(b_name)]\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_contact_dist \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 252\u001b[0m     contact_tab, contact_dist_tab \u001b[38;5;241m=\u001b[39m \u001b[43mget_contact_metrics_3D\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43minclude_dist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_contact_dist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43mdist_centering_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcentering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43mdist_num_bins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdist_num_bins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43mdist_zernike_degrees\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdist_zernike_degrees\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43mdist_center_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdist_center_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43mdist_keep_center_as_bin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdist_keep_center_as_bin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    262\u001b[0m     dist_tabs\u001b[38;5;241m.\u001b[39mappend(contact_dist_tab)\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Documents\\CohenLab\\scohen_lab_repo\\infer-subc\\infer_subc\\quantification\\stats.py:931\u001b[0m, in \u001b[0;36mget_contact_metrics_3D\u001b[1;34m(a, a_name, b, b_name, mask, scale, include_dist, dist_centering_obj, dist_num_bins, dist_zernike_degrees, dist_center_on, dist_keep_center_as_bin)\u001b[0m\n\u001b[0;32m    926\u001b[0m properties \u001b[38;5;241m=\u001b[39m properties \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextent\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meuler_number\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msolidity\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maxis_major_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mslice\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;66;03m# \"feret_diameter_max\", \"axis_minor_length\", \u001b[39;00m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;66;03m##################\u001b[39;00m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;66;03m## RUN REGIONPROPS\u001b[39;00m\n\u001b[0;32m    930\u001b[0m \u001b[38;5;66;03m##################\u001b[39;00m\n\u001b[1;32m--> 931\u001b[0m props \u001b[38;5;241m=\u001b[39m \u001b[43mregionprops_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintensity_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproperties\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_properties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspacing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    933\u001b[0m \u001b[38;5;66;03m##################################################################\u001b[39;00m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;66;03m## RUN SURFACE AREA FUNCTION SEPARATELY AND APPEND THE PROPS_TABLE\u001b[39;00m\n\u001b[0;32m    935\u001b[0m \u001b[38;5;66;03m##################################################################\u001b[39;00m\n\u001b[0;32m    936\u001b[0m surface_area_tab \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(surface_area_from_props(labels, props, scale))\n",
      "File \u001b[1;32mc:\\Users\\redre\\anaconda3\\envs\\infer-subc-sc\\lib\\site-packages\\skimage\\measure\\_regionprops.py:1045\u001b[0m, in \u001b[0;36mregionprops_table\u001b[1;34m(label_image, intensity_image, properties, cache, separator, extra_properties, spacing)\u001b[0m\n\u001b[0;32m   1041\u001b[0m     out_d \u001b[38;5;241m=\u001b[39m _props_to_dict(regions, properties\u001b[38;5;241m=\u001b[39mproperties,\n\u001b[0;32m   1042\u001b[0m                            separator\u001b[38;5;241m=\u001b[39mseparator)\n\u001b[0;32m   1043\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: v[:\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m out_d\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m-> 1045\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_props_to_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproperties\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseparator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseparator\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\redre\\anaconda3\\envs\\infer-subc-sc\\lib\\site-packages\\skimage\\measure\\_regionprops.py:854\u001b[0m, in \u001b[0;36m_props_to_dict\u001b[1;34m(regions, properties, separator)\u001b[0m\n\u001b[0;32m    852\u001b[0m     column_buffer \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(n, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[1;32m--> 854\u001b[0m         column_buffer[i] \u001b[38;5;241m=\u001b[39m \u001b[43mregions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprop\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    855\u001b[0m     out[orig_prop] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcopy(column_buffer)\n\u001b[0;32m    856\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\redre\\anaconda3\\envs\\infer-subc-sc\\lib\\site-packages\\skimage\\measure\\_regionprops.py:722\u001b[0m, in \u001b[0;36mRegionProperties.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m--> 722\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    723\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    724\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "File \u001b[1;32mc:\\Users\\redre\\anaconda3\\envs\\infer-subc-sc\\lib\\site-packages\\skimage\\measure\\_regionprops.py:629\u001b[0m, in \u001b[0;36mRegionProperties.solidity\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msolidity\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marea \u001b[38;5;241m/\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marea_convex\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\redre\\anaconda3\\envs\\infer-subc-sc\\lib\\site-packages\\skimage\\measure\\_regionprops.py:224\u001b[0m, in \u001b[0;36m_cached.<locals>.wrapper\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m    221\u001b[0m prop \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ((prop \u001b[38;5;129;01min\u001b[39;00m cache) \u001b[38;5;129;01mand\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_cache_active):\n\u001b[1;32m--> 224\u001b[0m     cache[prop] \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cache[prop]\n",
      "File \u001b[1;32mc:\\Users\\redre\\anaconda3\\envs\\infer-subc-sc\\lib\\site-packages\\skimage\\measure\\_regionprops.py:424\u001b[0m, in \u001b[0;36mRegionProperties.area_convex\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;129m@_cached\u001b[39m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21marea_convex\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_convex\u001b[49m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pixel_area\n",
      "File \u001b[1;32mc:\\Users\\redre\\anaconda3\\envs\\infer-subc-sc\\lib\\site-packages\\skimage\\measure\\_regionprops.py:224\u001b[0m, in \u001b[0;36m_cached.<locals>.wrapper\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m    221\u001b[0m prop \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ((prop \u001b[38;5;129;01min\u001b[39;00m cache) \u001b[38;5;129;01mand\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_cache_active):\n\u001b[1;32m--> 224\u001b[0m     cache[prop] \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cache[prop]\n",
      "File \u001b[1;32mc:\\Users\\redre\\anaconda3\\envs\\infer-subc-sc\\lib\\site-packages\\skimage\\measure\\_regionprops.py:430\u001b[0m, in \u001b[0;36mRegionProperties.image_convex\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;129m@_cached\u001b[39m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimage_convex\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmorphology\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvex_hull\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convex_hull_image\n\u001b[1;32m--> 430\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvex_hull_image\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\redre\\anaconda3\\envs\\infer-subc-sc\\lib\\site-packages\\skimage\\morphology\\convex_hull.py:159\u001b[0m, in \u001b[0;36mconvex_hull_image\u001b[1;34m(image, offset_coordinates, tolerance, include_borders)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    156\u001b[0m     gridcoords \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(np\u001b[38;5;241m.\u001b[39mmgrid[\u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mslice\u001b[39m, image\u001b[38;5;241m.\u001b[39mshape))],\n\u001b[0;32m    157\u001b[0m                             (ndim, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 159\u001b[0m     coords_in_hull \u001b[38;5;241m=\u001b[39m \u001b[43m_check_coords_in_hull\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgridcoords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mhull\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mequations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m     mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(coords_in_hull, image\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mask\n",
      "File \u001b[1;32mc:\\Users\\redre\\anaconda3\\envs\\infer-subc-sc\\lib\\site-packages\\skimage\\morphology\\convex_hull.py:65\u001b[0m, in \u001b[0;36m_check_coords_in_hull\u001b[1;34m(gridcoords, hull_equations, tolerance)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# A point is in the hull if it satisfies all of the hull's inequalities\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_hull_equations):\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Tests a hyperplane equation on all coordinates of volume\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m     \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhull_equations\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mndim\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgridcoords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdot_array\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m     np\u001b[38;5;241m.\u001b[39madd(dot_array, hull_equations[idx, ndim:], out\u001b[38;5;241m=\u001b[39mtest_ineq_temp)\n\u001b[0;32m     67\u001b[0m     np\u001b[38;5;241m.\u001b[39mless(test_ineq_temp, tolerance, out\u001b[38;5;241m=\u001b[39mcoords_single_ineq)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_final_org_tab, test_final_contact_tab, test_combined_dist_tab, test_final_regions_tab = _make_all_metrics_tables(source_file= test_img_name,\n",
    "                                                                                                                      list_obj_names=org_list,\n",
    "                                                                                                                      list_obj_segs= organelles,\n",
    "                                                                                                                      list_intensity_img=intensities,\n",
    "                                                                                                                      list_region_names=region_names,\n",
    "                                                                                                                      list_region_segs=regions,\n",
    "                                                                                                                      mask='cell',\n",
    "                                                                                                                      dist_centering_obj='nuc',\n",
    "                                                                                                                      dist_num_bins=5,\n",
    "                                                                                                                      dist_center_on=True,\n",
    "                                                                                                                      dist_keep_center_as_bin=True,\n",
    "                                                                                                                      dist_zernike_degrees=9,\n",
    "                                                                                                                      scale=scale,\n",
    "                                                                                                                      include_contact_dist=True,\n",
    "                                                                                                                      skel = ['ER'])\n",
    "\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3e6d4b",
   "metadata": {},
   "source": [
    "### **Update to `_find_segmentation_tiff_files`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb93cc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_segmentation_tiff_files(prototype:Union[Path,str],\n",
    "                                  name_list:List[str], \n",
    "                                  seg_path:Union[Path,str],\n",
    "                                  suffix:Union[str, None]=None):\n",
    "    \"\"\"\n",
    "    Find the matching segmentation files to the raw image file based on the raw image file path.\n",
    "\n",
    "    Paramters:\n",
    "    ---------\n",
    "    prototype:Union[Path,str]\n",
    "        the file path (as a string) for one raw image file; this file should have matching segmentation \n",
    "        output files with the same file name root and different file name ending that match the strings \n",
    "        provided in name_list\n",
    "    name_list:List[str]\n",
    "        a list of file name endings related to what segmentation is that file\n",
    "    seg_path:Union[Path,str]\n",
    "        the path (as a string) to the matching segmentation files.\n",
    "    suffix:Union[str, None]=None\n",
    "        any additional text that exists between the file root and the name_list ending\n",
    "        Ex) Prototype = \"C:/Users/Shannon/Documents/Python_Scripts/Infer-subc/raw/a48hrs-Ctrl_9_Unmixing.czi\"\n",
    "            Name of organelle file = a48hrs-Ctrl_9_Unmixing-20230426_test_cell.tiff\n",
    "            result of .stem = \"a48hrs-Ctrl_9_Unmixing\"\n",
    "            organelle/cell area type = \"cell\"\n",
    "            suffix = \"-20230426_test_\"\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    a dictionary of file paths for each image type (raw and all the different segmentations)\n",
    "\n",
    "    \"\"\"\n",
    "    # raw\n",
    "    prototype = Path(prototype)\n",
    "    if not prototype.exists():\n",
    "        print(f\"bad prototype. please choose an existing `raw` file as prototype\")\n",
    "        return dict()\n",
    "\n",
    "    out_files = {\"raw\":prototype}\n",
    "    seg_path = Path(seg_path) \n",
    "\n",
    "    # raw\n",
    "    if not seg_path.is_dir():\n",
    "        print(f\"bad path argument. please choose an existing path containing organelle segmentations\")\n",
    "        return out_files\n",
    "\n",
    "    # segmentations\n",
    "    for org_n in name_list:\n",
    "        org_name = Path(seg_path) / f\"{prototype.stem}{suffix}{org_n}.tiff\"\n",
    "        if org_name.exists():\n",
    "            out_files[org_n] = org_name\n",
    "        else:\n",
    "            # checker for .tif files usually as a result of manual segmentations\n",
    "            if (Path(seg_path) / f\"{prototype.stem}{suffix}{org_n}.tif\").exists():\n",
    "                out_files[org_n] = Path(seg_path) / f\"{prototype.stem}{suffix}{org_n}.tif\"\n",
    "                print(f\"{org_n} had a .tif file instead\")\n",
    "            else:\n",
    "                print(f\"{org_n} .tiff file not found in {seg_path} returning\")\n",
    "                out_files[org_n] = None\n",
    "    \n",
    "    return out_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65edb29d",
   "metadata": {},
   "source": [
    "### **Update to `batch_process_quantification`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20465572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for convex hull errors\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.simplefilter(\"ignore\", RuntimeWarning)\n",
    "\n",
    "def _batch_process_quantification(out_file_name: str,\n",
    "                                  seg_path: Union[Path,str],\n",
    "                                  out_path: Union[Path, str], \n",
    "                                  raw_path: Union[Path,str], \n",
    "                                  raw_file_type: str,\n",
    "                                  organelle_names: List[str],\n",
    "                                  organelle_channels: List[int],\n",
    "                                  region_names: List[str],\n",
    "                                  masks_file_name: str,\n",
    "                                  mask: str,\n",
    "                                  dist_centering_obj:str, \n",
    "                                  dist_num_bins: int,\n",
    "                                  dist_center_on: bool=False,\n",
    "                                  dist_keep_center_as_bin: bool=True,\n",
    "                                  dist_zernike_degrees: Union[int, None]=None,\n",
    "                                  include_contact_dist: bool = True,\n",
    "                                  scale:bool=True,\n",
    "                                  seg_suffix:Union[str, None]=None,\n",
    "                                  splitter: str = '_',\n",
    "                                  skel: List[str] = []) -> int :\n",
    "    \"\"\"  \n",
    "    batch process segmentation quantification (morphology, distribution, contacts); this function is currently optimized to process images from one file folder per image type (e.g., raw, segmentation)\n",
    "    the output csv files are saved to the indicated out_path folder\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    out_file_name: str\n",
    "        the prefix to use when naming the output datatables\n",
    "    seg_path: Union[Path,str]\n",
    "        Path or str to the folder that contains the segmentation tiff files\n",
    "    out_path: Union[Path, str]\n",
    "        Path or str to the folder that the output datatables will be saved to\n",
    "    raw_path: Union[Path,str]\n",
    "        Path or str to the folder that contains the raw image files\n",
    "    raw_file_type: str\n",
    "        the file type of the raw data; ex - \".tiff\", \".czi\"\n",
    "    organelle_names: List[str]\n",
    "        a list of all organelle names that will be analyzed; the names should be the same as the suffix used to name each of the tiff segmentation files\n",
    "        Note: the intensity measurements collect per region (from get_region_morphology_3D function) will only be from channels associated to these organelles \n",
    "    organelle_channels: List[int]\n",
    "        a list of channel indices associated to respective organelle staining in the raw image; the indices should listed in same order in which the respective segmentation name is listed in organelle_names\n",
    "    region_names: List[str]\n",
    "        a list of regions, or masks, to measure; the order should correlate to the order of the channels in the \"masks\" output segmentation file\n",
    "    masks_file_name: str\n",
    "        the suffix of the \"masks\" segmentation file; ex- \"masks_B\", \"masks\", etc.\n",
    "        this function currently does not accept indivial region segmentations \n",
    "    mask: str\n",
    "        the name of the region to use as the mask when measuring the organelles; this should be one of the names listed in regions list; usually this will be the \"cell\" mask\n",
    "    dist_centering_obj:str\n",
    "        the name of the region or object to use as the centering object in the get_XY_distribution function\n",
    "    dist_num_bins: int\n",
    "        the number of bins for the get_XY_distribution function\n",
    "    dist_center_on: bool=False,\n",
    "        for get_XY_distribution:\n",
    "        True = distribute the bins from the center of the centering object\n",
    "        False = distribute the bins from the edge of the centering object\n",
    "    dist_keep_center_as_bin: bool=True\n",
    "        for get_XY_distribution:\n",
    "        True = include the centering object area when creating the bins\n",
    "        False = do not include the centering object area when creating the bins\n",
    "    dist_zernike_degrees: Union[int, None]=None\n",
    "        for get_XY_distribution:\n",
    "        the number of zernike degrees to include for the zernike shape descriptors; if None, the zernike measurements will not \n",
    "        be included in the output\n",
    "    include_contact_dist:bool=True\n",
    "        whether to include the distribution of contact sites in get_contact_metrics_3d(); True = include contact distribution\n",
    "    scale:bool=True\n",
    "        a tuple that contains the real world dimensions for each dimension in the image (Z, Y, X)\n",
    "    seg_suffix:Union[str, None]=None\n",
    "        any additional text that is included in the segmentation tiff files between the file stem and the segmentation suffix\n",
    "        TODO: this can't be None!!! need to update!!!\n",
    "    skel: List[str] = []\n",
    "        The organelles in which skeleton quantification will be ran on\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    count: int\n",
    "        the number of images processed\n",
    "        \n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    count = 0\n",
    "\n",
    "    if isinstance(raw_path, str): raw_path = Path(raw_path)\n",
    "    if isinstance(seg_path, str): seg_path = Path(seg_path)\n",
    "    if isinstance(out_path, str): out_path = Path(out_path)\n",
    "    \n",
    "    if not Path.exists(out_path):\n",
    "        Path.mkdir(out_path)\n",
    "        print(f\"making {out_path}\")\n",
    "    \n",
    "    # reading list of files from the raw path\n",
    "    img_file_list = list_image_files(raw_path, raw_file_type)\n",
    "\n",
    "    # list of segmentation files to collect\n",
    "    segs_to_collect = organelle_names + [masks_file_name]\n",
    "\n",
    "    # containers to collect data tabels\n",
    "    org_tabs = []\n",
    "    contact_tabs = []\n",
    "    dist_tabs = []\n",
    "    region_tabs = []\n",
    "\n",
    "    for img_f in img_file_list:\n",
    "        count = count + 1\n",
    "        filez = _find_segmentation_tiff_files(img_f, segs_to_collect, seg_path, seg_suffix)\n",
    "\n",
    "        # read in raw file and metadata\n",
    "        img_data, meta_dict = read_czi_image(filez[\"raw\"])\n",
    "\n",
    "        # create intensities from raw file as list based on the channel order provided\n",
    "        intensities = [img_data[ch] for ch in organelle_channels]\n",
    "\n",
    "        # define the scale\n",
    "        if scale is True:\n",
    "            scale_tup = meta_dict['scale']\n",
    "        else:\n",
    "            scale_tup = None\n",
    "\n",
    "        # load regions as a list based on order in list (should match order in \"masks\" file)\n",
    "        # masks = read_tiff_image(filez[masks_file_name]) \n",
    "        # regions = [masks[r] for r, region in enumerate(region_names)]\n",
    "        \n",
    "        regions= [read_tiff_image(filez[masks_file_name])[0],\n",
    "                  read_tiff_image(filez[masks_file_name])[1]]\n",
    "\n",
    "        # store organelle images as list\n",
    "        organelles = [read_tiff_image(filez[org]) for org in organelle_names]\n",
    "\n",
    "        org_metrics, contact_metrics, dist_metrics, region_metrics = _make_all_metrics_tables(source_file=img_f,\n",
    "                                                                                             list_obj_names=organelle_names,\n",
    "                                                                                             list_obj_segs=organelles,\n",
    "                                                                                             list_intensity_img=intensities, \n",
    "                                                                                             list_region_names=region_names,\n",
    "                                                                                             list_region_segs=regions, \n",
    "                                                                                             mask=mask,\n",
    "                                                                                             dist_centering_obj=dist_centering_obj,\n",
    "                                                                                             dist_num_bins=dist_num_bins,\n",
    "                                                                                             dist_center_on=dist_center_on,\n",
    "                                                                                             dist_keep_center_as_bin=dist_keep_center_as_bin,\n",
    "                                                                                             dist_zernike_degrees=dist_zernike_degrees,\n",
    "                                                                                             scale=scale_tup,\n",
    "                                                                                             include_contact_dist=include_contact_dist,\n",
    "                                                                                             splitter=splitter,\n",
    "                                                                                             skel = skel)\n",
    "\n",
    "        org_tabs.append(org_metrics)\n",
    "        contact_tabs.append(contact_metrics)\n",
    "        dist_tabs.append(dist_metrics)\n",
    "        region_tabs.append(region_metrics)\n",
    "\n",
    "        end2 = time.time()\n",
    "        print(f\"Completed processing for {count} images in {(end2-start)/60} mins.\")\n",
    "\n",
    "    final_org = pd.concat(org_tabs, ignore_index=True)\n",
    "    final_contact = pd.concat(contact_tabs, ignore_index=True)\n",
    "    final_dist = pd.concat(dist_tabs, ignore_index=True)\n",
    "    final_region = pd.concat(region_tabs, ignore_index=True)\n",
    "\n",
    "    org_csv_path = out_path / f\"{out_file_name}organelles.csv\"\n",
    "    final_org.to_csv(org_csv_path)\n",
    "\n",
    "    contact_csv_path = out_path / f\"{out_file_name}contacts.csv\"\n",
    "    final_contact.to_csv(contact_csv_path)\n",
    "\n",
    "    dist_csv_path = out_path / f\"{out_file_name}distributions.csv\"\n",
    "    final_dist.to_csv(dist_csv_path)\n",
    "\n",
    "    region_csv_path = out_path / f\"{out_file_name}regions.csv\"\n",
    "    final_region.to_csv(region_csv_path)\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"Quantification for {count} files is COMPLETE! Files saved to '{out_path}'.\")\n",
    "    print(f\"It took {(end - start)/60} minutes to quantify these files.\")\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bfb7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_data_path = Path(os.path.expanduser(\"~\")) / \"OneDrive/Desktop/seg\"\n",
    "out_data_path = Path(os.path.expanduser(\"~\")) / \"OneDrive/Desktop/out\"\n",
    "raw_data_path = Path(os.path.expanduser(\"~\")) / \"OneDrive/Desktop/raw\"\n",
    "\n",
    "n_files = _batch_process_quantification(out_file_name = \"skel_test_\",\n",
    "                                  seg_path=seg_data_path,\n",
    "                                  out_path=out_data_path, \n",
    "                                  raw_path=raw_data_path, \n",
    "                                  raw_file_type=\".tiff\",\n",
    "                                  organelle_names=org_list,\n",
    "                                  organelle_channels=organelle_channels,\n",
    "                                  region_names=region_names,\n",
    "                                  masks_file_name=\"masks_B\",\n",
    "                                  mask=\"cell\",\n",
    "                                  dist_centering_obj=\"nuc\", \n",
    "                                  dist_num_bins=5,\n",
    "                                  dist_center_on=False,\n",
    "                                  dist_keep_center_as_bin=True,\n",
    "                                  dist_zernike_degrees=9,\n",
    "                                  include_contact_dist=True,\n",
    "                                  scale=True,\n",
    "                                  seg_suffix=\"-\",\n",
    "                                  splitter='_',\n",
    "                                  skel = ['ER','mito','golgi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9213eaa3",
   "metadata": {},
   "source": [
    "### **`batch_summary_stats`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee2c5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_existing_combo(contact, contact_list, splitter):\n",
    "    for ctc in contact_list:\n",
    "        if sorted(contact) == sorted(ctc.split(splitter)):\n",
    "            return(ctc.split(splitter))\n",
    "    return contact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f076f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _batch_summary_stats(csv_path_list: List[str],\n",
    "                         out_path: str,\n",
    "                         out_preffix: str,\n",
    "                         splitter: str='X'):\n",
    "    \"\"\"\" \n",
    "    csv_path_list: List[str],\n",
    "        A list of path strings where .csv files to analyze are located.\n",
    "    out_path: str,\n",
    "        A path string where the summary data file will be output to\n",
    "    out_preffix: str\n",
    "        The prefix used to name the output file.    \n",
    "    \"\"\"\n",
    "    ds_count = 0\n",
    "    fl_count = 0\n",
    "    ###################\n",
    "    # Read in the csv files and combine them into one of each type\n",
    "    ###################\n",
    "    org_tabs = []\n",
    "    contact_tabs = []\n",
    "    dist_tabs = []\n",
    "    region_tabs = []\n",
    "\n",
    "    for loc in csv_path_list:\n",
    "        print(loc)\n",
    "        ds_count = ds_count + 1\n",
    "        loc=Path(loc)\n",
    "        files_store = sorted(loc.glob(\"*.csv\"))\n",
    "        for file in files_store:\n",
    "            fl_count = fl_count + 1\n",
    "            stem = file.stem\n",
    "\n",
    "            org = \"organelles\"\n",
    "            contacts = \"contacts\"\n",
    "            dist = \"distributions\"\n",
    "            regions = \"regions\"\n",
    "\n",
    "            if org in stem:\n",
    "                test_orgs = pd.read_csv(file, index_col=0)\n",
    "                test_orgs.insert(0, \"dataset\", stem[:-11])\n",
    "                org_tabs.append(test_orgs)\n",
    "            if contacts in stem:\n",
    "                test_contact = pd.read_csv(file, index_col=0)\n",
    "                test_contact.insert(0, \"dataset\", stem[:-9])\n",
    "                contact_tabs.append(test_contact)\n",
    "            if dist in stem:\n",
    "                test_dist = pd.read_csv(file, index_col=0)\n",
    "                test_dist.insert(0, \"dataset\", stem[:-14])\n",
    "                dist_tabs.append(test_dist)\n",
    "            if regions in stem:\n",
    "                test_regions = pd.read_csv(file, index_col=0)\n",
    "                test_regions.insert(0, \"dataset\", stem[:-8])\n",
    "                region_tabs.append(test_regions)\n",
    "            \n",
    "    org_df = pd.concat(org_tabs,axis=0, join='outer')\n",
    "    contacts_df = pd.concat(contact_tabs,axis=0, join='outer')\n",
    "    dist_df = pd.concat(dist_tabs,axis=0, join='outer')\n",
    "    regions_df = pd.concat(region_tabs,axis=0, join='outer')\n",
    "    ##########################\n",
    "    # List organelles in cell\n",
    "    ###########################\n",
    "    all_orgs = list(set(org_df.loc[:, 'object'].tolist()))\n",
    "\n",
    "    ###################\n",
    "    # adding new metrics to the original sheets\n",
    "    ###################\n",
    "    # TODO: include these labels when creating the original sheets\n",
    "    contact_cnt = contacts_df[[\"dataset\", \"image_name\", \"object\", \"label\", \"volume\"]]\n",
    "    ctc = contact_cnt[\"object\"].values.tolist()\n",
    "    ##############################################################################\n",
    "    #  Creating New methods of storing A & B\n",
    "    ###############################################################################\n",
    "    # len(max(contact_cnt[\"object\"].str.split('X'), key=len))) provides max number of organelles involved in contact\n",
    "    contact_cnt[[f\"org{cha}\" for cha in string.ascii_uppercase[:(len(max(contact_cnt[\"object\"].str.split(splitter), key=len)))]]] = contact_cnt[\"object\"].str.split(splitter, expand=True)\n",
    "    contact_cnt[[f\"{cha}_ID\" for cha in string.ascii_uppercase[:(len(max(contact_cnt[\"label\"].str.split('_'), key=len)))]]] = contact_cnt[\"label\"].str.split('_', expand=True)\n",
    "    #iterating from a to val\n",
    "    unstacked_cont = []\n",
    "    for cha in string.ascii_uppercase[:len(max(contact_cnt[\"object\"].str.split(splitter), key=len))]:\n",
    "        valid = (contact_cnt[f\"org{cha}\"] != None) & (contact_cnt[f\"{cha}_ID\"] != None)\n",
    "        contact_cnt[f\"{cha}\"] = None\n",
    "        contact_cnt.loc[valid, f\"{cha}\"] = contact_cnt[f\"org{cha}\"] + \"_\" + contact_cnt[f\"{cha}_ID\"]\n",
    "        contact_cnt_percell = contact_cnt[[\"dataset\", \"image_name\", f\"org{cha}\", f\"{cha}_ID\", \"object\", \"volume\"]].groupby([\"dataset\", \"image_name\", f\"org{cha}\", f\"{cha}_ID\", \"object\"]).agg([\"count\", \"sum\"])\n",
    "        contact_cnt_percell.columns = [\"_\".join(col_name).rstrip('_') for col_name in contact_cnt_percell.columns.to_flat_index()]\n",
    "        unstacked = contact_cnt_percell.unstack(level='object')\n",
    "        unstacked.columns = [\"_\".join(col_name).rstrip('_') for col_name in unstacked.columns.to_flat_index()]\n",
    "        unstacked = unstacked.reset_index()\n",
    "        for col in unstacked.columns:\n",
    "            if col.startswith(\"volume_count_\"):\n",
    "                newname = col.split(\"_\")[-1] + \"_count\"\n",
    "                unstacked.rename(columns={col:newname}, inplace=True)\n",
    "            if col.startswith(\"volume_sum_\"):\n",
    "                newname = col.split(\"_\")[-1] + \"_volume\"\n",
    "                unstacked.rename(columns={col:newname}, inplace=True)\n",
    "        unstacked.rename(columns={f\"org{cha}\":\"object\", f\"{cha}_ID\":\"label\"}, inplace=True)\n",
    "        unstacked.set_index(['dataset', 'image_name', 'object', 'label'])    \n",
    "        unstacked_cont.append(unstacked)\n",
    "    contact_cnt = pd.concat(unstacked_cont, axis=0).sort_index(axis=0)\n",
    "    contact_cnt = contact_cnt.groupby(['dataset', 'image_name', 'object', 'label']).sum().reset_index()                 #adds together all duplicates at the index, then resets the index\n",
    "    contact_cnt['label']=contact_cnt['label'].astype(\"Int64\")  \n",
    "    org_df = pd.merge(org_df, contact_cnt, how='left', on=['dataset', 'image_name', 'object', 'label'], sort=True)\n",
    "    org_df[contact_cnt.columns] = org_df[contact_cnt.columns].fillna(0)\n",
    "\n",
    "    ###################\n",
    "    # summary stat group\n",
    "    ###################\n",
    "    group_by = ['dataset', 'image_name', 'object']\n",
    "    sharedcolumns = [\"SA_to_volume_ratio\", \"equivalent_diameter\", \"extent\", \"euler_number\", \"solidity\", \"axis_major_length\"]\n",
    "    ag_func_standard = ['mean', 'median', 'std']\n",
    "\n",
    "    ###################\n",
    "    # summarize shared measurements between org_df and contacts_df\n",
    "    ###################\n",
    "    org_cont_tabs = []\n",
    "    for tab in [org_df, contacts_df]:\n",
    "        tab1 = tab[group_by + ['volume']].groupby(group_by).agg(['count', 'sum'] + ag_func_standard)\n",
    "        tab2 = tab[group_by + ['surface_area']].groupby(group_by).agg(['sum'] + ag_func_standard)\n",
    "        tab3 = tab[group_by + sharedcolumns].groupby(group_by).agg(ag_func_standard)\n",
    "        shared_metrics = pd.merge(tab1, tab2, 'outer', on=group_by)\n",
    "        shared_metrics = pd.merge(shared_metrics, tab3, 'outer', on=group_by)\n",
    "        org_cont_tabs.append(shared_metrics)\n",
    "\n",
    "    org_summary = org_cont_tabs[0]\n",
    "    contact_summary = org_cont_tabs[1]\n",
    "\n",
    "    ###################\n",
    "    # group metrics from regions_df similar to the above\n",
    "    ###################\n",
    "    regions_summary = regions_df[group_by + ['volume', 'surface_area'] + sharedcolumns].set_index(group_by)\n",
    "\n",
    "    ###################\n",
    "    # summarize extra metrics from org_df\n",
    "    ###################\n",
    "    columns2 = [col for col in org_df.columns if col.endswith((\"_count\", \"_volume\"))]\n",
    "    contact_counts_summary = org_df[group_by + columns2].groupby(group_by).agg(['sum'] + ag_func_standard)\n",
    "    org_summary = pd.merge(org_summary, contact_counts_summary, 'outer', on=group_by)#left_on=group_by, right_on=True)\n",
    "\n",
    "    ###################\n",
    "    # summarize distribution measurements\n",
    "    ###################\n",
    "    # organelle distributions\n",
    "    hist_dfs = []\n",
    "    for ind in dist_df.index:\n",
    "        selection = dist_df.loc[[ind]]\n",
    "        bins_df = pd.DataFrame()\n",
    "        wedges_df = pd.DataFrame()\n",
    "        Z_df = pd.DataFrame()\n",
    "\n",
    "        bins_df[['bins', 'masks', 'obj']] = selection[['XY_bins', 'XY_mask_vox_cnt_perbin', 'XY_obj_vox_cnt_perbin']]\n",
    "        wedges_df[['bins', 'masks', 'obj']] = selection[['XY_wedges', 'XY_mask_vox_cnt_perwedge', 'XY_obj_vox_cnt_perwedge']]\n",
    "        Z_df[['bins', 'masks', 'obj']] = selection[['Z_slices', 'Z_mask_vox_cnt', 'Z_obj_vox_cnt']]\n",
    "\n",
    "        dfs = [selection[['dataset', 'image_name', 'object']].reset_index()]\n",
    "        for df, prefix in zip([bins_df, wedges_df, Z_df], [\"XY_bins_\", \"XY_wedges_\", \"Z_slices_\"]):\n",
    "            single_df = pd.DataFrame(list(zip(df[\"bins\"].values[0][1:-1].split(\", \"), \n",
    "                                            df[\"obj\"].values[0][1:-1].split(\", \"), \n",
    "                                            df[\"masks\"].values[0][1:-1].split(\", \"))), columns =['bins', 'obj', 'mask']).astype(int)\n",
    "\n",
    "            if \"Z_\" in prefix:\n",
    "                single_df =  single_df.drop(single_df[single_df['mask'] == 0].index)\n",
    "                single_df['bins'] = (single_df[\"bins\"]/max(single_df.bins)*10).apply(np.floor)\n",
    "        \n",
    "            single_df['mask_fract'] = single_df['mask']/single_df['mask'].max()\n",
    "            single_df['obj_norm'] = (single_df[\"obj\"]/single_df[\"mask_fract\"]).fillna(0)\n",
    "            single_df['portion_per_bin'] = (single_df[\"obj\"] / single_df[\"obj\"].sum())*100\n",
    "\n",
    "            # single_df['mask_fract'] = single_df['mask']/single_df['mask'].max()\n",
    "            # single_df['obj_norm'] = (single_df[\"obj\"]/single_df[\"mask_fract\"]).fillna(0)\n",
    "            # single_df['portion_per_bin'] = (single_df[\"obj\"] / single_df[\"obj\"].sum())*100\n",
    "\n",
    "            # if \"Z_\" in prefix:\n",
    "            #     single_df['bins'] = (single_df[\"bins\"]/max(single_df.bins)*10).apply(np.floor)\n",
    "\n",
    "\n",
    "            sumstats_df = pd.DataFrame()\n",
    "\n",
    "            s = single_df['bins'].repeat(single_df['obj_norm'])\n",
    "            sumstats_df['hist_mean']=[s.mean()]\n",
    "            sumstats_df['hist_median']=[s.median()]\n",
    "            if single_df['obj_norm'].sum() != 0: sumstats_df['hist_mode']=[s.mode()[0]]\n",
    "            else: sumstats_df['hist_mode']=['NaN']\n",
    "            sumstats_df['hist_min']=[s.min()]\n",
    "            sumstats_df['hist_max']=[s.max()]\n",
    "            sumstats_df['hist_range']=[s.max() - s.min()]\n",
    "            sumstats_df['hist_stdev']=[s.std()]\n",
    "            sumstats_df['hist_skew']=[s.skew()]\n",
    "            sumstats_df['hist_kurtosis']=[s.kurtosis()]\n",
    "            sumstats_df['hist_var']=[s.var()]\n",
    "            sumstats_df.columns = [prefix+col for col in sumstats_df.columns]\n",
    "            dfs.append(sumstats_df.reset_index())\n",
    "        combined_df = pd.concat(dfs, axis=1).drop(columns=\"index\")\n",
    "        hist_dfs.append(combined_df)\n",
    "    dist_org_summary = pd.concat(hist_dfs, ignore_index=True)\n",
    "\n",
    "    # nucleus distribution\n",
    "    nuc_dist_df = dist_df[[\"dataset\", \"image_name\", \n",
    "                        \"XY_bins\", \"XY_center_vox_cnt_perbin\", \"XY_mask_vox_cnt_perbin\",\n",
    "                        \"XY_wedges\", \"XY_center_vox_cnt_perwedge\", \"XY_mask_vox_cnt_perwedge\",\n",
    "                        \"Z_slices\", \"Z_center_vox_cnt\", \"Z_mask_vox_cnt\"]].set_index([\"dataset\", \"image_name\"])\n",
    "    nuc_hist_dfs = []\n",
    "    for idx in nuc_dist_df.index.unique():\n",
    "        selection = nuc_dist_df.loc[idx].iloc[[0]].reset_index()\n",
    "        bins_df = pd.DataFrame()\n",
    "        wedges_df = pd.DataFrame()\n",
    "        Z_df = pd.DataFrame()\n",
    "\n",
    "        bins_df[['bins', 'center', 'masks']] = selection[['XY_bins', 'XY_center_vox_cnt_perbin', 'XY_mask_vox_cnt_perbin']]\n",
    "        wedges_df[['bins', 'center', 'masks']] = selection[['XY_wedges', 'XY_center_vox_cnt_perwedge', 'XY_mask_vox_cnt_perwedge']]\n",
    "        Z_df[['bins', 'center', 'masks']] = selection[['Z_slices', 'Z_center_vox_cnt', 'Z_mask_vox_cnt']]\n",
    "\n",
    "        dfs = [selection[['dataset', 'image_name']]]\n",
    "        for df, prefix in zip([bins_df, wedges_df, Z_df], [\"XY_bins_\", \"XY_wedges_\", \"Z_slices_\"]):\n",
    "            single_df = pd.DataFrame(list(zip(df[\"bins\"].values[0][1:-1].split(\", \"), \n",
    "                                            df[\"masks\"].values[0][1:-1].split(\", \"),\n",
    "                                            df[\"center\"].values[0][1:-1].split(\", \"))), columns =['bins', 'mask', 'obj']).astype(int)\n",
    "            # single_df['mask_fract'] = single_df['mask']/single_df['mask'].max()\n",
    "            # single_df['obj_norm'] = (single_df[\"obj\"]/single_df[\"mask_fract\"]).fillna(0)\n",
    "            # single_df['portion_per_bin'] = (single_df[\"obj\"] / single_df[\"obj\"].sum())*100\n",
    "            # if \"Z_\" in prefix:\n",
    "            #     single_df['bins'] = (single_df[\"bins\"]/max(single_df.bins)*10).apply(np.floor)\n",
    "\n",
    "            if \"Z_\" in prefix:\n",
    "                single_df =  single_df.drop(single_df[single_df['mask'] == 0].index)\n",
    "                single_df['bins'] = (single_df[\"bins\"]/max(single_df.bins)*10).apply(np.floor)\n",
    "        \n",
    "            single_df['mask_fract'] = single_df['mask']/single_df['mask'].max()\n",
    "            single_df['obj_norm'] = (single_df[\"obj\"]/single_df[\"mask_fract\"]).fillna(0)\n",
    "            single_df['portion_per_bin'] = (single_df[\"obj\"] / single_df[\"obj\"].sum())*100\n",
    "\n",
    "            sumstats_df = pd.DataFrame()\n",
    "\n",
    "            s = single_df['bins'].repeat(single_df['obj_norm'])\n",
    "            sumstats_df['hist_mean']=[s.mean()]\n",
    "            sumstats_df['hist_median']=[s.median()]\n",
    "            if single_df['obj_norm'].sum() != 0: sumstats_df['hist_mode']=[s.mode()[0]]\n",
    "            else: sumstats_df['hist_mode']=['NaN']\n",
    "            sumstats_df['hist_min']=[s.min()]\n",
    "            sumstats_df['hist_max']=[s.max()]\n",
    "            sumstats_df['hist_range']=[s.max() - s.min()]\n",
    "            sumstats_df['hist_stdev']=[s.std()]\n",
    "            sumstats_df['hist_skew']=[s.skew()]\n",
    "            sumstats_df['hist_kurtosis']=[s.kurtosis()]\n",
    "            sumstats_df['hist_var']=[s.var()]\n",
    "            sumstats_df.columns = [prefix+col for col in sumstats_df.columns]\n",
    "            dfs.append(sumstats_df.reset_index())\n",
    "        combined_df = pd.concat(dfs, axis=1).drop(columns=\"index\")\n",
    "        nuc_hist_dfs.append(combined_df)\n",
    "    dist_center_summary = pd.concat(nuc_hist_dfs, ignore_index=True)\n",
    "    dist_center_summary.insert(2, column=\"object\", value=\"nuc\")\n",
    "\n",
    "    dist_summary = pd.concat([dist_org_summary, dist_center_summary], axis=0).set_index(group_by).sort_index()\n",
    "\n",
    "    ###################\n",
    "    # add normalization\n",
    "    ###################\n",
    "    # organelle area fraction\n",
    "    area_fractions = []\n",
    "    for idx in org_summary.index.unique():\n",
    "        org_vol = org_summary.loc[idx][('volume', 'sum')]\n",
    "        cell_vol = regions_summary.loc[idx[:-1] + ('cell',)][\"volume\"]\n",
    "        afrac = org_vol/cell_vol\n",
    "        area_fractions.append(afrac)\n",
    "    org_summary[('volume', 'fraction')] = area_fractions\n",
    "    # TODO: add in line to reorder the level=0 columns here\n",
    "\n",
    "    # contact sites volume normalized\n",
    "    # norm_toA_list = []\n",
    "    # norm_toB_list = []\n",
    "    norm_to_list = {}\n",
    "    for col in contact_summary.index:\n",
    "        for idx, cha in enumerate(string.ascii_uppercase[:len(max(contact_summary.index.get_level_values('object').str.split(splitter), key=len))]):\n",
    "            if cha not in norm_to_list:\n",
    "                norm_to_list[f\"{cha}\"] = []\n",
    "            if ((idx+1) <= len(col[-1].split(splitter))):\n",
    "                norm_to_list[f\"{cha}\"].append(contact_summary.loc[col][('volume', 'sum')]/org_summary.loc[col[:-1]+(col[-1].split(splitter)[idx],)][('volume', 'sum')])\n",
    "            else:\n",
    "                norm_to_list[f\"{cha}\"].append(None)\n",
    "    for cha in string.ascii_uppercase[:len(max(contact_summary.index.get_level_values('object').str.split(splitter), key=len))]:\n",
    "        contact_summary[('volume', f'norm_to_{cha}')] = norm_to_list[f\"{cha}\"]\n",
    "        # norm_toA_list.append(contact_summary.loc[col][('volume', 'sum')]/org_summary.loc[col[:-1]+(col[-1].split(splitter)[0],)][('volume', 'sum')])\n",
    "        # norm_toB_list.append(contact_summary.loc[col][('volume', 'sum')]/org_summary.loc[col[:-1]+(col[-1].split(splitter)[1],)][('volume', 'sum')])\n",
    "    # contact_summary[('volume', 'norm_to_A')] = norm_toA_list\n",
    "    # contact_summary[('volume', 'norm_to_B')] = norm_toB_list\n",
    "\n",
    "    # number and area of individuals organelle involved in contact\n",
    "    cont_cnt = org_df[group_by]\n",
    "    cont_cnt[[col.split('_')[0] for col in org_df.columns if col.endswith((\"_count\"))]] = org_df[[col for col in org_df.columns if col.endswith((\"_count\"))]].astype(bool)\n",
    "    cont_cnt_perorg = cont_cnt.groupby(group_by).agg('sum')\n",
    "    cont_cnt_perorg.columns = pd.MultiIndex.from_product([cont_cnt_perorg.columns, ['count_in']])\n",
    "    for col in cont_cnt_perorg.columns:\n",
    "        cont_cnt_perorg[(col[0], 'num_fraction_in')] = cont_cnt_perorg[col].values/org_summary[('volume', 'count')].values\n",
    "    cont_cnt_perorg.sort_index(axis=1, inplace=True)\n",
    "    org_summary = pd.merge(org_summary, cont_cnt_perorg, on=group_by, how='outer')\n",
    "\n",
    "\n",
    "    ###################\n",
    "    # flatten datasheets and combine\n",
    "    # TODO: restructure this so that all of the datasheets and unstacked and then reorded based on shared level 0 columns before flattening\n",
    "    ###################\n",
    "    # org flattening\n",
    "    org_final = org_summary.unstack(-1)\n",
    "    for col in org_final.columns:\n",
    "        if col[1] in ('count_in', 'num_fraction_in') or col[0].endswith(('_count', '_volume')):\n",
    "            if col[2] not in col[0]:\n",
    "                org_final.drop(col,axis=1, inplace=True)\n",
    "    ########################################################################\n",
    "    # MAKING new_col_order flexible to work with any organelle input values and combo number\n",
    "    #######################################################################\n",
    "    new_col_order = ['dataset', 'image_name', 'object', 'volume', 'surface_area', 'SA_to_volume_ratio', \n",
    "                     'equivalent_diameter', 'extent', 'euler_number', 'solidity', 'axis_major_length'] \n",
    "    all_combos = []\n",
    "    for n in list(map(lambda x:x+2, (range(len(all_orgs)-1)))):\n",
    "            for o in itertools.combinations(all_orgs, n):\n",
    "                all_combos.append(check_for_existing_combo(o, ctc, splitter))\n",
    "    combos = [splitter.join(cont) for cont in all_combos]\n",
    "    for combo in combos:\n",
    "        new_col_order += [f\"{combo}\", f\"{combo}_count\", f\"{combo}_volume\"]\n",
    "    new_cols = org_final.columns.reindex(new_col_order, level=0)\n",
    "    org_final = org_final.reindex(columns=new_cols[0])\n",
    "    org_final.columns = [\"_\".join((col_name[-1], col_name[1], col_name[0])) for col_name in org_final.columns.to_flat_index()]\n",
    "\n",
    "    #renaming, filling \"NaN\" with 0 when needed, and removing ER_std columns\n",
    "    for col in org_final.columns:\n",
    "        if '_count_in_' or '_fraction_in_' in col:\n",
    "            org_final[col] = org_final[col].fillna(0)\n",
    "        if col.endswith((\"_count_volume\",\"_sum_volume\", \"_mean_volume\", \"_median_volume\")):\n",
    "            org_final[col] = org_final[col].fillna(0)\n",
    "        if col.endswith(\"_count_volume\"):\n",
    "            org_final.rename(columns={col:col.split(\"_\")[0]+\"_count\"}, inplace=True)\n",
    "        if col.startswith(\"ER_std_\"):\n",
    "            org_final.drop(columns=[col], inplace=True)\n",
    "    org_final = org_final.reset_index()\n",
    "\n",
    "    # contacts flattened\n",
    "    contact_final = contact_summary.unstack(-1)\n",
    "    contact_final.columns = [\"_\".join((col_name[-1], col_name[1], col_name[0])) for col_name in contact_final.columns.to_flat_index()]\n",
    "\n",
    "    #renaming and filling \"NaN\" with 0 when needed\n",
    "    for col in contact_final.columns:\n",
    "        if col.endswith((\"_count_volume\",\"_sum_volume\", \"_mean_volume\", \"_median_volume\")):\n",
    "            contact_final[col] = contact_final[col].fillna(0)\n",
    "        if col.endswith(\"_count_volume\"):\n",
    "            contact_final.rename(columns={col:col.split(\"_\")[0]+\"_count\"}, inplace=True)\n",
    "    contact_final = contact_final.reset_index()\n",
    "\n",
    "    # distributions flattened\n",
    "    dist_final = dist_summary.unstack(-1)\n",
    "    dist_final.columns = [\"_\".join((col_name[1], col_name[0])) for col_name in dist_final.columns.to_flat_index()]\n",
    "    dist_final = dist_final.reset_index()\n",
    "\n",
    "    # regions flattened & normalization added\n",
    "    regions_final = regions_summary.unstack(-1)\n",
    "    regions_final.columns = [\"_\".join((col_name[1], col_name[0])) for col_name in regions_final.columns.to_flat_index()]\n",
    "    regions_final['nuc_area_fraction'] = regions_final['nuc_volume'] / regions_final['cell_volume']\n",
    "    regions_final = regions_final.reset_index()\n",
    "\n",
    "    # combining them all\n",
    "    combined = pd.merge(org_final, contact_final, on=[\"dataset\", \"image_name\"], how=\"outer\")\n",
    "    combined = pd.merge(combined, dist_final, on=[\"dataset\", \"image_name\"], how=\"outer\")\n",
    "    combined = pd.merge(combined, regions_final, on=[\"dataset\", \"image_name\"], how=\"outer\").set_index([\"dataset\", \"image_name\"])\n",
    "    combined.columns = [col.replace('sum', 'total') for col in combined.columns]\n",
    "\n",
    "    ###################\n",
    "    # export summary sheets\n",
    "    ###################\n",
    "    org_summary.to_csv(out_path + f\"/{out_preffix}per_org_summarystats.csv\")\n",
    "    contact_summary.to_csv(out_path + f\"/{out_preffix}per_contact_summarystats.csv\")\n",
    "    dist_summary.to_csv(out_path + f\"/{out_preffix}distribution_summarystats.csv\")\n",
    "    regions_summary.to_csv(out_path + f\"/{out_preffix}per_region_summarystats.csv\")\n",
    "    combined.to_csv(out_path + f\"/{out_preffix}summarystats_combined.csv\")\n",
    "\n",
    "    print(f\"Processing of {fl_count} files from {ds_count} dataset(s) is complete.\")\n",
    "    return f\"{fl_count} files from {ds_count} dataset(s) were processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b72abc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "_batch_summary_stats([str(out_data_path)],\n",
    "                     str(out_data_path),\n",
    "                     f\"{datetime.today().strftime('%Y%m%d')}_skel_test\",\n",
    "                     splitter=\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6939d6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _batch_summary_stats_debug(csv_path_list: List[str],\n",
    "                         out_path: str,\n",
    "                         out_preffix: str,\n",
    "                         splitter: str='X'):\n",
    "    \"\"\"\" \n",
    "    csv_path_list: List[str],\n",
    "        A list of path strings where .csv files to analyze are located.\n",
    "    out_path: str,\n",
    "        A path string where the summary data file will be output to\n",
    "    out_preffix: str\n",
    "        The prefix used to name the output file.    \n",
    "    \"\"\"\n",
    "    ds_count = 0\n",
    "    fl_count = 0\n",
    "    ###################\n",
    "    # Read in the csv files and combine them into one of each type\n",
    "    ###################\n",
    "    org_tabs = []\n",
    "    contact_tabs = []\n",
    "    dist_tabs = []\n",
    "    region_tabs = []\n",
    "\n",
    "    for loc in csv_path_list:\n",
    "        print(loc)\n",
    "        ds_count = ds_count + 1\n",
    "        loc=Path(loc)\n",
    "        files_store = sorted(loc.glob(\"*.csv\"))\n",
    "        for file in files_store:\n",
    "            fl_count = fl_count + 1\n",
    "            stem = file.stem\n",
    "\n",
    "            org = \"organelles\"\n",
    "            contacts = \"contacts\"\n",
    "            dist = \"distributions\"\n",
    "            regions = \"regions\"\n",
    "\n",
    "            if org in stem:\n",
    "                test_orgs = pd.read_csv(file, index_col=0)\n",
    "                test_orgs.insert(0, \"dataset\", stem[:-11])\n",
    "                org_tabs.append(test_orgs)\n",
    "            if contacts in stem:\n",
    "                test_contact = pd.read_csv(file, index_col=0)\n",
    "                test_contact.insert(0, \"dataset\", stem[:-9])\n",
    "                contact_tabs.append(test_contact)\n",
    "            if dist in stem:\n",
    "                test_dist = pd.read_csv(file, index_col=0)\n",
    "                test_dist.insert(0, \"dataset\", stem[:-14])\n",
    "                dist_tabs.append(test_dist)\n",
    "            if regions in stem:\n",
    "                test_regions = pd.read_csv(file, index_col=0)\n",
    "                test_regions.insert(0, \"dataset\", stem[:-8])\n",
    "                region_tabs.append(test_regions)\n",
    "            \n",
    "    org_df = pd.concat(org_tabs,axis=0, join='outer')\n",
    "    contacts_df = pd.concat(contact_tabs,axis=0, join='outer')\n",
    "    dist_df = pd.concat(dist_tabs,axis=0, join='outer')\n",
    "    regions_df = pd.concat(region_tabs,axis=0, join='outer')\n",
    "    ##########################\n",
    "    # List organelles in cell\n",
    "    ###########################\n",
    "    all_orgs = list(set(org_df.loc[:, 'object'].tolist()))\n",
    "\n",
    "    ###################\n",
    "    # adding new metrics to the original sheets\n",
    "    ###################\n",
    "    # TODO: include these labels when creating the original sheets\n",
    "    contact_cnt = contacts_df[[\"dataset\", \"image_name\", \"object\", \"label\", \"volume\"]]\n",
    "    ctc = contact_cnt[\"object\"].values.tolist()\n",
    "    ##############################################################################\n",
    "    #  Creating New methods of storing A & B\n",
    "    ###############################################################################\n",
    "    # len(max(contact_cnt[\"object\"].str.split('X'), key=len))) provides max number of organelles involved in contact\n",
    "    contact_cnt[[f\"org{cha}\" for cha in string.ascii_uppercase[:(len(max(contact_cnt[\"object\"].str.split(splitter), key=len)))]]] = contact_cnt[\"object\"].str.split(splitter, expand=True)\n",
    "    contact_cnt[[f\"{cha}_ID\" for cha in string.ascii_uppercase[:(len(max(contact_cnt[\"label\"].str.split('_'), key=len)))]]] = contact_cnt[\"label\"].str.split('_', expand=True)\n",
    "    #iterating from a to val\n",
    "    unstacked_cont = []\n",
    "    for cha in string.ascii_uppercase[:len(max(contact_cnt[\"object\"].str.split(splitter), key=len))]:\n",
    "        valid = (contact_cnt[f\"org{cha}\"] != None) & (contact_cnt[f\"{cha}_ID\"] != None)\n",
    "        contact_cnt[f\"{cha}\"] = None\n",
    "        contact_cnt.loc[valid, f\"{cha}\"] = contact_cnt[f\"org{cha}\"] + \"_\" + contact_cnt[f\"{cha}_ID\"]\n",
    "        contact_cnt_percell = contact_cnt[[\"dataset\", \"image_name\", f\"org{cha}\", f\"{cha}_ID\", \"object\", \"volume\"]].groupby([\"dataset\", \"image_name\", f\"org{cha}\", f\"{cha}_ID\", \"object\"]).agg([\"count\", \"sum\"])\n",
    "        contact_cnt_percell.columns = [\"_\".join(col_name).rstrip('_') for col_name in contact_cnt_percell.columns.to_flat_index()]\n",
    "        unstacked = contact_cnt_percell.unstack(level='object')\n",
    "        unstacked.columns = [\"_\".join(col_name).rstrip('_') for col_name in unstacked.columns.to_flat_index()]\n",
    "        unstacked = unstacked.reset_index()\n",
    "        for col in unstacked.columns:\n",
    "            if col.startswith(\"volume_count_\"):\n",
    "                newname = col.split(\"_\")[-1] + \"_count\"\n",
    "                unstacked.rename(columns={col:newname}, inplace=True)\n",
    "            if col.startswith(\"volume_sum_\"):\n",
    "                newname = col.split(\"_\")[-1] + \"_volume\"\n",
    "                unstacked.rename(columns={col:newname}, inplace=True)\n",
    "        unstacked.rename(columns={f\"org{cha}\":\"object\", f\"{cha}_ID\":\"label\"}, inplace=True)\n",
    "        unstacked.set_index(['dataset', 'image_name', 'object', 'label'])    \n",
    "        unstacked_cont.append(unstacked)\n",
    "\n",
    "    return(unstacked_cont, string.ascii_uppercase[:len(max(contact_cnt[\"object\"].str.split(splitter), key=len))])\n",
    "    # contact_cnt = pd.concat(unstacked_cont, axis=0).sort_index(axis=0)\n",
    "    # contact_cnt = contact_cnt.groupby(['dataset', 'image_name', 'object', 'label']).sum().reset_index()                 #adds together all duplicates at the index, then resets the index\n",
    "    # contact_cnt['label']=contact_cnt['label'].astype(\"Int64\")  \n",
    "    # org_df = pd.merge(org_df, contact_cnt, how='left', on=['dataset', 'image_name', 'object', 'label'], sort=True)\n",
    "    # org_df[contact_cnt.columns] = org_df[contact_cnt.columns].fillna(0)\n",
    "\n",
    "    # ###################\n",
    "    # # summary stat group\n",
    "    # ###################\n",
    "    # group_by = ['dataset', 'image_name', 'object']\n",
    "    # sharedcolumns = [\"SA_to_volume_ratio\", \"equivalent_diameter\", \"extent\", \"euler_number\", \"solidity\", \"axis_major_length\"]\n",
    "    # ag_func_standard = ['mean', 'median', 'std']\n",
    "\n",
    "    # ###################\n",
    "    # # summarize shared measurements between org_df and contacts_df\n",
    "    # ###################\n",
    "    # org_cont_tabs = []\n",
    "    # for tab in [org_df, contacts_df]:\n",
    "    #     tab1 = tab[group_by + ['volume']].groupby(group_by).agg(['count', 'sum'] + ag_func_standard)\n",
    "    #     tab2 = tab[group_by + ['surface_area']].groupby(group_by).agg(['sum'] + ag_func_standard)\n",
    "    #     tab3 = tab[group_by + sharedcolumns].groupby(group_by).agg(ag_func_standard)\n",
    "    #     shared_metrics = pd.merge(tab1, tab2, 'outer', on=group_by)\n",
    "    #     shared_metrics = pd.merge(shared_metrics, tab3, 'outer', on=group_by)\n",
    "    #     org_cont_tabs.append(shared_metrics)\n",
    "\n",
    "    # org_summary = org_cont_tabs[0]\n",
    "    # contact_summary = org_cont_tabs[1]\n",
    "\n",
    "    # ###################\n",
    "    # # group metrics from regions_df similar to the above\n",
    "    # ###################\n",
    "    # regions_summary = regions_df[group_by + ['volume', 'surface_area'] + sharedcolumns].set_index(group_by)\n",
    "\n",
    "    # ###################\n",
    "    # # summarize extra metrics from org_df\n",
    "    # ###################\n",
    "    # columns2 = [col for col in org_df.columns if col.endswith((\"_count\", \"_volume\"))]\n",
    "    # contact_counts_summary = org_df[group_by + columns2].groupby(group_by).agg(['sum'] + ag_func_standard)\n",
    "    # org_summary = pd.merge(org_summary, contact_counts_summary, 'outer', on=group_by)#left_on=group_by, right_on=True)\n",
    "\n",
    "    # ###################\n",
    "    # # summarize distribution measurements\n",
    "    # ###################\n",
    "    # # organelle distributions\n",
    "    # hist_dfs = []\n",
    "    # for ind in dist_df.index:\n",
    "    #     selection = dist_df.loc[[ind]]\n",
    "    #     bins_df = pd.DataFrame()\n",
    "    #     wedges_df = pd.DataFrame()\n",
    "    #     Z_df = pd.DataFrame()\n",
    "\n",
    "    #     bins_df[['bins', 'masks', 'obj']] = selection[['XY_bins', 'XY_mask_vox_cnt_perbin', 'XY_obj_vox_cnt_perbin']]\n",
    "    #     wedges_df[['bins', 'masks', 'obj']] = selection[['XY_wedges', 'XY_mask_vox_cnt_perwedge', 'XY_obj_vox_cnt_perwedge']]\n",
    "    #     Z_df[['bins', 'masks', 'obj']] = selection[['Z_slices', 'Z_mask_vox_cnt', 'Z_obj_vox_cnt']]\n",
    "\n",
    "    #     dfs = [selection[['dataset', 'image_name', 'object']].reset_index()]\n",
    "    #     for df, prefix in zip([bins_df, wedges_df, Z_df], [\"XY_bins_\", \"XY_wedges_\", \"Z_slices_\"]):\n",
    "    #         single_df = pd.DataFrame(list(zip(df[\"bins\"].values[0][1:-1].split(\", \"), \n",
    "    #                                         df[\"obj\"].values[0][1:-1].split(\", \"), \n",
    "    #                                         df[\"masks\"].values[0][1:-1].split(\", \"))), columns =['bins', 'obj', 'mask']).astype(int)\n",
    "\n",
    "    #         if \"Z_\" in prefix:\n",
    "    #             single_df =  single_df.drop(single_df[single_df['mask'] == 0].index)\n",
    "    #             single_df['bins'] = (single_df[\"bins\"]/max(single_df.bins)*10).apply(np.floor)\n",
    "        \n",
    "    #         single_df['mask_fract'] = single_df['mask']/single_df['mask'].max()\n",
    "    #         single_df['obj_norm'] = (single_df[\"obj\"]/single_df[\"mask_fract\"]).fillna(0)\n",
    "    #         single_df['portion_per_bin'] = (single_df[\"obj\"] / single_df[\"obj\"].sum())*100\n",
    "\n",
    "    #         # single_df['mask_fract'] = single_df['mask']/single_df['mask'].max()\n",
    "    #         # single_df['obj_norm'] = (single_df[\"obj\"]/single_df[\"mask_fract\"]).fillna(0)\n",
    "    #         # single_df['portion_per_bin'] = (single_df[\"obj\"] / single_df[\"obj\"].sum())*100\n",
    "\n",
    "    #         # if \"Z_\" in prefix:\n",
    "    #         #     single_df['bins'] = (single_df[\"bins\"]/max(single_df.bins)*10).apply(np.floor)\n",
    "\n",
    "\n",
    "    #         sumstats_df = pd.DataFrame()\n",
    "\n",
    "    #         s = single_df['bins'].repeat(single_df['obj_norm'])\n",
    "    #         sumstats_df['hist_mean']=[s.mean()]\n",
    "    #         sumstats_df['hist_median']=[s.median()]\n",
    "    #         if single_df['obj_norm'].sum() != 0: sumstats_df['hist_mode']=[s.mode()[0]]\n",
    "    #         else: sumstats_df['hist_mode']=['NaN']\n",
    "    #         sumstats_df['hist_min']=[s.min()]\n",
    "    #         sumstats_df['hist_max']=[s.max()]\n",
    "    #         sumstats_df['hist_range']=[s.max() - s.min()]\n",
    "    #         sumstats_df['hist_stdev']=[s.std()]\n",
    "    #         sumstats_df['hist_skew']=[s.skew()]\n",
    "    #         sumstats_df['hist_kurtosis']=[s.kurtosis()]\n",
    "    #         sumstats_df['hist_var']=[s.var()]\n",
    "    #         sumstats_df.columns = [prefix+col for col in sumstats_df.columns]\n",
    "    #         dfs.append(sumstats_df.reset_index())\n",
    "    #     combined_df = pd.concat(dfs, axis=1).drop(columns=\"index\")\n",
    "    #     hist_dfs.append(combined_df)\n",
    "    # dist_org_summary = pd.concat(hist_dfs, ignore_index=True)\n",
    "\n",
    "    # # nucleus distribution\n",
    "    # nuc_dist_df = dist_df[[\"dataset\", \"image_name\", \n",
    "    #                     \"XY_bins\", \"XY_center_vox_cnt_perbin\", \"XY_mask_vox_cnt_perbin\",\n",
    "    #                     \"XY_wedges\", \"XY_center_vox_cnt_perwedge\", \"XY_mask_vox_cnt_perwedge\",\n",
    "    #                     \"Z_slices\", \"Z_center_vox_cnt\", \"Z_mask_vox_cnt\"]].set_index([\"dataset\", \"image_name\"])\n",
    "    # nuc_hist_dfs = []\n",
    "    # for idx in nuc_dist_df.index.unique():\n",
    "    #     selection = nuc_dist_df.loc[idx].iloc[[0]].reset_index()\n",
    "    #     bins_df = pd.DataFrame()\n",
    "    #     wedges_df = pd.DataFrame()\n",
    "    #     Z_df = pd.DataFrame()\n",
    "\n",
    "    #     bins_df[['bins', 'center', 'masks']] = selection[['XY_bins', 'XY_center_vox_cnt_perbin', 'XY_mask_vox_cnt_perbin']]\n",
    "    #     wedges_df[['bins', 'center', 'masks']] = selection[['XY_wedges', 'XY_center_vox_cnt_perwedge', 'XY_mask_vox_cnt_perwedge']]\n",
    "    #     Z_df[['bins', 'center', 'masks']] = selection[['Z_slices', 'Z_center_vox_cnt', 'Z_mask_vox_cnt']]\n",
    "\n",
    "    #     dfs = [selection[['dataset', 'image_name']]]\n",
    "    #     for df, prefix in zip([bins_df, wedges_df, Z_df], [\"XY_bins_\", \"XY_wedges_\", \"Z_slices_\"]):\n",
    "    #         single_df = pd.DataFrame(list(zip(df[\"bins\"].values[0][1:-1].split(\", \"), \n",
    "    #                                         df[\"masks\"].values[0][1:-1].split(\", \"),\n",
    "    #                                         df[\"center\"].values[0][1:-1].split(\", \"))), columns =['bins', 'mask', 'obj']).astype(int)\n",
    "    #         # single_df['mask_fract'] = single_df['mask']/single_df['mask'].max()\n",
    "    #         # single_df['obj_norm'] = (single_df[\"obj\"]/single_df[\"mask_fract\"]).fillna(0)\n",
    "    #         # single_df['portion_per_bin'] = (single_df[\"obj\"] / single_df[\"obj\"].sum())*100\n",
    "    #         # if \"Z_\" in prefix:\n",
    "    #         #     single_df['bins'] = (single_df[\"bins\"]/max(single_df.bins)*10).apply(np.floor)\n",
    "\n",
    "    #         if \"Z_\" in prefix:\n",
    "    #             single_df =  single_df.drop(single_df[single_df['mask'] == 0].index)\n",
    "    #             single_df['bins'] = (single_df[\"bins\"]/max(single_df.bins)*10).apply(np.floor)\n",
    "        \n",
    "    #         single_df['mask_fract'] = single_df['mask']/single_df['mask'].max()\n",
    "    #         single_df['obj_norm'] = (single_df[\"obj\"]/single_df[\"mask_fract\"]).fillna(0)\n",
    "    #         single_df['portion_per_bin'] = (single_df[\"obj\"] / single_df[\"obj\"].sum())*100\n",
    "\n",
    "    #         sumstats_df = pd.DataFrame()\n",
    "\n",
    "    #         s = single_df['bins'].repeat(single_df['obj_norm'])\n",
    "    #         sumstats_df['hist_mean']=[s.mean()]\n",
    "    #         sumstats_df['hist_median']=[s.median()]\n",
    "    #         if single_df['obj_norm'].sum() != 0: sumstats_df['hist_mode']=[s.mode()[0]]\n",
    "    #         else: sumstats_df['hist_mode']=['NaN']\n",
    "    #         sumstats_df['hist_min']=[s.min()]\n",
    "    #         sumstats_df['hist_max']=[s.max()]\n",
    "    #         sumstats_df['hist_range']=[s.max() - s.min()]\n",
    "    #         sumstats_df['hist_stdev']=[s.std()]\n",
    "    #         sumstats_df['hist_skew']=[s.skew()]\n",
    "    #         sumstats_df['hist_kurtosis']=[s.kurtosis()]\n",
    "    #         sumstats_df['hist_var']=[s.var()]\n",
    "    #         sumstats_df.columns = [prefix+col for col in sumstats_df.columns]\n",
    "    #         dfs.append(sumstats_df.reset_index())\n",
    "    #     combined_df = pd.concat(dfs, axis=1).drop(columns=\"index\")\n",
    "    #     nuc_hist_dfs.append(combined_df)\n",
    "    # dist_center_summary = pd.concat(nuc_hist_dfs, ignore_index=True)\n",
    "    # dist_center_summary.insert(2, column=\"object\", value=\"nuc\")\n",
    "\n",
    "    # dist_summary = pd.concat([dist_org_summary, dist_center_summary], axis=0).set_index(group_by).sort_index()\n",
    "\n",
    "    # ###################\n",
    "    # # add normalization\n",
    "    # ###################\n",
    "    # # organelle area fraction\n",
    "    # area_fractions = []\n",
    "    # for idx in org_summary.index.unique():\n",
    "    #     org_vol = org_summary.loc[idx][('volume', 'sum')]\n",
    "    #     cell_vol = regions_summary.loc[idx[:-1] + ('cell',)][\"volume\"]\n",
    "    #     afrac = org_vol/cell_vol\n",
    "    #     area_fractions.append(afrac)\n",
    "    # org_summary[('volume', 'fraction')] = area_fractions\n",
    "    # # TODO: add in line to reorder the level=0 columns here\n",
    "\n",
    "    # # contact sites volume normalized\n",
    "    # # norm_toA_list = []\n",
    "    # # norm_toB_list = []\n",
    "    # norm_to_list = {}\n",
    "    # for col in contact_summary.index:\n",
    "    #     for idx, cha in enumerate(string.ascii_uppercase[:len(max(contact_summary.index.get_level_values('object').str.split(splitter), key=len))]):\n",
    "    #         if cha not in norm_to_list:\n",
    "    #             norm_to_list[f\"{cha}\"] = []\n",
    "    #         if ((idx+1) <= len(col[-1].split(splitter))):\n",
    "    #             norm_to_list[f\"{cha}\"].append(contact_summary.loc[col][('volume', 'sum')]/org_summary.loc[col[:-1]+(col[-1].split(splitter)[idx],)][('volume', 'sum')])\n",
    "    #         else:\n",
    "    #             norm_to_list[f\"{cha}\"].append(None)\n",
    "    # for cha in string.ascii_uppercase[:len(max(contact_summary.index.get_level_values('object').str.split(splitter), key=len))]:\n",
    "    #     contact_summary[('volume', f'norm_to_{cha}')] = norm_to_list[f\"{cha}\"]\n",
    "    #     # norm_toA_list.append(contact_summary.loc[col][('volume', 'sum')]/org_summary.loc[col[:-1]+(col[-1].split(splitter)[0],)][('volume', 'sum')])\n",
    "    #     # norm_toB_list.append(contact_summary.loc[col][('volume', 'sum')]/org_summary.loc[col[:-1]+(col[-1].split(splitter)[1],)][('volume', 'sum')])\n",
    "    # # contact_summary[('volume', 'norm_to_A')] = norm_toA_list\n",
    "    # # contact_summary[('volume', 'norm_to_B')] = norm_toB_list\n",
    "\n",
    "    # # number and area of individuals organelle involved in contact\n",
    "    # cont_cnt = org_df[group_by]\n",
    "    # cont_cnt[[col.split('_')[0] for col in org_df.columns if col.endswith((\"_count\"))]] = org_df[[col for col in org_df.columns if col.endswith((\"_count\"))]].astype(bool)\n",
    "    # cont_cnt_perorg = cont_cnt.groupby(group_by).agg('sum')\n",
    "    # cont_cnt_perorg.columns = pd.MultiIndex.from_product([cont_cnt_perorg.columns, ['count_in']])\n",
    "    # for col in cont_cnt_perorg.columns:\n",
    "    #     cont_cnt_perorg[(col[0], 'num_fraction_in')] = cont_cnt_perorg[col].values/org_summary[('volume', 'count')].values\n",
    "    # cont_cnt_perorg.sort_index(axis=1, inplace=True)\n",
    "    # org_summary = pd.merge(org_summary, cont_cnt_perorg, on=group_by, how='outer')\n",
    "\n",
    "\n",
    "    # ###################\n",
    "    # # flatten datasheets and combine\n",
    "    # # TODO: restructure this so that all of the datasheets and unstacked and then reorded based on shared level 0 columns before flattening\n",
    "    # ###################\n",
    "    # # org flattening\n",
    "    # org_final = org_summary.unstack(-1)\n",
    "    # for col in org_final.columns:\n",
    "    #     if col[1] in ('count_in', 'num_fraction_in') or col[0].endswith(('_count', '_volume')):\n",
    "    #         if col[2] not in col[0]:\n",
    "    #             org_final.drop(col,axis=1, inplace=True)\n",
    "    # ########################################################################\n",
    "    # # MAKING new_col_order flexible to work with any organelle input values and combo number\n",
    "    # #######################################################################\n",
    "    # new_col_order = ['dataset', 'image_name', 'object', 'volume', 'surface_area', 'SA_to_volume_ratio', \n",
    "    #                  'equivalent_diameter', 'extent', 'euler_number', 'solidity', 'axis_major_length'] \n",
    "    # all_combos = []\n",
    "    # for n in list(map(lambda x:x+2, (range(len(all_orgs)-1)))):\n",
    "    #         for o in itertools.combinations(all_orgs, n):\n",
    "    #             all_combos.append(check_for_existing_combo(o, ctc, splitter))\n",
    "    # combos = [splitter.join(cont) for cont in all_combos]\n",
    "    # for combo in combos:\n",
    "    #     new_col_order += [f\"{combo}\", f\"{combo}_count\", f\"{combo}_volume\"]\n",
    "    # new_cols = org_final.columns.reindex(new_col_order, level=0)\n",
    "    # org_final = org_final.reindex(columns=new_cols[0])\n",
    "    # org_final.columns = [\"_\".join((col_name[-1], col_name[1], col_name[0])) for col_name in org_final.columns.to_flat_index()]\n",
    "\n",
    "    # #renaming, filling \"NaN\" with 0 when needed, and removing ER_std columns\n",
    "    # for col in org_final.columns:\n",
    "    #     if '_count_in_' or '_fraction_in_' in col:\n",
    "    #         org_final[col] = org_final[col].fillna(0)\n",
    "    #     if col.endswith((\"_count_volume\",\"_sum_volume\", \"_mean_volume\", \"_median_volume\")):\n",
    "    #         org_final[col] = org_final[col].fillna(0)\n",
    "    #     if col.endswith(\"_count_volume\"):\n",
    "    #         org_final.rename(columns={col:col.split(\"_\")[0]+\"_count\"}, inplace=True)\n",
    "    #     if col.startswith(\"ER_std_\"):\n",
    "    #         org_final.drop(columns=[col], inplace=True)\n",
    "    # org_final = org_final.reset_index()\n",
    "\n",
    "    # # contacts flattened\n",
    "    # contact_final = contact_summary.unstack(-1)\n",
    "    # contact_final.columns = [\"_\".join((col_name[-1], col_name[1], col_name[0])) for col_name in contact_final.columns.to_flat_index()]\n",
    "\n",
    "    # #renaming and filling \"NaN\" with 0 when needed\n",
    "    # for col in contact_final.columns:\n",
    "    #     if col.endswith((\"_count_volume\",\"_sum_volume\", \"_mean_volume\", \"_median_volume\")):\n",
    "    #         contact_final[col] = contact_final[col].fillna(0)\n",
    "    #     if col.endswith(\"_count_volume\"):\n",
    "    #         contact_final.rename(columns={col:col.split(\"_\")[0]+\"_count\"}, inplace=True)\n",
    "    # contact_final = contact_final.reset_index()\n",
    "\n",
    "    # # distributions flattened\n",
    "    # dist_final = dist_summary.unstack(-1)\n",
    "    # dist_final.columns = [\"_\".join((col_name[1], col_name[0])) for col_name in dist_final.columns.to_flat_index()]\n",
    "    # dist_final = dist_final.reset_index()\n",
    "\n",
    "    # # regions flattened & normalization added\n",
    "    # regions_final = regions_summary.unstack(-1)\n",
    "    # regions_final.columns = [\"_\".join((col_name[1], col_name[0])) for col_name in regions_final.columns.to_flat_index()]\n",
    "    # regions_final['nuc_area_fraction'] = regions_final['nuc_volume'] / regions_final['cell_volume']\n",
    "    # regions_final = regions_final.reset_index()\n",
    "\n",
    "    # # combining them all\n",
    "    # combined = pd.merge(org_final, contact_final, on=[\"dataset\", \"image_name\"], how=\"outer\")\n",
    "    # combined = pd.merge(combined, dist_final, on=[\"dataset\", \"image_name\"], how=\"outer\")\n",
    "    # combined = pd.merge(combined, regions_final, on=[\"dataset\", \"image_name\"], how=\"outer\").set_index([\"dataset\", \"image_name\"])\n",
    "    # combined.columns = [col.replace('sum', 'total') for col in combined.columns]\n",
    "\n",
    "    # ###################\n",
    "    # # export summary sheets\n",
    "    # ###################\n",
    "    # org_summary.to_csv(out_path + f\"/{out_preffix}per_org_summarystats.csv\")\n",
    "    # contact_summary.to_csv(out_path + f\"/{out_preffix}per_contact_summarystats.csv\")\n",
    "    # dist_summary.to_csv(out_path + f\"/{out_preffix}distribution_summarystats.csv\")\n",
    "    # regions_summary.to_csv(out_path + f\"/{out_preffix}per_region_summarystats.csv\")\n",
    "    # combined.to_csv(out_path + f\"/{out_preffix}summarystats_combined.csv\")\n",
    "\n",
    "    # print(f\"Processing of {fl_count} files from {ds_count} dataset(s) is complete.\")\n",
    "    # return f\"{fl_count} files from {ds_count} dataset(s) were processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7f9ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = _batch_summary_stats_debug([str(out_data_path)],\n",
    "                     str(out_data_path),\n",
    "                     f\"{datetime.today().strftime('%Y%m%d')}_skel_test\",\n",
    "                     splitter=\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbb34a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(unstacked_cont, axis=0).sort_index(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1331b600",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([xx[3], xx[4]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe420a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b0d022",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b679dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ad9188",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(xx[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ba7af1",
   "metadata": {},
   "source": [
    "## **OUTPUT**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2716f479",
   "metadata": {},
   "source": [
    "### **Branch Output**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5133d6",
   "metadata": {},
   "source": [
    "#### **Column Reference for Branch Table**\n",
    "\n",
    "- `skel-obj-id` : the numeric ID assigned to the skeleton object to which the branch belongs, this ID is also the same as the label of organelle object the skeleton object represents\n",
    "- `node-id-src` : the numeric point ID assigned to the source node of the branch\n",
    "- `node-id-dst` : the numeric point ID assigned to the destination node of the branch\n",
    "- `deg-src` : the source node's degree of connectivity\n",
    "- `deg-dst`: the destination node's degree of connectivity\n",
    "- `branch-distance*` : the length of the branch\n",
    "- `branch-type` : the type of branch, dependent on node behavior:\n",
    "###### 0 - endpoint to endpoint | 1 - junction to endpoint | 2 - junction to junction | 3 - cycle\n",
    "- `image-coord-src-0` : the z-coordinate of the source node\n",
    "- `image-coord-src-1` : the y-coordinate of the source node\n",
    "- `image-coord-src-2` : the x-coordinate of the source node\n",
    "- `image-coord-dst-0` : the z-coordinate of the destination\n",
    "- `image-coord-dst-1` : the y-coordinate of the destination\n",
    "- `image-coord-dst-2` : the x-coordinate of the destination\n",
    "- `coord-src-0*` : the z-coordinate of the source node\n",
    "- `coord-src-1*` : the y-coordinate of the source node\n",
    "- `coord-src-2*` : the x-coordinate of the source node\n",
    "- `coord-dst-0*` : the z-coordinate of the destination node\n",
    "- `coord-dst-1*` : the y-coordinate of the destination node\n",
    "- `coord-dst-2*` : the x-coordinate of the destination node\n",
    "- `euclidean-distance*` : The length of the line segment between the source and destination node\n",
    "- `str-prop` : A proportion measuring the straightness of the branch, it is calculated by (euclidean-distance / branch-distance)\n",
    "\n",
    "###### The branch table is the only table where the branch-id column is redundant as the row index value is equivalent to the branch-id\n",
    "\n",
    "###### * Measurements affected by scale, thus values are in real world units (microns) terms  if a scale is not provided, these measurements will be in voxel terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f4b98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_branch_table.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221a7f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_branch_table.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942d5206",
   "metadata": {},
   "source": [
    "### **Node Output**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735cf3f6",
   "metadata": {},
   "source": [
    "#### **Column Reference for Node Table**\n",
    "\n",
    "- `node-id` : the numeric point ID assigned to the node\n",
    "- `node-type` : the node type dependent on the degree of connectivity\n",
    "- `connectivity`: the number of points the node contacts in voxel space (degree of connectivity)\n",
    "- `image-coord-0` : the z-coordinate of the node\n",
    "- `image-coord-1` : the y-coordinate of the node\n",
    "- `image-coord-2` : the x-coordinate of the node\n",
    "- `coord-0*` : the z-coordinate of the node\n",
    "- `coord-1*` : the y-coordinate of the node\n",
    "- `coord-2*` : the x-coordinate of the node\n",
    "- `branch-id(s)` : a list consisting of the numeric ID(s) of the branch or branches the that node is a part of\n",
    "- `skel-obj-id` : the numeric ID assigned to the skeleton object of which the node belongs to, this ID is also the same as the label of organelle object the skeleton object represents\n",
    "\n",
    "###### * Measurements affected by scale, thus values are in real world units (microns) terms  if a scale is not provided, these measurements will be in voxel terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e534d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_node_table.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7640d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_node_table.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6687c45",
   "metadata": {},
   "source": [
    "### **Skeleton Object Output**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c89627f",
   "metadata": {},
   "source": [
    "#### **Column Reference for Skeleton Object Table**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12660eb4",
   "metadata": {},
   "source": [
    "- `skel-obj-id` : the numeric ID assigned to the skeleton object, this ID is also the same as the label of organelle object this skeleton object represents\n",
    "- `skel-type` : the classification of the skeleton object based on its branch behavior\n",
    "- `skel-type-num` : the numeric classification of the skeleton object based on its branch behavior\n",
    "- `brh-count` : the number of branches the skeleton object contains\n",
    "- `branch-id(s)` : a list consisting of the numeric ID(s) of the branch or branches that the skeleton object contains\n",
    "- `min-brh-length*` : the minimum branch length of the branches within the skeleton\n",
    "- `max-brh-length*` : the maximum branch length of the branches within the skeleton\n",
    "- `ave-brh-length*` : the mean of the branch lengths within the skeleton\n",
    "- `sd-brh-length*` : the standard deviation of the branch lengths within the skeleton\n",
    "- `med-brh-length*` : the median of the branch lengths within the skeleton\n",
    "- `total-length*` : the sum of the branch lengths within the skeleton object\n",
    "- `brh-type-0-tot` : the number of type 0 (endpoint to endpoint) branches the skeleton object contains (maximum is 1)\n",
    "- `brh-type-0-ids` : the numeric ID of type 0 branch the skeleton object contains (if it contains one)\n",
    "- `brh-type-1-tot` : the number of type 1 (junction to endpoint) branches the skeleton object contains\n",
    "- `brh-type-1-ids` : the numeric ID(s) of the type 1 branch or branches the skeleton object contains\n",
    "- `brh-type-2-tot` : the number of type 2 (junction to junction) branches the skeleton object contains\n",
    "- `brh-type-2-ids` : the numeric ID(s) of the type 2 branch or branches the skeleton object contains\n",
    "- `brh-type-3-tot` : the number of type 3 (cycle) branches the skeleton object contains\n",
    "- `brh-type-3-ids` : the numeric ID(s) of the type 3 branch or branches the skeleton object contains\n",
    "- `node-count` : the number of nodes within the skeleton object\n",
    "- `ep-count` : the number of endpoints within the skeleton object\n",
    "- `jn-count` : the number of junction nodes within the skeleton object\n",
    "- `ave-jn-deg` : the average degree of the junction nodes in the skeleton object\n",
    "- `max-deg` : the maximum degree of connectivity of the nodes in the skeleton object\n",
    "- `node-id(s)` : the numeric point ID(s) of the nodes within the skeleton object\n",
    "- `mean-brh-str` : the mean branch straightness proportion of the branches in the skeleton object\n",
    "\n",
    "###### * Measurements affected by scale, thus values are in real world units (microns) terms  if a scale is not provided, these measurements will be in voxel terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f71695",
   "metadata": {},
   "outputs": [],
   "source": [
    "_skel_table.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd615547",
   "metadata": {},
   "outputs": [],
   "source": [
    "_skel_table.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878e0a60",
   "metadata": {},
   "source": [
    "### **Skeleton Summary Output**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b7b20b",
   "metadata": {},
   "source": [
    "#### **Column Reference for Skeleton Object Table**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234cd195",
   "metadata": {},
   "source": [
    "- `total-length*` : combined length of all the branches in the organelle skeleton\n",
    "- `point-count` : the amount of voxels throughout the entirety of the organelle skeleton\n",
    "\n",
    "-- Skeleton Object section --\n",
    "\n",
    "- `skel-obj-count` : the amount of skeleton objects in the organelle skeleton\n",
    "- `punc-count` : the amount of punctates in the organelle skeleton\n",
    "- `rod-count` : the amount of rods in the organelle skeleton\n",
    "- `net-count` : the amount of networks in the organelle skeleton\n",
    "- `prop-obj-punc` : the proportion of skeleton objects that are punctates\n",
    "- `prop-obj-rod` : the proportion of skeleton objects that are rods\n",
    "- `prop-obj-net` : the proportion of skeleton objects that are networks\n",
    "- `punc-tot-len*` : the total length of the punctates (only the non-absolute punctates provide length)\n",
    "- `rod-tot-len*` : the total length of the rods\n",
    "- `net-tot-len*` : the total length of the networks\n",
    "- `prop-len-punc` : the proportion of the organelle skeleton's length that are from punctates (only the non-absolute punctates provide length)\n",
    "- `prop-len-rod` : the proportion of the organelle skeleton's length that are from rods\n",
    "- `prop-len-net` : the proportion of the organelle skeleton's length that are from networks\n",
    "- `ave-len-obj` : the average total length of the skeleton objects\n",
    "- `min-len-obj` : the minimum total length of the skeleton objects\n",
    "- `max-len-obj` : the maximum total length of the skeleton objects\n",
    "- `ave-brh-obj` : the average amount of branches per skeleton object\n",
    "- `min-brh-obj` : the minimum amount of branches per skeleton object\n",
    "- `max-brh-obj` : the maximum amount of branches per skeleton object\n",
    "\n",
    "-- Branch section --\n",
    "\n",
    "- `brh-count` : the total amount of branches in the organelle skeleton\n",
    "- `min-brh-len*` : length of the shortest branch in the organelle skeleton\n",
    "- `max-brh-len*` : length of the longest branch in the organelle skeleton\n",
    "- `ave-brh-len*` : the mean branch length in the organelle skeleton\n",
    "- `type-0-brhs` : the amount of type 0 (endpoint to endpoint) branches\n",
    "- `type-1-brhs` : the amount of type 1 (junction to endpoint) branches\n",
    "- `type-2-brhs` : the amount of type 2 (junction to junction) branches\n",
    "- `type-3-brhs` : the amount of type 3 (cycle) branches\n",
    "- `prop-brh-t0` : the proportion of branches in the organelle skeleton that are type 0 branches\n",
    "- `prop-brh-t1` : the proportion of branches in the organelle skeleton that are type 1 branches\n",
    "- `prop-brh-t2` : the proportion of branches in the organelle skeleton that are type 2 branches\n",
    "- `prop-brh-t3` : the proportion of branches in the organelle skeleton that are type 3 branches\n",
    "- `t0-brh-len*` : combined length of all type 0 branches\n",
    "- `t1-brh-len*` : combined length of all type 1 branches\n",
    "- `t2-brh-len*` : combined length of all type 2 branches\n",
    "- `t3-brh-len*` : combined length of all type 3 branches\n",
    "- `prop-len-t0` : the proportion of the organelle skeleton's length that is from type 0 branches\n",
    "- `prop-len-t1` : the proportion of the organelle skeleton's length that is from type 1 branches\n",
    "- `prop-len-t2` : the proportion of the organelle skeleton's length that is from type 2 branches\n",
    "- `prop-len-t3` : the proportion of the organelle skeleton's length that is from type 3 branches\n",
    "\n",
    "-- Node section --\n",
    "\n",
    "- `node-count` : the amount of nodes in the organelle skeleton\n",
    "- `ave-deg-nodes` : the average degree of connectivity for the nodes in the organelle skeleton\n",
    "- `ep-count` : the amount of endpoint nodes in the organelle skeleton\n",
    "- `jn-count` : the amount of junction nodes in the organelle skeleton\n",
    "- `ap-count` : the amount of absolute punctates in the organelle skeleton\n",
    "- `prop-ep` : the proportion of the nodes that are endpoints\n",
    "- `prop-jn` : the proportion of the nodes that are junction nodes\n",
    "- `prop-ap` : the proportion of the nodes that are absolute punctates\n",
    "\n",
    "###### * Measurements affected by scale, thus values are in real world units (microns) terms  if a scale is not provided, these measurements will be in voxel terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c2af06",
   "metadata": {},
   "outputs": [],
   "source": [
    "_skel_sum_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c3bade",
   "metadata": {},
   "source": [
    "### **`_get_org_morphology_3D`** Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdb1e4e",
   "metadata": {},
   "source": [
    "#### **Column Reference for `_get_org_morphology_3D` output**\n",
    "\n",
    "-- Default morphology section --\n",
    "\n",
    "- `object`: the shorted name of the organelle being observed\n",
    "- `label`: the numeric ID assigned to the organelle object\n",
    "- `scale*`: the real world dimensions of each voxel in the microscopy image (ZYX in which each dimension is measured in microns)\n",
    "- `centroid-0*`: the Z coordinate of the centroid (center of mass) of the organelle object\n",
    "- `centroid-1*`: the Y coordinate of the centroid (center of mass) of the organelle object\n",
    "- `centroid-2*`: the X coordinate of the centroid (center of mass) of the organelle object\n",
    "- `bbox-0`: the minimum Z coordinate value of the cuboid that bounds the organelle object, lowest Z of the bounding box\n",
    "- `bbox-1`: the minimum Y coordinate value of the cuboid that bounds the organelle object, lowest Y of the bounding box\n",
    "- `bbox-2`: the minimum X coordinate value of the cuboid that bounds the organelle object, lowest X of the bounding box\n",
    "- `bbox-3`: the maximum Z coordinate value of the cuboid that bounds the organelle object, highest Z of the bounding box\n",
    "- `bbox-4`: the maximum Y coordinate value of the cuboid that bounds the organelle object, highest Y of the bounding box\n",
    "- `bbox-5`: the maximum X coordinate value of the cuboid that bounds the organelle object, highest X of the bounding box\n",
    "- `surface_area*`: the estimated area of the organelle object's surface (after triangulation and interpolation using the marching cubes method)\n",
    "- `volume*`: the amount of space covered by the organelle object\n",
    "- `SA_to_volume_ratio*`: the surface area divided by the volume of the organelle object, given as a ratio\n",
    "- `equivalent_diameter*`: the diameter of a perfect sphere with the same volume as the organelle object\n",
    "- `extent`: the proportion of the bounding cuboid/box filled by the organelle object\n",
    "- `euler_number`: the value is derived from the formula ***Ï‡ = V-E+F-C*** where **V** is the number of **verticies**, **E** is the number of **edges**, **F** is the number of **faces**  and **C** is the number of **disconnected components**\n",
    "- `solidity`: the proportion of the convex hull filled in by the organelle object; the convex hull being the smallest convex polygon that contains all of the voxels in the organelle object\n",
    "- `axis_major_length*`: The length of the major axis of the ellipsoid that shares the same second central moments as the organelle object\n",
    "- `min_intensity`: the minimum intensity/signal within the region of the organelle object\n",
    "- `max_intensity`: the maximum intensity/signal within the region of the organelle object\n",
    "- `mean_intensity` : the mean intensity/signal within the region of the organelle object\n",
    "- `standard_deviation_intensity`: the standard deviation of the intensity/signal values within the region of the organelle object\n",
    "\n",
    "-- Skeleton section --\n",
    "\n",
    "- `skel-type` - the classification of the skeleton object based on its branch behavior\n",
    "- `skel-brh-count` - the amount of branches the skeleton object contains\n",
    "- `skel-min-brh-length*` - the minimum branch length of the branches within the skeleton\n",
    "- `skel-max-brh-length*` - the maximum branch length of the branches within the skeleton\n",
    "- `skel-ave-brh-length*` - the mean of the branch lengths within the skeleton\n",
    "- `skel-sd-brh-length*` - the standard deviation of the branch lengths within the skeleton\n",
    "- `skel-med-brh-length*` - the median of the branch lengths within the skeleton\n",
    "- `skel-total-length*` - the sum of the branch lengths within the skeleton object\n",
    "- `skel-node-count` - the amount of nodes within the skeleton object\n",
    "- `skel-ep-count` - the amount of endpoints within the skeleton object\n",
    "- `skel-jn-count` - the amount of junction nodes within the skeleton object\n",
    "- `skel-ave-jn-deg` - the average degree of the junction nodes in the skeleton object\n",
    "- `skel-max-deg` - the maximum degree of connectivity of the nodes in the skeleton object\n",
    "- `skel-mean-brh-str` - the mean branch straightness proportion of the branches in the skeleton object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d2bf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_morph_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d69c027",
   "metadata": {},
   "source": [
    "## **CONCLUSION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3737bd3",
   "metadata": {},
   "source": [
    "###### Although this is all only the beginning in terms of the effectiveness of skeletonization analysis for three-dimensional anisotrophic data, there is a lot of potential. With more testing, corrections as well as new ideas will emerge to further improve the project. More specifically, there could be an additional metric depicting the complexity of a network. Regardless the future looks bright for skeletonization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299472fc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "infer-subc-sc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
