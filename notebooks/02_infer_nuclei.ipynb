{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer ***nuclei*** - 2️⃣ \n",
    "\n",
    "--------------\n",
    "\n",
    "## OBJECTIVE: ✅ Infer sub-cellular component #2: ***nuclei***  in order to understand interactome \n",
    "\n",
    "To measure shape, position, size, and interaction of eight organelles/cellular components ***nuclei***.  \n",
    "\n",
    "Dependencies:\n",
    "***Soma*** and ***cytosol*** inference rely on the ***nuclei*** inference.  Therefore all of the sub-cellular objects rely on the NU segmentation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top level imports\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "# TODO:  prune the imports.. this is the big set for almost all organelles\n",
    "# # function for core algorithm\n",
    "from scipy import ndimage as ndi\n",
    "import aicssegmentation\n",
    "from aicssegmentation.core.seg_dot import dot_3d_wrapper, dot_slice_by_slice, dot_2d_slice_by_slice_wrapper, dot_3d\n",
    "from aicssegmentation.core.pre_processing_utils import ( intensity_normalization, \n",
    "                                                         image_smoothing_gaussian_3d,  \n",
    "                                                         image_smoothing_gaussian_slice_by_slice )\n",
    "from aicssegmentation.core.utils import topology_preserving_thinning, size_filter\n",
    "from aicssegmentation.core.MO_threshold import MO\n",
    "from aicssegmentation.core.utils import hole_filling\n",
    "from aicssegmentation.core.vessel import filament_2d_wrapper, vesselnessSliceBySlice\n",
    "from aicssegmentation.core.output_utils import   save_segmentation,  generate_segmentation_contour\n",
    "                                                 \n",
    "from skimage import filters\n",
    "from skimage.segmentation import watershed\n",
    "from skimage.feature import peak_local_max\n",
    "from skimage.morphology import remove_small_objects, binary_closing, ball , dilation, remove_small_holes   # function for post-processing (size filter)\n",
    "from skimage.measure import label\n",
    "\n",
    "# # package for io \n",
    "from aicsimageio import AICSImage\n",
    "\n",
    "import napari\n",
    "\n",
    "### import local python functions in ../infer_subc_2d\n",
    "sys.path.append(os.path.abspath((os.path.join(os.getcwd(), '..'))))\n",
    "\n",
    "\n",
    "from infer_subc_2d.utils.file_io import (read_czi_image,\n",
    "                                                                    read_input_image, \n",
    "                                                                    list_image_files, \n",
    "                                                                    export_ome_tiff, \n",
    "                                                                    etree_to_dict, \n",
    "                                                                    save_parameters, \n",
    "                                                                    load_parameters, \n",
    "                                                                    export_ndarray)\n",
    "from infer_subc_2d.utils.img import *\n",
    "\n",
    "from infer_subc_2d.constants import (TEST_IMG_N,\n",
    "                                                                    NUC_CH ,\n",
    "                                                                    LYSO_CH ,\n",
    "                                                                    MITO_CH ,\n",
    "                                                                    GOLGI_CH ,\n",
    "                                                                    PEROXI_CH ,\n",
    "                                                                    ER_CH ,\n",
    "                                                                    LIPID_CH ,\n",
    "                                                                    RESIDUAL_CH )          \n",
    "\n",
    "from infer_subc_2d.organelles.soma import infer_soma\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "test_img_n = TEST_IMG_N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMAGE PROCESSING  OBJECTIVE :  infer ***nuclei***\n",
    " \n",
    "\n",
    "NOTE:  using Allen Cell Segmenter  [Nucleophosmin](https://www.allencell.org/cell-observations/category/nucleophosmin) might be a good generic mechanism.  e.g.\n",
    "-  [playground_npm1.ipynb](https://github.com/AllenInstitute/aics-segmentation/blob/master/lookup_table_demo/playground_npm1.ipynb) and [npm1.py](https://github.com/AllenInstitute/aics-segmentation/blob/master/aicssegmentation/structure_wrapper/seg_npm1.py) and [npm1_SR.py](https://github.com/AllenInstitute/aics-segmentation/blob/master/aicssegmentation/structure_wrapper/seg_npm1_SR.py)\n",
    "\n",
    "\n",
    "> #### Note:  this initial inferred object -- the ***nuclei*** of the brightest cell -- is implicitly used in inferring the ***soma*** and ***cytosol*** objects.  \n",
    "\n",
    "\n",
    "------------------------\n",
    "# LOAD RAW IMAGE DATA\n",
    "Identify path to _raw_ image data and load our example image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the datapath\n",
    "# all the imaging data goes here.\n",
    "# CUSTOMIZE HERE --->\n",
    "data_root_path = Path(os.path.expanduser(\"~\")) / \"Projects/Imaging/data\"\n",
    "\n",
    "# linearly unmixed \".czi\" files are here\n",
    "data_path = data_root_path / \"raw\"\n",
    "im_type = \".czi\"\n",
    "\n",
    "# get the list of all files\n",
    "img_file_list = list_image_files(data_path,im_type)\n",
    "test_img_name = img_file_list[test_img_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/ahenrie')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(os.path.expanduser(\"~\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/napariNEW/lib/python3.9/site-packages/ome_types/_convenience.py:105: FutureWarning: The default XML parser will be changing from 'xmlschema' to 'lxml' in version 0.4.0.  To silence this warning, please provide the `parser` argument, specifying either 'lxml' (to opt into the new behavior), or'xmlschema' (to retain the old behavior).\n",
      "  d = to_dict(os.fspath(xml), parser=parser, validate=validate)\n"
     ]
    }
   ],
   "source": [
    "img_data,meta_dict = read_czi_image(test_img_name)\n",
    "\n",
    "\n",
    "# get some top-level info about the RAW data\n",
    "channel_names = meta_dict['name']\n",
    "img = meta_dict['metadata']['aicsimage']\n",
    "scale = meta_dict['scale']\n",
    "channel_axis = meta_dict['channel_axis']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure we have removed Z\n",
    "if len(scale)>2:\n",
    "    scale = scale[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHOOZE Z-SLICE\n",
    "\n",
    "Lets find the slice with the most overall intensity..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_to_agg = ( LYSO_CH ,\n",
    "                        MITO_CH ,\n",
    "                        GOLGI_CH ,\n",
    "                        PEROXI_CH ,\n",
    "                        ER_CH ,\n",
    "                        LIPID_CH )\n",
    "                            \n",
    "nuc_ch = NUC_CH\n",
    "optimal_Z = find_optimal_Z(img_data, nuc_ch, ch_to_agg) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now get the single \"optimal\" slice of all our organelle channels...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_2D = img_data[:,[optimal_Z],:,:].copy()\n",
    "\n",
    "soma_mask =  infer_soma(img_2D) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SO_mask' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/ahenrie/Projects/Imaging/infer-subc-2D/notebooks/02_infer_nuclei.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ahenrie/Projects/Imaging/infer-subc-2D/notebooks/02_infer_nuclei.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m viewer \u001b[39m=\u001b[39m napari\u001b[39m.\u001b[39mViewer()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ahenrie/Projects/Imaging/infer-subc-2D/notebooks/02_infer_nuclei.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m viewer\u001b[39m.\u001b[39madd_image(SO_mask,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ahenrie/Projects/Imaging/infer-subc-2D/notebooks/02_infer_nuclei.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     scale\u001b[39m=\u001b[39mscale)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SO_mask' is not defined"
     ]
    }
   ],
   "source": [
    "viewer = napari.Viewer()\n",
    "viewer.add_image(SO_mask,\n",
    "    scale=scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# IMAGE PROCESSING PROTOTYPE, Objective 2:  infer NUCLEI\n",
    " \n",
    "## details\n",
    "\n",
    "➡️ INPUT\n",
    "\n",
    "- channel 0\n",
    "\n",
    "PRE-PROCESSING\n",
    "-  scale to min 0, max 1.0\n",
    "- median Filter window 4\n",
    "-  gaussian 1.34\n",
    "\n",
    "CORE-PROCESSING\n",
    "  - threshold method minimum cross-entropy.  \n",
    "    - objects 50-400 pixels, \n",
    "    - threshold smoothing scale: 1.34 (later 1 pixel\n",
    "    - threshold correction factor: 0.9 (later 1.2 )\n",
    "    - lower / upper bounds  (.1,1) ?\n",
    "    - log transformed thresholding\n",
    "    - fill holes\n",
    "\n",
    "\n",
    "POST-PROCESSING\n",
    "  - fill holes\n",
    "  - remove small objects\n",
    "\n",
    "\n",
    "OUTPUT ➡️ \n",
    "- labels of NUCLEI\n",
    "\n",
    "\n",
    "> #### Note:  in later steps we will limit each analysis to a single object, but at this stage we have multiple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA IMPORT\n",
    "\n",
    "Get the \"raw\" signals we need to analyze as well as any other dependencies in \"inferred\" objects.  \n",
    "\n",
    "> NOTE: we are operating on a single \"test\" image in this notebook.  The batch-processing of all the images will be happen at the end of the notebook after we have developed/confirmed the setmentation procedures and parameter settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRE- PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# INPUT\n",
    "###################\n",
    "raw_nuclei = img_2D[NUC_CH].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# PRE_PROCESSING\n",
    "###################           \n",
    "# \n",
    "nuclei = min_max_intensity_normalization(raw_nuclei )\n",
    "\n",
    "med_filter_size = 4   \n",
    "# structure_img_median_3D = ndi.median_filter(struct_img,    size=med_filter_size  )\n",
    "nuclei = median_filter_slice_by_slice( \n",
    "                                                                nuclei,\n",
    "                                                                size=med_filter_size  )\n",
    "\n",
    "gaussian_smoothing_sigma = 1.34\n",
    "nuclei = image_smoothing_gaussian_slice_by_slice(  nuclei,\n",
    "                                                                                            sigma=gaussian_smoothing_sigma)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> #### NOTE: Thresholding\n",
    "> [Thresholding](https://en.wikipedia.org/wiki/Thresholding_%28image_processing%29) is used to create binary images. A threshold value determines the intensity value separating foreground pixels from background pixels. Foregound pixels are pixels brighter than the threshold value, background pixels are darker. In many cases, images can be adequately segmented by thresholding followed by labelling of *connected components*, which is a fancy way of saying \"groups of pixels that touch each other\".\n",
    "> \n",
    "> Different thresholding algorithms produce different results. [Otsu's method](https://en.wikipedia.org/wiki/Otsu%27s_method) and [Li's minimum cross entropy threshold](https://scikit-image.org/docs/dev/auto_examples/developers/plot_threshold_li.html) are two common algorithms. Below, we use Li. You can use `skimage.filters.threshold_<TAB>` to find different thresholding methods.\n",
    "\n",
    "_Li_ procedure  better matches the CellProfiler pipeline which simply calls it \"Minimum Cross Entropy\" .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORE PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###################\n",
    "# CORE_PROCESSING\n",
    "###################\n",
    "\n",
    "threshold_factor = 0.9 #from cellProfiler\n",
    "thresh_min = .1\n",
    "thresh_max = 1.\n",
    "li_thresholded = apply_log_li_threshold(nuclei, threshold_factor=threshold_factor, thresh_min=thresh_min, thresh_max=thresh_max)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POST PROCESSING\n",
    "\n",
    "> NOTE: the size parameters are by convention defined as one dimensional \"width\", so the inputs to the functions need to be _squared_ i.e. raised to the power of 2: `** 2`.   For volumetric (3D) analysis this would be _cubed_:`**3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# POST_PROCESSING\n",
    "###################\n",
    "\n",
    "hole_width = 5  \n",
    "# # wrapper to remoce_small_objects\n",
    "#removed_holes = remove_small_holes(li_thresholded, hole_width ** 2 )\n",
    "removed_holes = hole_filling(li_thresholded, hole_min=0, hole_max=hole_width**2, fill_2d=True)\n",
    "\n",
    "small_object_width = 15\n",
    "cleaned_img = size_filter_2D(removed_holes, # wrapper to remove_small_objects which can do slice by slice\n",
    "                                                         min_size= small_object_width ** 2, \n",
    "                                                         connectivity=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUTPUT + Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuclei_object = cleaned_img\n",
    "NU_labels = label(cleaned_img   )\n",
    "NU_signal = struct_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "viewer.add_image(\n",
    "    nuclei_object,\n",
    "    scale=scale,\n",
    "    opacity=0.3,\n",
    ")    \n",
    "\n",
    "\n",
    "viewer.add_labels(\n",
    "    NU_labels,\n",
    "    scale=scale,\n",
    "    opacity=0.3,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINE `_infer_nuclei` function\n",
    "\n",
    "Based on the _prototyping_ above define the function to infer nuclei.  \n",
    "\n",
    "> NOTE:  although it takes the parameters as input, they are all \"hard coded\" below, and the function returns the parameters in the same `defaultdict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy this to base.py for easy import\n",
    "##########################\n",
    "#  _infer_nuclei\n",
    "##########################\n",
    "def _infer_nuclei(in_img:  np.ndarray, soma_mask:  np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Procedure to infer nuclei from linearly unmixed input.\n",
    "\n",
    "    Parameters:\n",
    "    ------------\n",
    "    in_img: np.ndarray\n",
    "        a 3d image containing all the channels\n",
    "\n",
    "    soma_mask: np.ndarray\n",
    "        mask\n",
    " \n",
    "    Returns:\n",
    "    -------------\n",
    "    nuclei_object\n",
    "        mask defined extent of NU\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    ###################\n",
    "    # PRE_PROCESSING\n",
    "    ###################                \n",
    "    nuc_ch = NUC_CH\n",
    "    nuclei = _select_channel_from_raw(in_img, nuc_ch)\n",
    "    # nuclei = min_max_intensity_normalization(in_img[NUC_CH].copy() )\n",
    "\n",
    "    med_filter_size = 4   \n",
    "    # structure_img_median_3D = ndi.median_filter(struct_img,    size=med_filter_size  )\n",
    "    nuclei = median_filter_slice_by_slice( nuclei,\n",
    "                                                                    size=med_filter_size  )\n",
    "\n",
    "    gaussian_smoothing_sigma = 1.34\n",
    "    nuclei = image_smoothing_gaussian_slice_by_slice(  nuclei,\n",
    "                                                                                                sigma=gaussian_smoothing_sigma )\n",
    "\n",
    "    ###################\n",
    "    # CORE_PROCESSING\n",
    "    ###################\n",
    "    #struct_obj = struct_img > filters.threshold_li(struct_img)\n",
    "    threshold_factor = 0.9 #from cellProfiler\n",
    "    thresh_min = .1\n",
    "    thresh_max = 1.\n",
    "    nuclei_object = apply_log_li_threshold(nuclei, threshold_factor=threshold_factor, thresh_min=thresh_min, thresh_max=thresh_max)\n",
    "\n",
    "\n",
    "    NU_labels = label(nuclei_object)\n",
    "    ###################\n",
    "    # POST_PROCESSING\n",
    "    ###################\n",
    "    hole_width = 5  \n",
    "    # # wrapper to remoce_small_objects\n",
    "    #nuclei_object = remove_small_holes(nuclei_object, hole_width ** 2 )\n",
    "    nuclei_object = hole_filling(nuclei_object, hole_min=0, hole_max=hole_width**2, fill_2d=True)\n",
    "    nuclei_object = apply_mask(nuclei_object, soma_mask)\n",
    "\n",
    "    small_object_width = 15\n",
    "    nuclei_object = size_filter_2D(nuclei_object, \n",
    "                                                                min_size= small_object_width**2, \n",
    "                                                                connectivity=1)\n",
    "\n",
    "    return nuclei_object\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "# TEST `_infer_nuclei`  function defined above\n",
    "\n",
    "\n",
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_NU_object =  _infer_nuclei(img_2D, SO_mask) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer.add_image(\n",
    "    _NU_object,\n",
    "    scale=scale,\n",
    "    opacity=0.3,\n",
    ")    \n",
    "\n",
    "\n",
    "viewer.add_labels(\n",
    "    label(_NU_object),\n",
    "    scale=scale,\n",
    "    opacity=0.3,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "# TEST `infer_nuclei` exported functions\n",
    "\n",
    "> the prototype `_infer_nuclei` was copied to the [`.organelles.nuclei`](../infer_subc_2d/organelles/nuclei.py) sub-module \n",
    "##\n",
    "`infer_nuclei` procedure\n",
    "\n",
    "Use the `infer_nuclei` function to infer the Nucleus and export it as an _ome.tif_ for easy reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from infer_subc_2d.organelles.nuclei import infer_nuclei\n",
    "\n",
    "nuclei_object =  infer_nuclei(img_2D, soma_mask) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer.add_image(\n",
    "    nuclei_object,\n",
    "    scale=scale,\n",
    "    opacity=0.3,\n",
    ")    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from napari.utils.notebook_display import nbscreenshot\n",
    "\n",
    "# viewer.dims.ndisplay = 3\n",
    "# viewer.camera.angles = (-30, 25, 120)\n",
    "nbscreenshot(viewer, canvas_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "## make function json to add to `all_functions.json`\n",
    "### infer_nuclei "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from infer_subc_2d.organelles_config.helper import add_function_spec_to_widget_json\n",
    "\n",
    "_infer_nuclei =  {\n",
    "        \"name\": \"infer nuclei\",\n",
    "        \"python::module\": \"infer_subc_2d.organelles\",\n",
    "        \"python::function\": \"infer_nuclei\",\n",
    "        \"parameters\": None\n",
    "        }\n",
    "\n",
    "add_function_spec_to_widget_json(\"infer_nuclei\",_infer_nuclei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function median_filter_slice_by_slice is already in all_functions.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_median_filter_slice_by_slice =  {\n",
    "                \"name\": \"Median Smoothing Slice by Slice\",\n",
    "                \"python::module\": \"infer_subc_2d.utils.img\",\n",
    "                \"python::function\": \"median_filter_slice_by_slice\",\n",
    "                \"parameters\": {\n",
    "                    \"size\": {\n",
    "                        \"widget_type\": \"slider\",\n",
    "                        \"data_type\": \"int\",\n",
    "                        \"min\": 1,\n",
    "                        \"max\": 20,\n",
    "                        \"increment\": 1\n",
    "                    }\n",
    "                }\n",
    "            } \n",
    "add_function_spec_to_widget_json(\"median_filter_slice_by_slice\",_median_filter_slice_by_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "    # gaussian_smoothing_sigma = 1.34\n",
    "    # gaussian_smoothing_truncate_range = 3.0\n",
    "    # nuclei = image_smoothing_gaussian_slice_by_slice(  nuclei,\n",
    "    #                                                                                             sigma=gaussian_smoothing_sigma,\n",
    "    #                                                                                             truncate_range = gaussian_smoothing_truncate_range\n",
    "    #                                                                                             )\n",
    "_image_smoothing_gaussian_slice_by_slice = {\n",
    "        \"name\": \"Gaussian Smoothing Slice by Slice\",\n",
    "        \"python::module\": \"aicssegmentation.core.pre_processing_utils\",\n",
    "        \"python::function\": \"image_smoothing_gaussian_slice_by_slice\",\n",
    "        \"parameters\": {\n",
    "            \"sigma\": {\n",
    "                \"widget_type\": \"slider\",\n",
    "                \"data_type\": \"float\",\n",
    "                \"min\": 0.8,\n",
    "                \"max\": 20,\n",
    "                \"increment\": 0.2\n",
    "            }\n",
    "        }\n",
    "        }\n",
    "\n",
    "# json.dumps({\"image_smoothing_gaussian_slice_by_slice\": _image_smoothing_gaussian_slice_by_slice} )\n",
    "add_function_spec_to_widget_json(\"image_smoothing_gaussian_slice_by_slice\",_image_smoothing_gaussian_slice_by_slice)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "    # threshold_factor = 0.9 #from cellProfiler\n",
    "    # thresh_min = .1\n",
    "    # thresh_max = 1.\n",
    "    # nuclei_object = apply_log_li_threshold(nuclei, threshold_factor=threshold_factor, thresh_min=thresh_min, thresh_max=thresh_max)\n",
    "\n",
    "# WARNING: not a good way to set to None\n",
    "_apply_log_li_threshold = {\n",
    "        \"name\": \"threshold log Li\",\n",
    "        \"python::module\": \"infer_subc_2d.utils.img\",\n",
    "        \"python::function\": \"apply_log_li_threshold\",\n",
    "        \"parameters\": {\n",
    "            \"threshold_factor\": {\n",
    "                \"widget_type\": \"slider\",\n",
    "                \"data_type\": \"float\",\n",
    "                \"min\": 0.3,\n",
    "                \"max\": 1.1,\n",
    "                \"increment\": 0.05\n",
    "            },\n",
    "            \"thresh_min\": {\n",
    "                \"widget_type\": \"slider\",\n",
    "                \"data_type\": \"float\",\n",
    "                \"min\": 0.0,\n",
    "                \"max\": 0.8,\n",
    "                \"increment\": 0.01\n",
    "            },\n",
    "            \"thresh_max\": {\n",
    "                \"widget_type\": \"slider\",\n",
    "                \"data_type\": \"float\",\n",
    "                \"min\": 0.3,\n",
    "                \"max\": 1.0,\n",
    "                \"increment\": 0.05\n",
    "            },\n",
    "        }\n",
    "        }\n",
    "\n",
    "# json.dumps({\"apply_log_li_threshold\": _apply_log_li_threshold} )\n",
    "add_function_spec_to_widget_json(\"apply_log_li_threshold\",_apply_log_li_threshold)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "    # NU_labels = label(nuclei_object)\n",
    "\n",
    "_label =  {\n",
    "        \"name\": \"label objects\",\n",
    "        \"python::module\": \"skimage.measure\",\n",
    "        \"python::function\": \"label\",\n",
    "        \"parameters\": None\n",
    "        }\n",
    "# json.dumps({\"label\":_label})\n",
    "add_function_spec_to_widget_json(\"label\",_label)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#  nulei_object = apply_mask(nuclei_object, soma_mask)\n",
    "\n",
    "_apply_mask=  {\n",
    "        \"name\": \"label objects\",\n",
    "        \"python::module\": \"infer_subc_2d.utils.img\",\n",
    "        \"python::function\": \"apply_mask\",\n",
    "        \"parameters\": None\n",
    "        }\n",
    "# json.dumps({\"apply_mask\":_apply_mask})\n",
    "add_function_spec_to_widget_json(\"apply_mask\",_apply_mask)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "    # small_object_width = 45\n",
    "    # nuclei_object = size_filter_2D(nuclei_object, \n",
    "    #                                                             min_size= small_object_width**2, \n",
    "    #                                                             connectivity=1)\n",
    "\n",
    "\n",
    "_size_filter_2D = {\n",
    "        \"name\": \"Size Filter 2D\",\n",
    "        \"python::module\": \"infer_subc_2d.utils.img\",\n",
    "        \"python::function\": \"size_filter_2D\",\n",
    "        \"parameters\": {\n",
    "            \"min_size\": {\n",
    "                \"widget_type\": \"slider\",\n",
    "                \"data_type\": \"int\",\n",
    "                \"min\": 0,\n",
    "                \"max\": 500,\n",
    "                \"increment\": 1\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "# json.dumps({  \"size_filter_2D\":  _size_filter_2D   })\n",
    "\n",
    "add_function_spec_to_widget_json(\"size_filter_2D\",_size_filter_2D)        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write workflow .json\n",
    "Now that we've added our function specs we can compose workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_infer_nuclei_dict():\n",
    "    \"\"\"\n",
    "    Procedure to infer nuclei from linearly unmixed input.\n",
    "\n",
    "    Parameters:\n",
    "    ------------\n",
    "    in_img: np.ndarray\n",
    "        a 3d image containing all the channels\n",
    "\n",
    "    soma_mask: np.ndarray\n",
    "        mask\n",
    "\n",
    "    Returns:\n",
    "    -------------\n",
    "    nuclei_object\n",
    "        mask defined extent of NU\n",
    "\n",
    "    \"\"\"\n",
    "    step_name = []\n",
    "    function_name = []\n",
    "    category =[]\n",
    "    parameter_values = []\n",
    "    parent = []\n",
    "   \n",
    "    ###################\n",
    "    # EXTRACT\n",
    "    ###################   \n",
    "    # struct_img = _raw_soma_MCZ(in_img)\n",
    "    step_name.append(\"1\")\n",
    "    function_name.append(\"extract\")\n",
    "    category.append(\"preprocessing\")\n",
    "    parameter_values.append(None)\n",
    "    parent.append(0)\n",
    "\n",
    "    ###################\n",
    "    # PRE_PROCESSING\n",
    "    ###################                         \n",
    "    # nuclei = min_max_intensity_normalization(in_img[NUC_CH].copy() )\n",
    "    step_name.append(\"2\")\n",
    "    function_name.append(\"min_max_intensity_normalization\")\n",
    "    category.append(\"preprocessing\")\n",
    "    parameter_values.append(None)\n",
    "    parent.append(1)\n",
    "\n",
    "    # size = 4   \n",
    "    # nuclei = median_filter_slice_by_slice( \n",
    "    #                                                                 nuclei,\n",
    "    #                                                                 size=size  )\n",
    "    step_name.append(\"3\")\n",
    "    function_name.append(\"median_filter_slice_by_slice\")\n",
    "    category.append(\"preprocessing\")\n",
    "    parameter_values.append(dict(size = 4 ))\n",
    "    parent.append(2)\n",
    "\n",
    "    # sigma = 1.34\n",
    "    # truncate_range = 3.0\n",
    "    # nuclei = image_smoothing_gaussian_slice_by_slice(  nuclei,\n",
    "    #                                                                                             sigma=sigma,\n",
    "    #                                                                                             truncate_range = truncate_range\n",
    "    #                                                                                             )\n",
    "    step_name.append(\"4\")\n",
    "    function_name.append(\"image_smoothing_gaussian_slice_by_slice\")\n",
    "    category.append(\"preprocessing\")\n",
    "    parameter_values.append(dict( sigma = 1.34 ))\n",
    "    parent.append(3)\n",
    "\n",
    "    ###################\n",
    "    # CORE_PROCESSING\n",
    "    ###################\n",
    "    # threshold_factor = 0.9 \n",
    "    # thresh_min = .1\n",
    "    # thresh_max = 1.\n",
    "    # nuclei_object = apply_log_li_threshold(nuclei, threshold_factor=threshold_factor, thresh_min=thresh_min, thresh_max=thresh_max)\n",
    "    step_name.append(\"5\")\n",
    "    function_name.append(\"apply_log_li_threshold\")\n",
    "    category.append(\"core\")\n",
    "    parameter_values.append(dict(threshold_factor = 0.9, \n",
    "                                                            thresh_min = .1,\n",
    "                                                            thresh_max = 1.))\n",
    "    parent.append(4)\n",
    "\n",
    "\n",
    "    # NU_labels = label(nuclei_object)\n",
    "    step_name.append(\"6\")\n",
    "    function_name.append(\"label\")\n",
    "    category.append(\"core\")\n",
    "    parameter_values.append(None)\n",
    "    parent.append(5)\n",
    "\n",
    "\n",
    "    ###################\n",
    "    # POST_PROCESSING\n",
    "    ###################\n",
    "    # hole_width = 5  \n",
    "    # nuclei_object = hole_filling(nuclei_object, hole_min=0, hole_max=hole_width**2, fill_2d=True)\n",
    "    step_name.append(\"6\")\n",
    "    function_name.append(\"hole_filling\")\n",
    "    category.append(\"postprocessing\")\n",
    "    parameter_values.append(dict( hole_min=0, hole_max=5**2, fill_2d=True))\n",
    "    parent.append(5)\n",
    "\n",
    "    # # EEEEEK I don't know how to compose where the mask comes from... \n",
    "    # nuclei_object = apply_mask(nuclei_object, soma_mask)\n",
    "\n",
    "    # small_object_width = 15\n",
    "    # nuclei_object = size_filter_2D(nuclei_object, \n",
    "    #                                                             min_size= small_object_width**2, \n",
    "    #                                                             connectivity=1)\n",
    "    step_name.append(\"7\")\n",
    "    function_name.append(\"size_filter_2D\")\n",
    "    category.append(\"postprocessing\")\n",
    "    parameter_values.append(dict( min_size = 15**2  ))\n",
    "    parent.append(6)\n",
    "\n",
    "    out_dict = dict()\n",
    "    for i,stepn in enumerate(step_name):\n",
    "        entry = dict(category=category[i],\n",
    "                            function=function_name[i],\n",
    "                            parameter_values=parameter_values[i],\n",
    "                            parent=parent[i]\n",
    "        )\n",
    "        if entry['parameter_values'] is None:\n",
    "            _ = entry.pop('parameter_values')\n",
    "        out_dict[stepn] = entry\n",
    "        \n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _write_workflow_json(wf_name, wf_dict):\n",
    "\n",
    "    # read all_functions.json into dict\n",
    "    # if not wf_name.startswith(\"conf\"):\n",
    "    #     wf_name = f\"conf_{wf_name}\"\n",
    "    path = Directories.get_structure_config_dir() / f\"{wf_name}.json\"\n",
    "\n",
    "    # re-write file\n",
    "    with open(path, \"w\") as file:\n",
    "        json.dump(wf_dict, file, indent=4, sort_keys=False)\n",
    "\n",
    "    return path\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/ahenrie/Projects/Imaging/infer-subc-2D/infer_subc_2d/organelles_config/infer_nuclei.json')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from infer_subc_2d.organelles_config.helper import write_workflow_json\n",
    "\n",
    "infer_nuclei_dict = make_infer_nuclei_dict()\n",
    "\n",
    "_write_workflow_json(\"infer_nuclei\", infer_nuclei_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, 'conf_nuclei')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wf_name = \"conf_nuclei\"\n",
    "not wf_name.startswith(\"conf\"), wf_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'small_object_width': 2025}, {'small_object_width': 2025})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entry['parameter_values'],entry.pop('parameter_values')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'category': 'postprocessing', 'function': 'size_filter_2D', 'parent': 6}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:  make export ome_tiff export:   XX_object, XX_label, XX_signal\n",
    "#              also fix Path vs. str action for export wrapper\n",
    "\n",
    "chan_name = 'nuclei'\n",
    "out_path = data_root_path / \"inferred_objects\" \n",
    "object_name = 'NU_object'\n",
    "\n",
    "nuclei_object_filen = export_ome_tiff(nuclei_object, meta_dict, object_name, str(out_path)+\"/\", curr_chan=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuclei_object_filen\n",
    "nuclei_object_filen = export_ndarray(nuclei_object,  object_name, str(out_path)+\"/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NU_object_filen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d6148ef1fb015fb20f0b6da2ea61c87c6b848bdf3dabb03087e5d5cd0c4607e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
