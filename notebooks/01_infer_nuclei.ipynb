{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer NUCLEI - part 1\n",
    "\n",
    "--------------\n",
    "\n",
    "## OBJECTIVE:  Infer sub-cellular component #1: NUCLEI  in order to understand interactome \n",
    "\n",
    "To measure shape, position, size, and interaction of eight organelles/cellular components Nuclei (NU).  \n",
    "\n",
    "Dependencies:\n",
    "SOMA and CYTOSOL inference rely on the Nuclei inference.  Therefore all of the sub-cellular objects rely on the NU segmentation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this needs to be organzied to explain the imports\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import scipy\n",
    "from scipy import ndimage as ndi\n",
    "\n",
    "import napari\n",
    "\n",
    "# function for core algorithm\n",
    "import aicssegmentation\n",
    "\n",
    "from aicssegmentation.core.seg_dot import dot_3d_wrapper, dot_slice_by_slice, dot_2d_slice_by_slice_wrapper, dot_3d\n",
    "from aicssegmentation.core.pre_processing_utils import ( intensity_normalization, \n",
    "                                                         image_smoothing_gaussian_3d,  \n",
    "                                                         image_smoothing_gaussian_slice_by_slice )\n",
    "from aicssegmentation.core.utils import topology_preserving_thinning\n",
    "from aicssegmentation.core.MO_threshold import MO\n",
    "\n",
    "from aicssegmentation.core.utils import hole_filling\n",
    "from aicssegmentation.core.vessel import filament_2d_wrapper, vesselnessSliceBySlice\n",
    "\n",
    "from aicssegmentation.core.output_utils import   save_segmentation,  generate_segmentation_contour\n",
    "                                                 \n",
    "from skimage import filters\n",
    "from skimage import morphology\n",
    "\n",
    "from skimage.segmentation import watershed\n",
    "from skimage.feature import peak_local_max\n",
    "from skimage.morphology import remove_small_objects, binary_closing, ball , dilation   # function for post-processing (size filter)\n",
    "from skimage.measure import label\n",
    "\n",
    "# # package for io \n",
    "# from aicsimageio import AICSImage\n",
    "\n",
    "from napari.utils.notebook_display import nbscreenshot\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "if sys.platform == \"win32\":\n",
    "    sys.path.append(os.path.abspath((os.path.join(os.getcwd(), '..'))))\n",
    "    from infer_subc.base import *\n",
    "else:\n",
    "    from infer_subc.base import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OBJECTIVE :  infer NUCLEI\n",
    " \n",
    "\n",
    "NOTE:  using Allen Cell Segmenter  [Nucleophosmin](https://www.allencell.org/cell-observations/category/nucleophosmin) might be a good generic mechanism.  e.g.\n",
    "-  [playground_npm1.ipynb](/Users/ahenrie/Projects/Imaging/mcz_subcell/napari/aics-segmentation/lookup_table_demo/playground_npm1.ipynb) and [npm1.py](/Users/ahenrie/Projects/Imaging/mcz_subcell/napari/aics-segmentation/aicssegmentation/structure_wrapper/seg_npm1.py) and [npm1_SR.py](/Users/ahenrie/Projects/Imaging/mcz_subcell/napari/aics-segmentation/aicssegmentation/structure_wrapper/seg_npm1_SR.py) \n",
    "\n",
    "\n",
    "## summary of steps\n",
    "\n",
    "\n",
    "INPUT\n",
    "- channel 0 from raw data :: raw_nuclei\n",
    "\n",
    "PRE-PROCESSING\n",
    "-  scale to min 0, max 1.0\n",
    "- median Filter window 4\n",
    "\n",
    "CORE-PROCESSING\n",
    "  - threshold method minimum cross-entropy.  (Li's method)\n",
    "    - objects size: 50-400 pixels, \n",
    "    - threshold smoothing scale: 1 pixel\n",
    "    - threshold correction factor: 1.2 (more stringent)\n",
    "    - lower / upper bounds  (.1,1)\n",
    "    - log transformed thresholding\n",
    "    - fill holes\n",
    "\n",
    "\n",
    "POST-PROCESSING\n",
    "  - fill holes\n",
    "  - remove small objects\n",
    "\n",
    "\n",
    "OUTPUT\n",
    "- inferred of NUCLEI :: inferer_NU an image of labels export as ome.tiff\n",
    "\n",
    "\n",
    "> #### Note:  this initial inferred object -- the Nuclei of the brightest cell -- will be used in inferring the Soma and Cytosol objects.   This is a straightforward procedure, but also note that any inconsistencies will flow into the Soma and Cytosol objects which in turn affect ALL inferred objects.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ahenrie/Projects/Imaging/mcz_subcell/data/raw/ZSTACK_PBTOhNGN2hiPSCs_BR3_N04_Unmixed.czi\n"
     ]
    }
   ],
   "source": [
    "# assumptions:   .czi \"unmixed\" collections.\n",
    "#czi_img_folder = f\"{os.getenv('HOME')}/Projects/Imaging/mcz_subcell/data\"\n",
    "\n",
    "data_path = Path( f\"{os.getenv('HOME')}/Projects/Imaging/mcz_subcell/data\")\n",
    "czi_img_folder = data_path/\"raw\"\n",
    "\n",
    "list_img_files = lambda img_folder,f_type: [os.path.join(img_folder,f_name) for f_name in os.listdir(img_folder) if f_name.endswith(f_type)]\n",
    "\n",
    "img_file_list = list_img_files(czi_img_folder,'.czi')\n",
    "print(img_file_list[5])\n",
    "#Read the data into memeory from the `.czi` files.  (Note: there is also the 2D slice .tif file read for later comparision).  WE will also collect metatdata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/napariNEW/lib/python3.9/site-packages/ome_types/_convenience.py:105: FutureWarning: The default XML parser will be changing from 'xmlschema' to 'lxml' in version 0.4.0.  To silence this warning, please provide the `parser` argument, specifying either 'lxml' (to opt into the new behavior), or'xmlschema' (to retain the old behavior).\n",
      "  d = to_dict(os.fspath(xml), parser=parser, validate=validate)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_img_name = img_file_list[5]\n",
    "\n",
    "img_data, meta_dict = read_input_image(test_img_name)\n",
    "\n",
    "raw_meta_data, ome_types = get_raw_meta_data(meta_dict)\n",
    "\n",
    "# get some top-level info about the RAW data\n",
    "channel_names = meta_dict['name']\n",
    "img = meta_dict['metadata']['aicsimage']\n",
    "scale = meta_dict['scale']\n",
    "channel_axis = meta_dict['channel_axis']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "viewer = napari.view_image(\n",
    "    img_data,\n",
    "    channel_axis=0,\n",
    "    name=channel_names,\n",
    "    scale=scale\n",
    ")\n",
    "viewer.scale_bar.visible = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# IMAGE PROCESSING Objective 1:  infer NUCLEI\n",
    " \n",
    " [OUTLINE: Objective #1](#summary-of-objectives)\n",
    "## summary of steps\n",
    "\n",
    "\n",
    "INPUT\n",
    "- channel 0\n",
    "\n",
    "\n",
    "PRE-PROCESSING\n",
    "-  scale to min 0, max 1.0\n",
    "- median Filter window 4\n",
    "-  gaussian 1.34\n",
    "\n",
    "CORE-PROCESSING\n",
    "  - threshold method minimum cross-entropy.  \n",
    "    - objects 50-400 pixels, \n",
    "    - threshold smoothing scale: 1.34 (later 1 pixel\n",
    "    - threshold correction factor: 0.9 (later 1.2 )\n",
    "    - lower / upper bounds  (.1,1) ?\n",
    "    - log transformed thresholding\n",
    "    - fill holes\n",
    "\n",
    "\n",
    "POST-PROCESSING\n",
    "  - fill holes\n",
    "  - remove small objects\n",
    "\n",
    "\n",
    "OUTPUT\n",
    "- labels of NUCLEI\n",
    "\n",
    "\n",
    "> #### Note:  in later steps we will limit each analysis to a single object, but at this stage we have multiple\n",
    "\n",
    "\n",
    "\n",
    "NOTE:  using Allen Cell Segmenter  [Nucleophosmin](https://www.allencell.org/cell-observations/category/nucleophosmin) might e a good place to start.  [playground_npm1.ipynb](/Users/ahenrie/Projects/Imaging/mcz_subcell/napari/aics-segmentation/lookup_table_demo/playground_npm1.ipynb) and [npm1.py](/Users/ahenrie/Projects/Imaging/mcz_subcell/napari/aics-segmentation/aicssegmentation/structure_wrapper/seg_npm1.py) and [npm1_SR.py](/Users/ahenrie/Projects/Imaging/mcz_subcell/napari/aics-segmentation/aicssegmentation/structure_wrapper/seg_npm1_SR.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "chan_name = 'nuclei'\n",
    "out_path = data_path / \"inferred_objects\" \n",
    "object_name = 'NU_object'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median filtering scale is ~ : [0.5804527163320905, 0.3194866073934927, 0.3194866073934927]\n"
     ]
    }
   ],
   "source": [
    "##########################################################################\n",
    "# DEFAULT PARAMETERS:\n",
    "#   note that these parameters are supposed to be fixed for the structure\n",
    "#   and work well accross different datasets\n",
    "from collections import defaultdict\n",
    "# default_params = defaultdict(str)\n",
    "\n",
    "default_params = defaultdict(str, **{\n",
    "    #\"intensity_norm_param\" : [0.5, 15]\n",
    "    \"intensity_norm_param\" : [0],\n",
    "    \"gaussian_smoothing_sigma\" : 1.34,\n",
    "    \"gaussian_smoothing_truncate_range\" : 3.0,\n",
    "    \"dot_2d_sigma\" : 2,\n",
    "    \"dot_2d_sigma_extra\" : 1,\n",
    "    \"dot_2d_cutoff\" : 0.025,\n",
    "    \"min_area\" : 10,\n",
    "    \"low_level_min_size\" :  100,\n",
    "    \"median_filter_size\" : 4\n",
    "})\n",
    "\n",
    "\n",
    "################################\n",
    "\n",
    "# calculate a filter dimension for median filtering which considers the difference in scale of Z\n",
    "z_factor = scale[0]//scale[1]\n",
    "med_filter_size = 4 #2D \n",
    "med_filter_size_3D = (1,med_filter_size,med_filter_size)  # set the scale for a typical median filter\n",
    "print(f\"median filtering scale is ~ : { [x*y for x,y in zip(scale,med_filter_size_3D)]}\")\n",
    "\n",
    "default_params['z_factor'] = z_factor\n",
    "default_params['scale'] = scale\n",
    "\n",
    "\n",
    "gaussian_smoothing_sigma = 1.34\n",
    "gaussian_smoothing_truncate_range = 3.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA IMPORT\n",
    "\n",
    "Get the \"raw\" signals we need to analyze as well as any other dependencies in \"inferred\" objects.  \n",
    "\n",
    "> NOTE: we are operating on a single \"test\" image in this notebook.  The batch-processing of all the images will be happen at the end of the notebook after we have developed/confirmed the setmentation procedures and parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "###################\n",
    "# INPUT\n",
    "###################\n",
    "raw_nuclei = img_data[0,:,:,:].copy()\n",
    "# raw_lyso    = img_data[1,:,:,:].copy()\n",
    "# raw_mito    = img_data[2,:,:,:].copy()\n",
    "# raw_golgi   = img_data[3,:,:,:].copy()\n",
    "# raw_peroxi = img_data[4,:,:,:].copy()\n",
    "# raw_ER      = img_data[5,:,:,:].copy()\n",
    "# raw_lipid   = img_data[6,:,:,:].copy()\n",
    "# raw_residual = img_data[7,:,:,:].copy()\n",
    "# total_flourescence = intensity_normalization(img_data.copy(), scaling_param=[0]).sum(axis=0)\n",
    "# total_flourescence_scaled = intensity_normalization(total_flourescence, scaling_param=[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRE- PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intensity normalization: min-max normalization with NO absoluteintensity upper bound\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "###################\n",
    "# PRE_PROCESSING\n",
    "###################                         )\n",
    "struct_img = intensity_normalization(raw_nuclei.copy(), scaling_param=[0])\n",
    "\n",
    "# structure_img_median_3D = ndi.median_filter(struct_img,    size=med_filter_size  )\n",
    "# # very little difference in 2D vs 3D\n",
    "structure_img_median = median_filter_slice_by_slice( \n",
    "                                                                struct_img,\n",
    "                                                                size=med_filter_size  )\n",
    "\n",
    "\n",
    "structure_img_smooth = image_smoothing_gaussian_slice_by_slice(   structure_img_median,\n",
    "                                                                                                                        sigma=gaussian_smoothing_sigma,\n",
    "                                                                                                                        truncate_range=gaussian_smoothing_truncate_range,\n",
    "                                                                                                                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> #### NOTE: Thresholding\n",
    "> [Thresholding](https://en.wikipedia.org/wiki/Thresholding_%28image_processing%29) is used to create binary images. A threshold value determines the intensity value separating foreground pixels from background pixels. Foregound pixels are pixels brighter than the threshold value, background pixels are darker. In many cases, images can be adequately segmented by thresholding followed by labelling of *connected components*, which is a fancy way of saying \"groups of pixels that touch each other\".\n",
    "> \n",
    "> Different thresholding algorithms produce different results. [Otsu's method](https://en.wikipedia.org/wiki/Otsu%27s_method) and [Li's minimum cross entropy threshold](https://scikit-image.org/docs/dev/auto_examples/developers/plot_threshold_li.html) are two common algorithms. Below, we use Li. You can use `skimage.filters.threshold_<TAB>` to find different thresholding methods.\n",
    "\n",
    "_Li_ procedure  better matches the CellProfiler pipeline which simply calls it \"Minimum Cross Entropy\" .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORE PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "###################\n",
    "# CORE_PROCESSING\n",
    "###################\n",
    "\n",
    "#tol = max(numpy.min(numpy.diff(numpy.unique(structure_img_median))) / 2, 0.5 / 65536) #assumes 16bit?\n",
    "#threshold_value = filters.threshold_li(structure_img_smooth)\n",
    "\n",
    "threshold_value_log = threshold_li_log(structure_img_smooth)\n",
    "\n",
    "threshold_factor = 0.9 #from cellProfiler\n",
    "thresh_min = 0.1\n",
    "thresh_max = 1.0\n",
    "threshold = min( max(threshold_value_log*threshold_factor, thresh_min), thresh_max)\n",
    "\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "li_thresholded = structure_img_smooth > threshold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POST PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# POST_PROCESSING\n",
    "###################\n",
    "\n",
    "hole_width = 5  \n",
    "# # wrapper to remoce_small_objects\n",
    "removed_holes = morphology.remove_small_holes(li_thresholded, hole_width ** 3 )\n",
    "\n",
    "\n",
    "small_object_max = 5\n",
    "cleaned_img = aicssegmentation.core.utils.size_filter(removed_holes, # wrapper to remove_small_objects which can do slice by slice\n",
    "                                                         min_size= small_object_max**3, \n",
    "                                                         method = \"slice_by_slice\", #\"3D\", # \n",
    "                                                         connectivity=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUTPUT + Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "NU_object = cleaned_img\n",
    "NU_labels = label(cleaned_img   )\n",
    "NU_signal = struct_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'NU_labels [2]' at 0x1749d2eb0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viewer.add_image(\n",
    "    NU_object,\n",
    "    scale=scale,\n",
    "    opacity=0.3,\n",
    ")    \n",
    "\n",
    "\n",
    "viewer.add_labels(\n",
    "    NU_labels,\n",
    "    scale=scale,\n",
    "    opacity=0.3,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINE `infer_NUCLEI` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy this to base.py for easy import\n",
    "\n",
    "def infer_NUCLEI(struct_img, in_params) -> tuple:\n",
    "    \"\"\"\n",
    "    Procedure to infer NUCLEI from linearly unmixed input.\n",
    "\n",
    "    Parameters:\n",
    "    ------------\n",
    "    struct_img: np.ndarray\n",
    "        a 3d image containing the NUCLEI signal\n",
    "\n",
    "    in_params: dict\n",
    "        holds the needed parameters\n",
    "\n",
    "    Returns:\n",
    "    -------------\n",
    "    tuple of:\n",
    "        object\n",
    "            mask defined boundaries of NU\n",
    "        label\n",
    "            label (could be more than 1)\n",
    "        signal\n",
    "            scaled/filtered (pre-processed) flourescence image\n",
    "        parameters: dict\n",
    "            updated parameters in case any needed were missing\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    out_p= in_params.copy()\n",
    "\n",
    "\n",
    "    ###################\n",
    "    # PRE_PROCESSING\n",
    "    ###################                         \n",
    "\n",
    "    #TODO: replace params below with the input params\n",
    "    scaling_param =  [0]   \n",
    "    struct_img = intensity_normalization(struct_img, scaling_param=scaling_param)\n",
    "    out_p[\"intensity_norm_param\"] = scaling_param\n",
    "\n",
    "    med_filter_size = 4   \n",
    "    # structure_img_median_3D = ndi.median_filter(struct_img,    size=med_filter_size  )\n",
    "    struct_img = median_filter_slice_by_slice( \n",
    "                                                                    struct_img,\n",
    "                                                                    size=med_filter_size  )\n",
    "    out_p[\"median_filter_size\"] = med_filter_size \n",
    "\n",
    "    gaussian_smoothing_sigma = 1.34\n",
    "    gaussian_smoothing_truncate_range = 3.0\n",
    "    struct_img = image_smoothing_gaussian_slice_by_slice(   struct_img,\n",
    "                                                                                                        sigma=gaussian_smoothing_sigma,\n",
    "                                                                                                        truncate_range = gaussian_smoothing_truncate_range\n",
    "                                                                                                    )\n",
    "    out_p[\"gaussian_smoothing_sigma\"] = gaussian_smoothing_sigma \n",
    "    out_p[\"gaussian_smoothing_truncate_range\"] = gaussian_smoothing_truncate_range\n",
    "\n",
    "    ###################\n",
    "    # CORE_PROCESSING\n",
    "    ###################\n",
    "\n",
    "    struct_obj = struct_img > filters.threshold_li(struct_img)\n",
    "    threshold_value_log = threshold_li_log(struct_img)\n",
    "\n",
    "    threshold_factor = 0.9 #from cellProfiler\n",
    "    thresh_min = .1\n",
    "    thresh_max = 1.\n",
    "    threshold = min( max(threshold_value_log*threshold_factor, thresh_min), thresh_max)\n",
    "    out_p['threshold_factor'] = threshold_factor\n",
    "    out_p['thresh_min'] = thresh_min\n",
    "    out_p['thresh_max'] = thresh_max\n",
    "\n",
    "    struct_obj = struct_img > threshold\n",
    "\n",
    "    ###################\n",
    "    # POST_PROCESSING\n",
    "    ###################\n",
    "\n",
    "    hole_width = 5  \n",
    "    # # wrapper to remoce_small_objects\n",
    "    struct_obj = morphology.remove_small_holes(struct_obj, hole_width ** 3 )\n",
    "    out_p['hole_width'] = hole_width\n",
    "\n",
    "\n",
    "    small_object_max = 5\n",
    "    struct_obj = aicssegmentation.core.utils.size_filter(struct_obj, # wrapper to remove_small_objects which can do slice by slice\n",
    "                                                            min_size= small_object_max**3, \n",
    "                                                         method = \"slice_by_slice\", #\"3D\", # \n",
    "                                                            connectivity=1)\n",
    "    out_p['small_object_max'] = small_object_max\n",
    "\n",
    "\n",
    "    retval = (struct_obj,  label(struct_obj), out_p)\n",
    "    return retval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intensity normalization: min-max normalization with NO absoluteintensity upper bound\n"
     ]
    }
   ],
   "source": [
    "# test - 9.7 secods to run...\n",
    "\n",
    "NU_object, NU_label, out_p =  infer_NUCLEI(raw_nuclei.copy(), default_params) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NU_object']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/napariNEW/lib/python3.9/site-packages/ome_types/_convenience.py:105: FutureWarning: The default XML parser will be changing from 'xmlschema' to 'lxml' in version 0.4.0.  To silence this warning, please provide the `parser` argument, specifying either 'lxml' (to opt into the new behavior), or'xmlschema' (to retain the old behavior).\n",
      "  d = to_dict(os.fspath(xml), parser=parser, validate=validate)\n"
     ]
    }
   ],
   "source": [
    "# TODO:  make export ome_tiff export:   XX_object, XX_label, XX_signal\n",
    "#              also fix Path vs. str action for export wrapper\n",
    "\n",
    "chan_name = 'nuclei'\n",
    "out_path = data_path / \"inferred_objects\" \n",
    "object_name = 'NU_object'\n",
    "\n",
    "NU_object_filen = export_ome_tiff(NU_object, meta_dict, object_name, str(out_path)+\"/\", curr_chan=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ahenrie/Projects/Imaging/mcz_subcell/data/inferred_objects/NU_object.ome.tiff'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NU_object_filen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(str,\n",
       "            {'intensity_norm_param': [0],\n",
       "             'gaussian_smoothing_sigma': 1.34,\n",
       "             'gaussian_smoothing_truncate_range': 3.0,\n",
       "             'dot_2d_sigma': 2,\n",
       "             'dot_2d_sigma_extra': 1,\n",
       "             'dot_2d_cutoff': 0.025,\n",
       "             'min_area': 10,\n",
       "             'low_level_min_size': 100,\n",
       "             'median_filter_size': 4,\n",
       "             'z_factor': 7.0,\n",
       "             'scale': (0.5804527163320905,\n",
       "              0.07987165184837318,\n",
       "              0.07987165184837318),\n",
       "             'threshold_factor': 0.9,\n",
       "             'thresh_min': 0.1,\n",
       "             'thresh_max': 1.0,\n",
       "             'hole_width': 5,\n",
       "             'small_object_max': 5})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('napariNEW')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d6148ef1fb015fb20f0b6da2ea61c87c6b848bdf3dabb03087e5d5cd0c4607e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
