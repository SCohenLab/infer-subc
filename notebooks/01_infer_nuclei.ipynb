{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer NUCLEI - 1️⃣\n",
    "\n",
    "--------------\n",
    "\n",
    "## OBJECTIVE: ✅ Infer sub-cellular component #1: NUCLEI  in order to understand interactome \n",
    "\n",
    "To measure shape, position, size, and interaction of eight organelles/cellular components Nuclei (NU).  \n",
    "\n",
    "Dependencies:\n",
    "SOMA and CYTOSOL inference rely on the Nuclei inference.  Therefore all of the sub-cellular objects rely on the NU segmentation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (nuclei.py, line 40)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m/opt/anaconda3/envs/napariNEW/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3398\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  Input \u001b[1;32mIn [1]\u001b[0m in \u001b[1;35m<cell line: 43>\u001b[0m\n    from infer_subc.organelles.nuclei import infer_NUCLEI\n",
      "\u001b[0;36m  File \u001b[0;32m~/Projects/Imaging/infer-subc/infer_subc/organelles/__init__.py:1\u001b[0;36m in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from .nuclei import infer_NUCLEI\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m~/Projects/Imaging/infer-subc/infer_subc/organelles/nuclei.py:40\u001b[0;36m\u001b[0m\n\u001b[0;31m    def __init__(self, priors: ObjectStats)\u001b[0m\n\u001b[0m                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# top level imports\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "# # function for core algorithm\n",
    "from scipy import ndimage as ndi\n",
    "import aicssegmentation\n",
    "from aicssegmentation.core.seg_dot import dot_3d_wrapper, dot_slice_by_slice, dot_2d_slice_by_slice_wrapper, dot_3d\n",
    "from aicssegmentation.core.pre_processing_utils import ( intensity_normalization, \n",
    "                                                         image_smoothing_gaussian_3d,  \n",
    "                                                         image_smoothing_gaussian_slice_by_slice )\n",
    "from aicssegmentation.core.utils import topology_preserving_thinning\n",
    "from aicssegmentation.core.MO_threshold import MO\n",
    "from aicssegmentation.core.utils import hole_filling\n",
    "from aicssegmentation.core.vessel import filament_2d_wrapper, vesselnessSliceBySlice\n",
    "from aicssegmentation.core.output_utils import   save_segmentation,  generate_segmentation_contour\n",
    "                                                 \n",
    "from skimage import filters\n",
    "from skimage import morphology\n",
    "from skimage.segmentation import watershed\n",
    "from skimage.feature import peak_local_max\n",
    "from skimage.morphology import remove_small_objects, binary_closing, ball , dilation   # function for post-processing (size filter)\n",
    "from skimage.measure import label\n",
    "\n",
    "# # package for io \n",
    "from aicsimageio import AICSImage\n",
    "\n",
    "import napari\n",
    "\n",
    "### import local python functions in ../infer_subc\n",
    "sys.path.append(os.path.abspath((os.path.join(os.getcwd(), '..'))))\n",
    "\n",
    "from infer_subc.utils.file_io import read_input_image, list_image_files, export_ome_tiff\n",
    "from infer_subc.utils.img import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from infer_subc.organelles.nuclei import infer_NUCLEI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMAGE PROCESSING  OBJECTIVE :  infer NUCLEI\n",
    " \n",
    "\n",
    "NOTE:  using Allen Cell Segmenter  [Nucleophosmin](https://www.allencell.org/cell-observations/category/nucleophosmin) might be a good generic mechanism.  e.g.\n",
    "-  [playground_npm1.ipynb](https://github.com/AllenInstitute/aics-segmentation/blob/master/lookup_table_demo/playground_npm1.ipynb) and [npm1.py](https://github.com/AllenInstitute/aics-segmentation/blob/master/aicssegmentation/structure_wrapper/seg_npm1.py) and [npm1_SR.py](https://github.com/AllenInstitute/aics-segmentation/blob/master/aicssegmentation/structure_wrapper/seg_npm1_SR.py)\n",
    "\n",
    "\n",
    "> #### Note:  this initial inferred object -- the Nuclei of the brightest cell -- will be used in inferring the Soma and Cytosol objects.   This is a straightforward procedure, but also note that any inconsistencies will flow into the Soma and Cytosol objects which in turn affect ALL inferred objects.\n",
    "\n",
    "\n",
    "------------------------\n",
    "# LOAD RAW IMAGE DATA\n",
    "Identify path to _raw_ image data and load our example image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the datapath\n",
    "# all the imaging data goes here.\n",
    "# CUSTOMIZE HERE --->\n",
    "data_root_path = Path(os.path.expanduser(\"~\")) / \"Projects/Imaging/data\"\n",
    "\n",
    "# linearly unmixed \".czi\" files are here\n",
    "data_path = data_root_path / \"raw\"\n",
    "im_type = \".czi\"\n",
    "\n",
    "# get the list of all files\n",
    "img_file_list = list_image_files(data_path,im_type)\n",
    "test_img_name = img_file_list[5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/ahenrie')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(os.path.expanduser(\"~\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/napariNEW/lib/python3.9/site-packages/ome_types/_convenience.py:105: FutureWarning: The default XML parser will be changing from 'xmlschema' to 'lxml' in version 0.4.0.  To silence this warning, please provide the `parser` argument, specifying either 'lxml' (to opt into the new behavior), or'xmlschema' (to retain the old behavior).\n",
      "  d = to_dict(os.fspath(xml), parser=parser, validate=validate)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bioim_image = read_input_image(test_img_name)\n",
    "img_data = bioim_image.image\n",
    "raw_meta_data = bioim_image.raw_meta\n",
    "ome_types = []\n",
    "meta_dict = bioim_image.meta\n",
    "\n",
    "\n",
    "# get some top-level info about the RAW data\n",
    "channel_names = meta_dict['name']\n",
    "img = meta_dict['metadata']['aicsimage']\n",
    "scale = meta_dict['scale']\n",
    "channel_axis = meta_dict['channel_axis']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant skips harvesting pyclesperanto as it's not installed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "viewer = napari.view_image(\n",
    "    img_data,\n",
    "    channel_axis=0,\n",
    "    name=channel_names,\n",
    "    scale=scale\n",
    ")\n",
    "viewer.scale_bar.visible = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# IMAGE PROCESSING Objective 1:  infer NUCLEI\n",
    " \n",
    "## details\n",
    "\n",
    "➡️ INPUT\n",
    "\n",
    "- channel 0\n",
    "\n",
    "PRE-PROCESSING\n",
    "-  scale to min 0, max 1.0\n",
    "- median Filter window 4\n",
    "-  gaussian 1.34\n",
    "\n",
    "CORE-PROCESSING\n",
    "  - threshold method minimum cross-entropy.  \n",
    "    - objects 50-400 pixels, \n",
    "    - threshold smoothing scale: 1.34 (later 1 pixel\n",
    "    - threshold correction factor: 0.9 (later 1.2 )\n",
    "    - lower / upper bounds  (.1,1) ?\n",
    "    - log transformed thresholding\n",
    "    - fill holes\n",
    "\n",
    "\n",
    "POST-PROCESSING\n",
    "  - fill holes\n",
    "  - remove small objects\n",
    "\n",
    "\n",
    "OUTPUT ➡️ \n",
    "- labels of NUCLEI\n",
    "\n",
    "\n",
    "> #### Note:  in later steps we will limit each analysis to a single object, but at this stage we have multiple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "chan_name = 'nuclei'\n",
    "out_path = data_path / \"inferred_objects\" \n",
    "object_name = 'NU_object'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median filtering scale is ~ : [0.5804527163320905, 0.3194866073934927, 0.3194866073934927]\n"
     ]
    }
   ],
   "source": [
    "##########################################################################\n",
    "# DEFAULT PARAMETERS:\n",
    "#   note that these parameters are supposed to be fixed for the structure\n",
    "#   and work well accross different datasets\n",
    "from collections import defaultdict\n",
    "# default_params = defaultdict(str)\n",
    "\n",
    "default_params = defaultdict(str, **{\n",
    "    #\"intensity_norm_param\" : [0.5, 15]\n",
    "    \"intensity_norm_param\" : [0],\n",
    "    \"gaussian_smoothing_sigma\" : 1.34,\n",
    "    \"gaussian_smoothing_truncate_range\" : 3.0,\n",
    "    \"dot_2d_sigma\" : 2,\n",
    "    \"dot_2d_sigma_extra\" : 1,\n",
    "    \"dot_2d_cutoff\" : 0.025,\n",
    "    \"min_area\" : 10,\n",
    "    \"low_level_min_size\" :  100,\n",
    "    \"median_filter_size\" : 4\n",
    "})\n",
    "\n",
    "\n",
    "################################\n",
    "\n",
    "# calculate a filter dimension for median filtering which considers the difference in scale of Z\n",
    "z_factor = scale[0]//scale[1]\n",
    "med_filter_size = 4 #2D \n",
    "med_filter_size_3D = (1,med_filter_size,med_filter_size)  # set the scale for a typical median filter\n",
    "print(f\"median filtering scale is ~ : { [x*y for x,y in zip(scale,med_filter_size_3D)]}\")\n",
    "\n",
    "default_params['z_factor'] = z_factor\n",
    "default_params['scale'] = scale\n",
    "\n",
    "\n",
    "gaussian_smoothing_sigma = 1.34\n",
    "gaussian_smoothing_truncate_range = 3.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA IMPORT\n",
    "\n",
    "Get the \"raw\" signals we need to analyze as well as any other dependencies in \"inferred\" objects.  \n",
    "\n",
    "> NOTE: we are operating on a single \"test\" image in this notebook.  The batch-processing of all the images will be happen at the end of the notebook after we have developed/confirmed the setmentation procedures and parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "###################\n",
    "# INPUT\n",
    "###################\n",
    "raw_nuclei = img_data[0,:,:,:].copy()\n",
    "# raw_lyso    = img_data[1,:,:,:].copy()\n",
    "# raw_mito    = img_data[2,:,:,:].copy()\n",
    "# raw_golgi   = img_data[3,:,:,:].copy()\n",
    "# raw_peroxi = img_data[4,:,:,:].copy()\n",
    "# raw_ER      = img_data[5,:,:,:].copy()\n",
    "# raw_lipid   = img_data[6,:,:,:].copy()\n",
    "# raw_residual = img_data[7,:,:,:].copy()\n",
    "# total_flourescence = intensity_normalization(img_data.copy(), scaling_param=[0]).sum(axis=0)\n",
    "# total_flourescence_scaled = intensity_normalization(total_flourescence, scaling_param=[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRE- PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intensity normalization: min-max normalization with NO absoluteintensity upper bound\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "###################\n",
    "# PRE_PROCESSING\n",
    "###################                         )\n",
    "struct_img = intensity_normalization(raw_nuclei.copy(), scaling_param=[0])\n",
    "\n",
    "# structure_img_median_3D = ndi.median_filter(struct_img,    size=med_filter_size  )\n",
    "# # very little difference in 2D vs 3D\n",
    "structure_img_median = median_filter_slice_by_slice( \n",
    "                                                                struct_img,\n",
    "                                                                size=med_filter_size  )\n",
    "\n",
    "\n",
    "structure_img_smooth = image_smoothing_gaussian_slice_by_slice(   structure_img_median,\n",
    "                                                                                                                        sigma=gaussian_smoothing_sigma,\n",
    "                                                                                                                        truncate_range=gaussian_smoothing_truncate_range,\n",
    "                                                                                                                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> #### NOTE: Thresholding\n",
    "> [Thresholding](https://en.wikipedia.org/wiki/Thresholding_%28image_processing%29) is used to create binary images. A threshold value determines the intensity value separating foreground pixels from background pixels. Foregound pixels are pixels brighter than the threshold value, background pixels are darker. In many cases, images can be adequately segmented by thresholding followed by labelling of *connected components*, which is a fancy way of saying \"groups of pixels that touch each other\".\n",
    "> \n",
    "> Different thresholding algorithms produce different results. [Otsu's method](https://en.wikipedia.org/wiki/Otsu%27s_method) and [Li's minimum cross entropy threshold](https://scikit-image.org/docs/dev/auto_examples/developers/plot_threshold_li.html) are two common algorithms. Below, we use Li. You can use `skimage.filters.threshold_<TAB>` to find different thresholding methods.\n",
    "\n",
    "_Li_ procedure  better matches the CellProfiler pipeline which simply calls it \"Minimum Cross Entropy\" .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORE PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "###################\n",
    "# CORE_PROCESSING\n",
    "###################\n",
    "\n",
    "#tol = max(numpy.min(numpy.diff(numpy.unique(structure_img_median))) / 2, 0.5 / 65536) #assumes 16bit?\n",
    "#threshold_value = filters.threshold_li(structure_img_smooth)\n",
    "\n",
    "threshold_value_log = threshold_li_log(structure_img_smooth)\n",
    "\n",
    "threshold_factor = 0.9 #from cellProfiler\n",
    "thresh_min = 0.1\n",
    "thresh_max = 1.0\n",
    "threshold = min( max(threshold_value_log*threshold_factor, thresh_min), thresh_max)\n",
    "\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "li_thresholded = structure_img_smooth > threshold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POST PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# POST_PROCESSING\n",
    "###################\n",
    "\n",
    "hole_width = 5  \n",
    "# # wrapper to remoce_small_objects\n",
    "removed_holes = morphology.remove_small_holes(li_thresholded, hole_width ** 3 )\n",
    "\n",
    "\n",
    "small_object_max = 5\n",
    "cleaned_img = aicssegmentation.core.utils.size_filter(removed_holes, # wrapper to remove_small_objects which can do slice by slice\n",
    "                                                         min_size= small_object_max**3, \n",
    "                                                         method = \"slice_by_slice\", #\"3D\", # \n",
    "                                                         connectivity=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUTPUT + Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "NU_object = cleaned_img\n",
    "NU_labels = label(cleaned_img   )\n",
    "NU_signal = struct_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'NU_labels' at 0x170987c10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "viewer.add_image(\n",
    "    NU_object,\n",
    "    scale=scale,\n",
    "    opacity=0.3,\n",
    ")    \n",
    "\n",
    "\n",
    "viewer.add_labels(\n",
    "    NU_labels,\n",
    "    scale=scale,\n",
    "    opacity=0.3,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINE `infer_NUCLEI` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy this to base.py for easy import\n",
    "\n",
    "def _infer_NUCLEI(struct_img, in_params) -> tuple:\n",
    "    \"\"\"\n",
    "    Procedure to infer NUCLEI from linearly unmixed input.\n",
    "\n",
    "    Parameters:\n",
    "    ------------\n",
    "    struct_img: np.ndarray\n",
    "        a 3d image containing the NUCLEI signal\n",
    "\n",
    "    in_params: dict\n",
    "        holds the needed parameters\n",
    "\n",
    "    Returns:\n",
    "    -------------\n",
    "    tuple of:\n",
    "        object\n",
    "            mask defined boundaries of NU\n",
    "        label\n",
    "            label (could be more than 1)\n",
    "        signal\n",
    "            scaled/filtered (pre-processed) flourescence image\n",
    "        parameters: dict\n",
    "            updated parameters in case any needed were missing\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    out_p= in_params.copy()\n",
    "\n",
    "\n",
    "    ###################\n",
    "    # PRE_PROCESSING\n",
    "    ###################                         \n",
    "\n",
    "    #TODO: replace params below with the input params\n",
    "    scaling_param =  [0]   \n",
    "    struct_img = intensity_normalization(struct_img, scaling_param=scaling_param)\n",
    "    out_p[\"intensity_norm_param\"] = scaling_param\n",
    "\n",
    "    med_filter_size = 4   \n",
    "    # structure_img_median_3D = ndi.median_filter(struct_img,    size=med_filter_size  )\n",
    "    struct_img = median_filter_slice_by_slice( \n",
    "                                                                    struct_img,\n",
    "                                                                    size=med_filter_size  )\n",
    "    out_p[\"median_filter_size\"] = med_filter_size \n",
    "\n",
    "    gaussian_smoothing_sigma = 1.34\n",
    "    gaussian_smoothing_truncate_range = 3.0\n",
    "    struct_img = image_smoothing_gaussian_slice_by_slice(   struct_img,\n",
    "                                                                                                        sigma=gaussian_smoothing_sigma,\n",
    "                                                                                                        truncate_range = gaussian_smoothing_truncate_range\n",
    "                                                                                                    )\n",
    "    out_p[\"gaussian_smoothing_sigma\"] = gaussian_smoothing_sigma \n",
    "    out_p[\"gaussian_smoothing_truncate_range\"] = gaussian_smoothing_truncate_range\n",
    "\n",
    "    ###################\n",
    "    # CORE_PROCESSING\n",
    "    ###################\n",
    "\n",
    "    struct_obj = struct_img > filters.threshold_li(struct_img)\n",
    "    threshold_value_log = threshold_li_log(struct_img)\n",
    "\n",
    "    threshold_factor = 0.9 #from cellProfiler\n",
    "    thresh_min = .1\n",
    "    thresh_max = 1.\n",
    "    threshold = min( max(threshold_value_log*threshold_factor, thresh_min), thresh_max)\n",
    "    out_p['threshold_factor'] = threshold_factor\n",
    "    out_p['thresh_min'] = thresh_min\n",
    "    out_p['thresh_max'] = thresh_max\n",
    "\n",
    "    struct_obj = struct_img > threshold\n",
    "\n",
    "    ###################\n",
    "    # POST_PROCESSING\n",
    "    ###################\n",
    "\n",
    "    hole_width = 5  \n",
    "    # # wrapper to remoce_small_objects\n",
    "    struct_obj = morphology.remove_small_holes(struct_obj, hole_width ** 3 )\n",
    "    out_p['hole_width'] = hole_width\n",
    "\n",
    "\n",
    "    small_object_max = 5\n",
    "    struct_obj = aicssegmentation.core.utils.size_filter(struct_obj, # wrapper to remove_small_objects which can do slice by slice\n",
    "                                                            min_size= small_object_max**3, \n",
    "                                                         method = \"slice_by_slice\", #\"3D\", # \n",
    "                                                            connectivity=1)\n",
    "    out_p['small_object_max'] = small_object_max\n",
    "\n",
    "\n",
    "    retval = (struct_obj,  label(struct_obj), out_p)\n",
    "    return retval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "Use the `infer_NUCLEI` function to infer the Nucleus and export it as an _ome.tif_ for easy reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intensity normalization: min-max normalization with NO absoluteintensity upper bound\n",
      "intensity normalization: min-max normalization with NO absoluteintensity upper bound\n"
     ]
    }
   ],
   "source": [
    "# test - 9.7 secods to run...\n",
    "\n",
    "\n",
    "NU_object, NU_label, out_p =  infer_NUCLEI(raw_nuclei.copy(), default_params) \n",
    "NU_object, NU_label, out_p =  _infer_NUCLEI(raw_nuclei.copy(), default_params) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NU_object']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/napariNEW/lib/python3.9/site-packages/ome_types/_convenience.py:105: FutureWarning: The default XML parser will be changing from 'xmlschema' to 'lxml' in version 0.4.0.  To silence this warning, please provide the `parser` argument, specifying either 'lxml' (to opt into the new behavior), or'xmlschema' (to retain the old behavior).\n",
      "  d = to_dict(os.fspath(xml), parser=parser, validate=validate)\n"
     ]
    }
   ],
   "source": [
    "# TODO:  make export ome_tiff export:   XX_object, XX_label, XX_signal\n",
    "#              also fix Path vs. str action for export wrapper\n",
    "\n",
    "chan_name = 'nuclei'\n",
    "out_path = data_root_path / \"inferred_objects\" \n",
    "object_name = 'NU_object'\n",
    "\n",
    "NU_object_filen = export_ome_tiff(NU_object, meta_dict, object_name, str(out_path)+\"/\", curr_chan=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ahenrie/Projects/Imaging/mcz_subcell/data/inferred_objects/NU_object.ome.tiff'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NU_object_filen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(str,\n",
       "            {'intensity_norm_param': [0],\n",
       "             'gaussian_smoothing_sigma': 1.34,\n",
       "             'gaussian_smoothing_truncate_range': 3.0,\n",
       "             'dot_2d_sigma': 2,\n",
       "             'dot_2d_sigma_extra': 1,\n",
       "             'dot_2d_cutoff': 0.025,\n",
       "             'min_area': 10,\n",
       "             'low_level_min_size': 100,\n",
       "             'median_filter_size': 4,\n",
       "             'z_factor': 7.0,\n",
       "             'scale': (0.5804527163320905,\n",
       "              0.07987165184837318,\n",
       "              0.07987165184837318),\n",
       "             'threshold_factor': 0.9,\n",
       "             'thresh_min': 0.1,\n",
       "             'thresh_max': 1.0,\n",
       "             'hole_width': 5,\n",
       "             'small_object_max': 5})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d6148ef1fb015fb20f0b6da2ea61c87c6b848bdf3dabb03087e5d5cd0c4607e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
